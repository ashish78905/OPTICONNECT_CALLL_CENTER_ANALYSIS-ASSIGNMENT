{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ashish78905/OPTICONNECT_CALLL_CENTER_ANALYSIS-ASSIGNMENT/blob/main/user_behaviour_analysis_roadmap.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5a176de9",
      "metadata": {
        "id": "5a176de9"
      },
      "source": [
        "---\n",
        "\n",
        "# ðŸ—ºï¸ UBA_PRO COMPLETE DEVELOPMENT ROADMAP\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "# ðŸ“‹ ROADMAP OVERVIEW\n",
        "\n",
        "```\n",
        "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
        "â”‚                        UBA_PRO DEVELOPMENT PHASES                                â”‚\n",
        "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
        "â”‚                                                                                  â”‚\n",
        "â”‚  PHASE 1: FOUNDATION                                                             â”‚\n",
        "â”‚  â”œâ”€â”€ Environment Setup                                                           â”‚\n",
        "â”‚  â”œâ”€â”€ Project Structure Creation                                                  â”‚\n",
        "â”‚  â””â”€â”€ Database Models & Migrations                                                â”‚\n",
        "â”‚                                                                                  â”‚\n",
        "â”‚  PHASE 2: CORE UTILITIES                                                         â”‚\n",
        "â”‚  â”œâ”€â”€ Hash Utilities                                                              â”‚\n",
        "â”‚  â”œâ”€â”€ Encoding Utilities                                                          â”‚\n",
        "â”‚  â”œâ”€â”€ Timestamp Utilities                                                         â”‚\n",
        "â”‚  â””â”€â”€ Configuration System                                                        â”‚\n",
        "â”‚                                                                                  â”‚\n",
        "â”‚  PHASE 3: DATA LAYER                                                             â”‚\n",
        "â”‚  â”œâ”€â”€ DSPM Database Connector                                                     â”‚\n",
        "â”‚  â”œâ”€â”€ Data Extraction Services                                                    â”‚\n",
        "â”‚  â”œâ”€â”€ CSV/Parquet Connectors                                                      â”‚\n",
        "â”‚  â””â”€â”€ CoreDataFrame Implementation                                                â”‚\n",
        "â”‚                                                                                  â”‚\n",
        "â”‚  PHASE 4: USER & ENTITY MANAGEMENT                                               â”‚\n",
        "â”‚  â”œâ”€â”€ User Profile Service                                                        â”‚\n",
        "â”‚  â”œâ”€â”€ Entity Management                                                           â”‚\n",
        "â”‚  â”œâ”€â”€ Behavior Baseline Storage                                                   â”‚\n",
        "â”‚  â””â”€â”€ User Risk Profile Database                                                  â”‚\n",
        "â”‚                                                                                  â”‚\n",
        "â”‚  PHASE 5: FEATURE EXTRACTION                                                     â”‚\n",
        "â”‚  â”œâ”€â”€ Login Feature Extractor                                                     â”‚\n",
        "â”‚  â”œâ”€â”€ Access Feature Extractor                                                    â”‚\n",
        "â”‚  â”œâ”€â”€ Data Movement Feature Extractor                                             â”‚\n",
        "â”‚  â”œâ”€â”€ Time-based Feature Extractor                                                â”‚\n",
        "â”‚  â””â”€â”€ Feature Pipeline Integration                                                â”‚\n",
        "â”‚                                                                                  â”‚\n",
        "â”‚  PHASE 6: ML MODEL DEVELOPMENT                                                   â”‚\n",
        "â”‚  â”œâ”€â”€ Base Model Interface                                                        â”‚\n",
        "â”‚  â”œâ”€â”€ Login Anomaly Model (Isolation Forest)                                      â”‚\n",
        "â”‚  â”œâ”€â”€ Access Anomaly Model                                                        â”‚\n",
        "â”‚  â”œâ”€â”€ Data Transfer Anomaly Model                                                 â”‚\n",
        "â”‚  â”œâ”€â”€ Time Anomaly Model                                                          â”‚\n",
        "â”‚  â”œâ”€â”€ Model Training Pipeline                                                     â”‚\n",
        "â”‚  â”œâ”€â”€ Model Validation & Testing                                                  â”‚\n",
        "â”‚  â””â”€â”€ Model Persistence (Save/Load)                                               â”‚\n",
        "â”‚                                                                                  â”‚\n",
        "â”‚  PHASE 7: ANALYTICS ENGINE                                                       â”‚\n",
        "â”‚  â”œâ”€â”€ Process Engine (Data Ingestion)                                             â”‚\n",
        "â”‚  â”œâ”€â”€ Model Execution Engine                                                      â”‚\n",
        "â”‚  â”œâ”€â”€ Risk Score Calculator                                                       â”‚\n",
        "â”‚  â”œâ”€â”€ Anomaly Aggregator                                                          â”‚\n",
        "â”‚  â””â”€â”€ Scheduler Integration                                                       â”‚\n",
        "â”‚                                                                                  â”‚\n",
        "â”‚  PHASE 8: ALERT & CASE SYSTEM                                                    â”‚\n",
        "â”‚  â”œâ”€â”€ Alert Generation Service                                                    â”‚\n",
        "â”‚  â”œâ”€â”€ Alert Priority System                                                       â”‚\n",
        "â”‚  â”œâ”€â”€ Case Management Service                                                     â”‚\n",
        "â”‚  â””â”€â”€ Workflow Integration                                                        â”‚\n",
        "â”‚                                                                                  â”‚\n",
        "â”‚  PHASE 9: RULE ENGINE                                                            â”‚\n",
        "â”‚  â”œâ”€â”€ Rule Definition System                                                      â”‚\n",
        "â”‚  â”œâ”€â”€ Rule Evaluation Engine                                                      â”‚\n",
        "â”‚  â”œâ”€â”€ Rule-based Detection                                                        â”‚\n",
        "â”‚  â””â”€â”€ MITRE ATT&CK Mapping                                                        â”‚\n",
        "â”‚                                                                                  â”‚\n",
        "â”‚  PHASE 10: API LAYER                                                             â”‚\n",
        "â”‚  â”œâ”€â”€ FastAPI Routes Implementation                                               â”‚\n",
        "â”‚  â”œâ”€â”€ Pydantic Schemas                                                            â”‚\n",
        "â”‚  â”œâ”€â”€ API Authentication                                                          â”‚\n",
        "â”‚  â””â”€â”€ DSPM Integration Endpoints                                                  â”‚\n",
        "â”‚                                                                                  â”‚\n",
        "â”‚  PHASE 11: TESTING & VALIDATION                                                  â”‚\n",
        "â”‚  â”œâ”€â”€ Unit Tests                                                                  â”‚\n",
        "â”‚  â”œâ”€â”€ Integration Tests                                                           â”‚\n",
        "â”‚  â”œâ”€â”€ ML Model Performance Tests                                                  â”‚\n",
        "â”‚  â””â”€â”€ End-to-End Testing                                                          â”‚\n",
        "â”‚                                                                                  â”‚\n",
        "â”‚  PHASE 12: DEPLOYMENT & INTEGRATION                                              â”‚\n",
        "â”‚  â”œâ”€â”€ DSPM Integration                                                            â”‚\n",
        "â”‚  â”œâ”€â”€ Docker Configuration                                                        â”‚\n",
        "â”‚  â”œâ”€â”€ Production Deployment                                                       â”‚\n",
        "â”‚  â””â”€â”€ Monitoring Setup                                                            â”‚\n",
        "â”‚                                                                                  â”‚\n",
        "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "# ðŸ“Š FEATURE MAPPING: OpenUBA â†’ UBA_PRO\n",
        "\n",
        "## What You Will Implement (ALL OpenUBA Features)\n",
        "\n",
        "| OpenUBA Feature | OpenUBA Status | What This Means | UBA_PRO Location | Priority |\n",
        "|-----------------|---------------|-----------------|------------------|----------|\n",
        "| **Core Engine** | âœ… Implemented | Flask server + scheduler working | `uba/analytics/engine.py` | P0 |\n",
        "| **Process Engine** | âœ… Implemented | Data loading pipeline works | `uba/services/behavior_service.py` | P0 |\n",
        "| **Model Engine** | âš ï¸ Framework Only | Infrastructure exists, models are stubs | `uba/analytics/engine.py` | P0 |\n",
        "| **Model Library** | âš ï¸ Test/Example Only | Contains test models, not production ML | `uba/analytics/model_library/` | P0 |\n",
        "| **Dataset Loading** | âœ… Implemented | CSV, Parquet, ES loaders work | `uba/connectors/` + `uba/analytics/data_loader.py` | P0 |\n",
        "| **User Management** | âœ… Implemented | User extraction from logs works | `uba/services/profile_service.py` | P1 |\n",
        "| **Risk Calculation** | âŒ Empty Stub | Only class definition, no logic (OpenUBA has `RiskJob` and `RiskScore` classes in `risk.py`) | `uba/analytics/risk_calculator.py` + `uba/services/risk_service.py` | P0 |\n",
        "| **Risk Score System** | âŒ Empty Stub | OpenUBA uses `RiskJob`/`RiskScore` classes (not RiskManager) - basic structure with scheduling | `uba/services/risk_service.py` | P0 |\n",
        "| **Anomaly Detection** | âŒ Empty Stub | Only class definition, no logic | `uba/services/anomaly_service.py` | P0 |\n",
        "| **Alert System** | âŒ Empty Stub | Only class definition, no logic | `uba/services/alert_service.py` | P1 |\n",
        "| **Case Management** | âŒ Empty Stub | Only class definition, no logic | `uba/services/case_service.py` | P2 |\n",
        "| **Rule Engine** | âŒ Empty Stub | Only class definition, no logic | `uba/services/rule_service.py` (NEW) | P1 |\n",
        "| **Hash Utilities** | âœ… Implemented | SHA-256 hashing works | `uba/utils/hash_utils.py` | P0 |\n",
        "| **Encode Utilities** | âœ… Implemented | Base64 encode/decode works | `uba/utils/encode_utils.py` | P0 |\n",
        "| **Timestamp Utilities** | âœ… Implemented | Already exists in `core/utility.py` | `uba/utils/timestamp.py` | P0 |\n",
        "| **Entity Management** | âš ï¸ Partial | Basic structure, needs expansion | `uba/services/entity_service.py` (NEW) | P1 |\n",
        "| **Display/API** | âš ï¸ Partial | Basic Flask routes exist | `uba/routes/` | P1 |\n",
        "| **MITRE Mapping** | Config only | JSON config exists, no logic | `uba/config/mitre_mapping.py` (NEW) | P2 |\n",
        "| **Model Verification** | âœ… Implemented | Hash-based verification exists | `uba/utils/model_verification.py` (NEW) | P1 |\n",
        "| **Library API** | âœ… Implemented | Remote model download works | `uba/services/model_library_api.py` (NEW) | P1 |\n",
        "| **DSPM Integration** | ðŸ†• NEW | Must build from scratch | `uba/services/dspm_integration.py` | P0 |\n",
        "\n",
        "---\n",
        "\n",
        "## âš ï¸ IMPORTANT: What \"Implemented\" Really Means\n",
        "\n",
        "**In OpenUBA, many features are FRAMEWORKS, not complete implementations:**\n",
        "- âœ… **Implemented** = Working code you can use/adapt\n",
        "- âš ï¸ **Framework Only** = Structure exists, but core logic is minimal\n",
        "- âš ï¸ **Test/Example Only** = Demo code, not production-ready\n",
        "- âŒ **Empty Stub** = Just class/function definitions with `pass` or TODO\n",
        "\n",
        "**You will build ~70% of the actual UBA logic from scratch!**\n",
        "\n",
        "---\n",
        "\n",
        "## Priority Legend\n",
        "- **P0**: Must have for MVP (Minimum Viable Product)\n",
        "- **P1**: Important for full functionality\n",
        "- **P2**: Nice to have, can be added later"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "79b08916",
      "metadata": {
        "id": "79b08916"
      },
      "source": [
        "---\n",
        "\n",
        "# ðŸ”´ CRITICAL ADDITIONS: Missing OpenUBA Patterns\n",
        "\n",
        "## These patterns from OpenUBA are ESSENTIAL but were missing from the original roadmap\n",
        "\n",
        "---\n",
        "\n",
        "## ðŸ“ ADDITION 1: Storage Configuration System (Add to Phase 1)\n",
        "\n",
        "### What OpenUBA Has:\n",
        "OpenUBA uses a `storage/` folder with JSON configuration files that control how the system operates.\n",
        "\n",
        "### You Must Create These Configuration Files:\n",
        "\n",
        "| File | Purpose | When It's Used |\n",
        "|------|---------|----------------|\n",
        "| `storage/models.json` | Defines all ML models, their components, hashes, and settings | Model Engine reads this to know which models to run |\n",
        "| `storage/scheme.json` | Defines data sources (which log files to process) | Process Engine reads this to load data |\n",
        "| `storage/settings.json` | System-wide settings | Application startup |\n",
        "| `storage/users.json` | Master list of discovered users | User tracking |\n",
        "| `storage/model_sessions.json` | Tracks model execution history | Analytics reporting |\n",
        "\n",
        "### Directory Structure to Add:\n",
        "\n",
        "```\n",
        "UBA_PRO/\n",
        "â”œâ”€â”€ uba/\n",
        "â”‚   â””â”€â”€ storage/\n",
        "â”‚       â”œâ”€â”€ models.json              # Model configurations\n",
        "â”‚       â”œâ”€â”€ scheme.json              # Data source definitions\n",
        "â”‚       â”œâ”€â”€ settings.json            # System settings\n",
        "â”‚       â”œâ”€â”€ users.json               # User registry\n",
        "â”‚       â”œâ”€â”€ model_sessions.json      # Execution logs\n",
        "â”‚       â””â”€â”€ users/                   # Per-user folders\n",
        "â”‚           â”œâ”€â”€ user1/               # Individual user data\n",
        "â”‚           â”œâ”€â”€ user2/\n",
        "â”‚           â””â”€â”€ ...\n",
        "```\n",
        "\n",
        "### How models.json Works:\n",
        "- Contains MODEL GROUPS (collections of related models)\n",
        "- Each model group shares a DATA LOADER (where to get data)\n",
        "- Each model group can have INLINE RULES\n",
        "- Each model has COMPONENTS (files), HASH verification, and RETURN TYPE\n",
        "\n",
        "### How scheme.json Works:\n",
        "- Defines SOURCE GROUPS (collections of log sources)\n",
        "- Each source specifies: log type, delimiter, folder location, ID column\n",
        "- Process Engine iterates through this to load all data\n",
        "\n",
        "---\n",
        "\n",
        "## ðŸ“ ADDITION 2: Model Modules System (Add to Phase 3)\n",
        "\n",
        "### What OpenUBA Has:\n",
        "A `model_modules/` folder containing REUSABLE data loaders that models can import.\n",
        "\n",
        "### You Must Create These Modules:\n",
        "\n",
        "| Module | Purpose | Used By |\n",
        "|--------|---------|---------|\n",
        "| `LocalPandasCSV` | Load CSV files from local disk | Models needing CSV data |\n",
        "| `LocalPandasParquet` | Load Parquet files from local disk | Models needing Parquet data |\n",
        "| `ESGeneric` | Query Elasticsearch indices | Models needing ES data |\n",
        "| `HDFSPandasCSV` | Load CSV from Hadoop | Big data deployments |\n",
        "| `HDFSSparkCSV` | Load CSV using Spark | Very large datasets |\n",
        "\n",
        "### Directory Structure to Add:\n",
        "\n",
        "```\n",
        "UBA_PRO/\n",
        "â”œâ”€â”€ uba/\n",
        "â”‚   â””â”€â”€ model_modules/\n",
        "â”‚       â”œâ”€â”€ __init__.py              # Exports all modules\n",
        "â”‚       â”œâ”€â”€ local_pandas.py          # LocalPandasCSV, LocalPandasParquet\n",
        "â”‚       â”œâ”€â”€ es_loader.py             # ESGeneric\n",
        "â”‚       â””â”€â”€ hdfs_loader.py           # HDFS loaders (optional)\n",
        "```\n",
        "\n",
        "### How Model Modules Work:\n",
        "1. Model Engine reads models.json to find data_loader_type\n",
        "2. Based on type (e.g., \"local_pandas_csv\"), it imports the right module\n",
        "3. Module loads data and returns a CoreDataFrame\n",
        "4. Model receives the data for analysis\n",
        "\n",
        "---\n",
        "\n",
        "## ðŸ“ ADDITION 3: Model Configuration Structure (Add to Phase 6)\n",
        "\n",
        "### OpenUBA Model Configuration Pattern:\n",
        "\n",
        "Each model in models.json has this structure:\n",
        "\n",
        "| Field | Type | Purpose |\n",
        "|-------|------|---------|\n",
        "| `model_name` | string | Unique identifier |\n",
        "| `description` | string | What the model does |\n",
        "| `enabled` | boolean | Whether to run this model |\n",
        "| `mitre_technique_id` | string | MITRE ATT&CK mapping (e.g., \"T1078\") |\n",
        "| `return.return_type` | string | What the model outputs (e.g., \"user_risks\") |\n",
        "| `score` | integer | Base risk score if model triggers |\n",
        "| `components` | array | Files that make up the model |\n",
        "| `components[].type` | string | \"native\" (built-in) or \"external\" (plugin) |\n",
        "| `components[].filename` | string | File name (e.g., \"MODEL.py\") |\n",
        "| `components[].file_hash` | string | SHA-256 hash for verification |\n",
        "| `components[].file_payload` | string | Base64-encoded file content |\n",
        "\n",
        "### Model Component Types:\n",
        "- **NATIVE**: Model is built into UBA_PRO core\n",
        "- **EXTERNAL**: Model is a plugin that gets downloaded/installed\n",
        "\n",
        "### Model Return Types:\n",
        "- **user_risks**: Returns risk scores per user\n",
        "- **anomalies**: Returns list of detected anomalies\n",
        "- **alerts**: Returns generated alerts\n",
        "\n",
        "---\n",
        "\n",
        "## ðŸ“ ADDITION 4: Process Engine Concept (Add to Phase 7)\n",
        "\n",
        "### What OpenUBA Has:\n",
        "A `ProcessEngine` class that handles the DATA INGESTION pipeline.\n",
        "\n",
        "### Process Engine Responsibilities:\n",
        "1. Read the data scheme (scheme.json)\n",
        "2. For each source group, load all log files\n",
        "3. Parse log records using appropriate delimiters\n",
        "4. Extract users/entities from loaded data\n",
        "5. Store discovered users in storage/users/\n",
        "\n",
        "### Process Engine Flow:\n",
        "```\n",
        "scheme.json â†’ ProcessEngine.execute() â†’ Load Each Log â†’ Extract Users â†’ Store Users\n",
        "```\n",
        "\n",
        "### What You Must Implement:\n",
        "- ProcessEngine class with execute() method\n",
        "- DatasetSession class for managing data loading sessions\n",
        "- Parser class for splitting log records\n",
        "- Integration with scheme.json configuration\n",
        "\n",
        "---\n",
        "\n",
        "## ðŸ“ ADDITION 5: Model Session Concept (Add to Phase 7)\n",
        "\n",
        "### What OpenUBA Has:\n",
        "A `ModelSession` class that runs a SINGLE model job.\n",
        "\n",
        "### Model Session Responsibilities:\n",
        "1. Receive model configuration and data\n",
        "2. Check if model is installed locally\n",
        "3. If not installed, download from model library\n",
        "4. Verify model hash (security check)\n",
        "5. Execute the model's execute() function\n",
        "6. Return results based on return_type\n",
        "\n",
        "### Model Session Flow:\n",
        "```\n",
        "ModelEngine â†’ Create ModelSession â†’ Verify Model â†’ Execute â†’ Return Results\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## ðŸ“ ADDITION 6: User Extraction Pattern (Add to Phase 4)\n",
        "\n",
        "### What OpenUBA Has:\n",
        "An `ExtractAllUsersCSV` class that discovers users from log data.\n",
        "\n",
        "### User Extraction Process:\n",
        "1. Receive loaded DatasetSession\n",
        "2. Look up the ID column from log metadata (e.g., \"cs-username\")\n",
        "3. Extract all unique values from that column\n",
        "4. Create User objects for each unique value\n",
        "5. Save to UserSet\n",
        "6. Write to storage/users.json\n",
        "7. Create individual folders in storage/users/\n",
        "\n",
        "### Why Per-User Folders:\n",
        "- Each user gets their own folder (storage/users/username/)\n",
        "- Store user-specific data: baselines, history, features\n",
        "- Enables efficient per-user queries\n",
        "\n",
        "---\n",
        "\n",
        "## ðŸ“ ADDITION 7: Database Abstraction Pattern (Add to Phase 2)\n",
        "\n",
        "### What OpenUBA Has:\n",
        "Abstract base classes for database operations.\n",
        "\n",
        "### Classes to Implement:\n",
        "\n",
        "| Class | Purpose |\n",
        "|-------|---------|\n",
        "| `DB` | Base class for all database operations |\n",
        "| `DBReadFile` | Base class for reading files |\n",
        "| `DBWriteFile` | Base class for writing files |\n",
        "| `Connector` | Factory for creating storage connections |\n",
        "| `FSConnector` | File system storage connector |\n",
        "| `WriteJSONFileFS` | Write dictionary to JSON file |\n",
        "| `ReadJSONFileFS` | Read JSON file to dictionary |\n",
        "| `WriteListToDirectories` | Create folders for each item in list |\n",
        "\n",
        "### Why This Pattern:\n",
        "- Abstraction allows switching storage backends (local â†’ HDFS â†’ cloud)\n",
        "- Consistent interface for all file operations\n",
        "- Easy to add new storage types later\n",
        "\n",
        "---\n",
        "\n",
        "## ðŸ“ ADDITION 8: Display Service Pattern (Add to Phase 10)\n",
        "\n",
        "### What OpenUBA Has:\n",
        "A `Display` class that provides dashboard data.\n",
        "\n",
        "### Display Service Responsibilities:\n",
        "- Provide system statistics (monitored users, high risk count, etc.)\n",
        "- Format data for API responses\n",
        "- Cache expensive queries\n",
        "\n",
        "### API Type Enumeration (APIType Enum in api.py):\n",
        "OpenUBA defines these as an Enum class for type safety:\n",
        "\n",
        "```python\n",
        "class APIType(Enum):\n",
        "    GET_ALL_ENTITIES = \"get_all_entities\"\n",
        "    GET_ALL_USERS = \"get_all_users\"\n",
        "    GET_HOME_SUMMARY = \"get_home_summary\"\n",
        "    GET_SYSTEM_LOG = \"get_system_log\"\n",
        "```\n",
        "\n",
        "| Type | Endpoint | Returns |\n",
        "|------|----------|---------|\n",
        "| `GET_ALL_USERS` | /display/get_all_users | All users with risk scores |\n",
        "| `GET_ALL_ENTITIES` | /display/get_all_entities | All monitored entities |\n",
        "| `GET_SYSTEM_LOG` | /display/get_system_log | System statistics |\n",
        "| `GET_HOME_SUMMARY` | /display/get_home_summary | Dashboard summary |\n",
        "\n",
        "### PriorGetDisplay Decorator:\n",
        "OpenUBA uses a `@PriorGetDisplay` decorator in `api.py` that intercepts API calls and fetches the appropriate data based on display_type before the main function executes. This pattern ensures data is loaded before the API response is generated.\n",
        "\n",
        "---\n",
        "\n",
        "## ðŸ“ ADDITION 9: Inline Rules Pattern (Add to Phase 9)\n",
        "\n",
        "### What OpenUBA Has:\n",
        "Rules can be defined INLINE within model groups in models.json.\n",
        "\n",
        "### Inline Rule Structure:\n",
        "\n",
        "| Field | Purpose |\n",
        "|-------|---------|\n",
        "| `condition_name` | Unique name for the rule |\n",
        "| `condition_description` | Human-readable description |\n",
        "| `features` | Which data columns to check (pipe-separated) |\n",
        "| `condition` | The logic expression to evaluate |\n",
        "| `condition_type` | \"single-fire\" (exact match) or \"deviation\" (statistical) |\n",
        "| `score` | Risk points to add if rule matches |\n",
        "\n",
        "### Condition Types:\n",
        "- **single-fire**: Triggers when exact condition is met (e.g., user == 'admin')\n",
        "- **deviation**: Triggers when value deviates from mean by N standard deviations\n",
        "\n",
        "### âš ï¸ CRITICAL: Parameter Mapping in Rules\n",
        "The `features` field uses **pipe-separated** feature names that map to `parameter1`, `parameter2`, etc.:\n",
        "\n",
        "```\n",
        "Example Rule:\n",
        "{\n",
        "  \"features\": \"cs-username|cs-username\",\n",
        "  \"condition\": \"parameter1 == 'alice' and parameter2 != 'bob'\"\n",
        "}\n",
        "\n",
        "Mapping:\n",
        "- parameter1 = first feature value (cs-username column, row 1)\n",
        "- parameter2 = second feature value (cs-username column, row 2)\n",
        "```\n",
        "\n",
        "For `deviation` type:\n",
        "```\n",
        "{\n",
        "  \"condition\": \"(param_mean > (param_metric + param_range)) or (param_mean < (param_metric - param_range))\",\n",
        "  \"condition_type\": \"deviation\"\n",
        "}\n",
        "```\n",
        "\n",
        "### Why Inline Rules:\n",
        "- Rules run alongside ML models in same model group\n",
        "- Share the same data loader (efficient)\n",
        "- Combine ML + rule-based detection\n",
        "\n",
        "---\n",
        "\n",
        "## ðŸ“ ADDITION 10: Model Verification (Add to Phase 6)\n",
        "\n",
        "### What OpenUBA Has:\n",
        "Hash-based verification to ensure model files haven't been tampered with.\n",
        "\n",
        "### Verification Process:\n",
        "1. When model is installed, compute SHA-256 hash of each component file\n",
        "2. Store hash in models.json (file_hash field)\n",
        "3. Before running model, recompute hash\n",
        "4. Compare current hash vs stored hash\n",
        "5. If mismatch â†’ REJECT model (possible tampering)\n",
        "\n",
        "### Why This Matters:\n",
        "- Security: Prevents running malicious modified models\n",
        "- Integrity: Ensures model behaves as expected\n",
        "- Audit: Track which model version was used\n",
        "\n",
        "---\n",
        "\n",
        "## âš ï¸ IMPORTANT INTEGRATION NOTES\n",
        "\n",
        "### These additions integrate with existing phases:\n",
        "\n",
        "| Addition | Integrates With |\n",
        "|----------|-----------------|\n",
        "| Storage config system | Phase 1 (project structure) |\n",
        "| Model modules | Phase 3 (data layer) |\n",
        "| Model configuration | Phase 6 (ML models) |\n",
        "| Process Engine | Phase 7 (analytics engine) |\n",
        "| Model Session | Phase 7 (analytics engine) |\n",
        "| User extraction | Phase 4 (user management) |\n",
        "| Database abstraction | Phase 2 (core utilities) |\n",
        "| Display service | Phase 10 (API layer) |\n",
        "| Inline rules | Phase 9 (rule engine) |\n",
        "| Model verification | Phase 6 (ML models) |\n",
        "\n",
        "### Implementation Order:\n",
        "1. First: Storage config + Database abstraction (Phase 1-2)\n",
        "2. Second: Model modules + User extraction (Phase 3-4)\n",
        "3. Third: Model configuration + verification (Phase 6)\n",
        "4. Fourth: Process Engine + Model Session (Phase 7)\n",
        "5. Fifth: Display service + Inline rules (Phase 9-10)\n",
        "\n",
        "---\n",
        "\n",
        "## ðŸ”´ CRITICAL ADDITION 11: OpenUBA Model Pattern (MUST READ!)\n",
        "\n",
        "### âš ï¸ THIS IS THE MOST IMPORTANT PATTERN TO UNDERSTAND\n",
        "\n",
        "OpenUBA models are **FUNCTION-BASED**, not class-based!\n",
        "\n",
        "### OpenUBA Model Structure:\n",
        "```\n",
        "model_library/\n",
        "â””â”€â”€ model_name/\n",
        "    â”œâ”€â”€ __init__.py      # MUST contain: from .MODEL import execute\n",
        "    â””â”€â”€ MODEL.py         # MUST contain: def execute() -> dict\n",
        "```\n",
        "\n",
        "### The `execute()` Function Contract:\n",
        "Every OpenUBA model MUST expose an `execute()` function:\n",
        "\n",
        "```python\n",
        "# MODEL.py - REQUIRED STRUCTURE\n",
        "def execute() -> dict:\n",
        "    \\\"\\\"\\\"\n",
        "    Execute the model analysis.\n",
        "    \n",
        "    Returns:\n",
        "        dict: Results based on return_type in models.json\n",
        "              For \"user_risks\": {\"user1\": {\"risk\": 0.8}, \"user2\": {...}}\n",
        "    \\\"\\\"\\\"\n",
        "    return_object: dict = {}\n",
        "    \n",
        "    # Your model logic here\n",
        "    # Data is loaded by ModelEngine BEFORE this is called\n",
        "    \n",
        "    return return_object\n",
        "```\n",
        "\n",
        "### __init__.py MUST contain:\n",
        "```python\n",
        "# __init__.py - REQUIRED\n",
        "from .MODEL import execute\n",
        "```\n",
        "\n",
        "### How ModelEngine Runs Models:\n",
        "```python\n",
        "# Simplified from OpenUBA's model.py\n",
        "sys.path.insert(0, 'model_library/' + model_name)\n",
        "import MODEL\n",
        "result = MODEL.execute()  # <-- This is called!\n",
        "sys.path.remove('model_library/' + model_name)\n",
        "del sys.modules[\"MODEL\"]\n",
        "```\n",
        "\n",
        "### âš ï¸ For UBA_PRO: Hybrid Approach\n",
        "Since you're building a production system, use BOTH patterns:\n",
        "\n",
        "1. **OpenUBA-Compatible Models** (for model library compatibility):\n",
        "   - Use `execute()` function pattern\n",
        "   - Can be downloaded from model library\n",
        "   - Hash verification works\n",
        "\n",
        "2. **UBA_PRO Native Models** (for your ML models):\n",
        "   - Use class-based pattern with `train()`, `predict()`, `save()`, `load()`\n",
        "   - Better for scikit-learn, TensorFlow models\n",
        "   - Wrapper function `execute()` calls class methods\n",
        "\n",
        "### Bridge Pattern Example:\n",
        "```python\n",
        "# MODEL.py - Hybrid approach\n",
        "from .login_anomaly_model import LoginAnomalyModel\n",
        "\n",
        "_model = None\n",
        "\n",
        "def execute(data=None) -> dict:\n",
        "    \\\"\\\"\\\"OpenUBA-compatible entry point\\\"\\\"\\\"\n",
        "    global _model\n",
        "    if _model is None:\n",
        "        _model = LoginAnomalyModel()\n",
        "        _model.load(\"model.pkl\")\n",
        "    \n",
        "    if data is not None:\n",
        "        predictions = _model.predict(data)\n",
        "        return {\"predictions\": predictions}\n",
        "    return {}\n",
        "```\n",
        "\n",
        "### Why This Matters:\n",
        "- OpenUBA's ModelEngine expects `execute()` function\n",
        "- Your scikit-learn models need `fit()`, `predict()`\n",
        "- The bridge pattern gives you both!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "acc0a174",
      "metadata": {
        "id": "acc0a174"
      },
      "source": [
        "---\n",
        "\n",
        "# ðŸ”§ PHASE 1: FOUNDATION\n",
        "\n",
        "## Goal: Set up the complete development environment and project structure\n",
        "\n",
        "---\n",
        "\n",
        "## Step 1.1: Environment Setup\n",
        "\n",
        "### What You Need to Install:\n",
        "1. **Python 3.10+** - The programming language\n",
        "2. **PostgreSQL** - Database (same as DSPM uses)\n",
        "3. **Git** - Version control\n",
        "4. **VS Code** - Code editor (you already have this)\n",
        "5. **Virtual Environment** - Isolated Python environment\n",
        "\n",
        "### How to Do It:\n",
        "\n",
        "**Task 1.1.1: Create Virtual Environment**\n",
        "- Open terminal in UBA_PRO folder\n",
        "- Create a new Python virtual environment\n",
        "- Activate the virtual environment\n",
        "- This keeps your project dependencies separate from other projects\n",
        "\n",
        "**Task 1.1.2: Create requirements.txt**\n",
        "- List all Python packages your project needs\n",
        "- Key packages: fastapi, uvicorn, sqlalchemy, pandas, numpy, scikit-learn, pydantic\n",
        "\n",
        "**Task 1.1.3: Install Dependencies**\n",
        "- Use pip to install all packages from requirements.txt\n",
        "- Verify installation by importing key packages\n",
        "\n",
        "### Verification Checklist:\n",
        "- [ ] Python 3.10+ installed and working\n",
        "- [ ] Virtual environment created and activated\n",
        "- [ ] All packages installed without errors\n",
        "- [ ] Can import pandas, numpy, sklearn, fastapi\n",
        "\n",
        "---\n",
        "\n",
        "## Step 1.2: Project Structure Creation\n",
        "\n",
        "### What You Will Create:\n",
        "Convert all `.info` files to actual `.py` files with proper structure\n",
        "\n",
        "### Directory Structure to Create:\n",
        "\n",
        "```\n",
        "UBA_PRO/\n",
        "â”œâ”€â”€ uba/\n",
        "â”‚   â”œâ”€â”€ __init__.py                 # Package initializer\n",
        "â”‚   â”œâ”€â”€ main.py                     # FastAPI application entry point\n",
        "â”‚   â”‚\n",
        "â”‚   â”œâ”€â”€ config/\n",
        "â”‚   â”‚   â”œâ”€â”€ __init__.py\n",
        "â”‚   â”‚   â”œâ”€â”€ settings.py             # Environment configuration\n",
        "â”‚   â”‚   â””â”€â”€ constants.py            # Static values and enums\n",
        "â”‚   â”‚\n",
        "â”‚   â”œâ”€â”€ core/\n",
        "â”‚   â”‚   â”œâ”€â”€ __init__.py\n",
        "â”‚   â”‚   â”œâ”€â”€ database.py             # Database connection\n",
        "â”‚   â”‚   â”œâ”€â”€ logging.py              # Logging configuration\n",
        "â”‚   â”‚   â””â”€â”€ exceptions.py           # Custom exceptions\n",
        "â”‚   â”‚\n",
        "â”‚   â”œâ”€â”€ models/\n",
        "â”‚   â”‚   â”œâ”€â”€ __init__.py\n",
        "â”‚   â”‚   â””â”€â”€ uba_models.py           # SQLAlchemy database models\n",
        "â”‚   â”‚\n",
        "â”‚   â”œâ”€â”€ schemas/\n",
        "â”‚   â”‚   â”œâ”€â”€ __init__.py\n",
        "â”‚   â”‚   â””â”€â”€ uba_schemas.py          # Pydantic request/response schemas\n",
        "â”‚   â”‚\n",
        "â”‚   â”œâ”€â”€ routes/\n",
        "â”‚   â”‚   â”œâ”€â”€ __init__.py\n",
        "â”‚   â”‚   â”œâ”€â”€ dashboard.py\n",
        "â”‚   â”‚   â”œâ”€â”€ profiles.py\n",
        "â”‚   â”‚   â”œâ”€â”€ risk.py\n",
        "â”‚   â”‚   â”œâ”€â”€ anomalies.py\n",
        "â”‚   â”‚   â”œâ”€â”€ alerts.py\n",
        "â”‚   â”‚   â”œâ”€â”€ cases.py\n",
        "â”‚   â”‚   â”œâ”€â”€ models.py\n",
        "â”‚   â”‚   â””â”€â”€ behaviors.py\n",
        "â”‚   â”‚\n",
        "â”‚   â”œâ”€â”€ services/\n",
        "â”‚   â”‚   â”œâ”€â”€ __init__.py\n",
        "â”‚   â”‚   â”œâ”€â”€ profile_service.py\n",
        "â”‚   â”‚   â”œâ”€â”€ risk_service.py\n",
        "â”‚   â”‚   â”œâ”€â”€ anomaly_service.py\n",
        "â”‚   â”‚   â”œâ”€â”€ alert_service.py\n",
        "â”‚   â”‚   â”œâ”€â”€ case_service.py\n",
        "â”‚   â”‚   â”œâ”€â”€ behavior_service.py\n",
        "â”‚   â”‚   â”œâ”€â”€ model_service.py\n",
        "â”‚   â”‚   â”œâ”€â”€ rule_service.py         # NEW - not in OpenUBA\n",
        "â”‚   â”‚   â”œâ”€â”€ entity_service.py       # NEW - expanded from OpenUBA\n",
        "â”‚   â”‚   â””â”€â”€ dspm_integration.py     # NEW - DSPM connector\n",
        "â”‚   â”‚\n",
        "â”‚   â”œâ”€â”€ connectors/\n",
        "â”‚   â”‚   â”œâ”€â”€ __init__.py\n",
        "â”‚   â”‚   â”œâ”€â”€ dspm_connector.py       # Connect to DSPM PostgreSQL\n",
        "â”‚   â”‚   â”œâ”€â”€ csv_connector.py\n",
        "â”‚   â”‚   â”œâ”€â”€ parquet_connector.py\n",
        "â”‚   â”‚   â””â”€â”€ elasticsearch_connector.py\n",
        "â”‚   â”‚\n",
        "â”‚   â”œâ”€â”€ analytics/\n",
        "â”‚   â”‚   â”œâ”€â”€ __init__.py\n",
        "â”‚   â”‚   â”œâ”€â”€ engine.py               # Main analytics orchestrator\n",
        "â”‚   â”‚   â”œâ”€â”€ data_loader.py          # Load data from various sources\n",
        "â”‚   â”‚   â”œâ”€â”€ feature_extractor.py    # Extract ML features\n",
        "â”‚   â”‚   â”œâ”€â”€ risk_calculator.py      # Calculate risk scores\n",
        "â”‚   â”‚   â”œâ”€â”€ core_dataframe.py       # DataFrame wrapper\n",
        "â”‚   â”‚   â””â”€â”€ model_library/\n",
        "â”‚   â”‚       â”œâ”€â”€ __init__.py\n",
        "â”‚   â”‚       â”œâ”€â”€ base_model.py       # Abstract base for all models\n",
        "â”‚   â”‚       â”œâ”€â”€ login_anomaly.py    # Login behavior model\n",
        "â”‚   â”‚       â”œâ”€â”€ access_anomaly.py   # Data access model\n",
        "â”‚   â”‚       â”œâ”€â”€ transfer_anomaly.py # Data transfer model\n",
        "â”‚   â”‚       â””â”€â”€ time_anomaly.py     # Time-based model\n",
        "â”‚   â”‚\n",
        "â”‚   â”œâ”€â”€ utils/\n",
        "â”‚   â”‚   â”œâ”€â”€ __init__.py\n",
        "â”‚   â”‚   â”œâ”€â”€ hash_utils.py           # SHA-256 hashing\n",
        "â”‚   â”‚   â”œâ”€â”€ encode_utils.py         # Base64 encoding\n",
        "â”‚   â”‚   â”œâ”€â”€ timestamp.py            # Time utilities\n",
        "â”‚   â”‚   â””â”€â”€ model_verification.py   # Model integrity checks\n",
        "â”‚   â”‚\n",
        "â”‚   â”œâ”€â”€ middleware/\n",
        "â”‚   â”‚   â”œâ”€â”€ __init__.py\n",
        "â”‚   â”‚   â””â”€â”€ auth_middleware.py      # Authentication\n",
        "â”‚   â”‚\n",
        "â”‚   â””â”€â”€ tests/\n",
        "â”‚       â”œâ”€â”€ __init__.py\n",
        "â”‚       â”œâ”€â”€ test_utils.py\n",
        "â”‚       â”œâ”€â”€ test_connectors.py\n",
        "â”‚       â”œâ”€â”€ test_services.py\n",
        "â”‚       â”œâ”€â”€ test_models.py\n",
        "â”‚       â””â”€â”€ test_analytics.py\n",
        "â”‚\n",
        "â”œâ”€â”€ alembic/                        # Database migrations\n",
        "â”‚   â”œâ”€â”€ versions/\n",
        "â”‚   â””â”€â”€ env.py\n",
        "â”‚\n",
        "â”œâ”€â”€ docs/\n",
        "â”‚   â””â”€â”€ TRAINING_DATA_STRATEGY.md\n",
        "â”‚\n",
        "â”œâ”€â”€ requirements.txt\n",
        "â”œâ”€â”€ alembic.ini\n",
        "â”œâ”€â”€ .env                            # Environment variables\n",
        "â”œâ”€â”€ .gitignore\n",
        "â””â”€â”€ README.md\n",
        "```\n",
        "\n",
        "### How to Do It:\n",
        "\n",
        "**Task 1.2.1: Create All __init__.py Files**\n",
        "- Every folder needs an `__init__.py` file\n",
        "- This makes Python recognize it as a package\n",
        "- Start with empty files, add imports later\n",
        "\n",
        "**Task 1.2.2: Create Base Python Files**\n",
        "- Convert each `.info` file to a `.py` file\n",
        "- Add proper docstrings and class/function stubs\n",
        "- Do NOT write implementation yet - just structure\n",
        "\n",
        "**Task 1.2.3: Create Configuration Files**\n",
        "- Create `.env` file for environment variables\n",
        "- Create `requirements.txt` with all dependencies\n",
        "- Create `.gitignore` to exclude unnecessary files\n",
        "\n",
        "### Verification Checklist:\n",
        "- [ ] All directories created\n",
        "- [ ] All `__init__.py` files in place\n",
        "- [ ] Base `.py` files created (empty stubs)\n",
        "- [ ] Can run `python -c \"import uba\"` without errors\n",
        "\n",
        "---\n",
        "\n",
        "## Step 1.3: Database Models & Migrations\n",
        "\n",
        "### What You Will Create:\n",
        "SQLAlchemy models for UBA-specific tables\n",
        "\n",
        "### UBA Database Tables to Create:\n",
        "\n",
        "| Table Name | Purpose | Key Fields |\n",
        "|------------|---------|------------|\n",
        "| `uba_user_profiles` | Store user behavior baselines | user_id, baseline_data, risk_score, last_updated |\n",
        "| `uba_risk_scores` | Historical risk score tracking | user_id, score, components, calculated_at |\n",
        "| `uba_anomalies` | Detected anomalies | user_id, anomaly_type, score, details, detected_at |\n",
        "| `uba_alerts` | Generated alerts | anomaly_id, severity, status, created_at |\n",
        "| `uba_cases` | Investigation cases | alert_ids, status, assignee, created_at |\n",
        "| `uba_rules` | Detection rules | rule_name, conditions, severity, enabled |\n",
        "| `uba_model_registry` | ML model metadata | model_name, version, path, status, metrics |\n",
        "| `uba_model_runs` | Model execution logs | model_id, started_at, completed_at, results |\n",
        "| `uba_feature_cache` | Cached extracted features | user_id, feature_type, features, extracted_at |\n",
        "\n",
        "### How to Do It:\n",
        "\n",
        "**Task 1.3.1: Create SQLAlchemy Models**\n",
        "- Define each table as a Python class\n",
        "- Use same patterns as DSPM models (you analyzed these)\n",
        "- Include proper indexes and relationships\n",
        "\n",
        "**Task 1.3.2: Set Up Alembic for Migrations**\n",
        "- Initialize Alembic in your project\n",
        "- Configure it to connect to PostgreSQL\n",
        "- This allows database schema versioning\n",
        "\n",
        "**Task 1.3.3: Create Initial Migration**\n",
        "- Generate migration from your models\n",
        "- Review the generated SQL\n",
        "- Apply migration to create tables\n",
        "\n",
        "**Task 1.3.4: Verify Tables Created**\n",
        "- Connect to PostgreSQL\n",
        "- Check that all tables exist\n",
        "- Verify columns and indexes are correct\n",
        "\n",
        "### Verification Checklist:\n",
        "- [ ] All SQLAlchemy models defined\n",
        "- [ ] Alembic configured correctly\n",
        "- [ ] Migration file generated\n",
        "- [ ] Tables created in PostgreSQL\n",
        "- [ ] Can query empty tables without errors\n",
        "\n",
        "---\n",
        "\n",
        "## ðŸ“ END OF PHASE 1\n",
        "\n",
        "### What You Have After Phase 1:\n",
        "1. âœ… Working development environment\n",
        "2. âœ… Complete project structure with all files\n",
        "3. âœ… Database tables created and ready\n",
        "4. âœ… Can run the application (empty, but no errors)\n",
        "\n",
        "### How to Test Phase 1 is Complete:\n",
        "1. Run `python -c \"from uba import main\"` - should not error\n",
        "2. Run `alembic current` - should show migration version\n",
        "3. Connect to PostgreSQL and see UBA tables"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "af805d11",
      "metadata": {
        "id": "af805d11"
      },
      "source": [
        "---\n",
        "\n",
        "# ðŸ”§ PHASE 2: CORE UTILITIES\n",
        "\n",
        "## Goal: Implement reusable utility functions (directly from OpenUBA)\n",
        "\n",
        "---\n",
        "\n",
        "## Step 2.1: Hash Utilities\n",
        "\n",
        "### What This Does:\n",
        "Generates SHA-256 cryptographic hashes for data integrity verification.\n",
        "Used for: Model verification, file integrity, data fingerprinting.\n",
        "\n",
        "### OpenUBA Reference:\n",
        "File: `core/hash.py` (87 lines - 100% reusable)\n",
        "\n",
        "### Implementation Tasks:\n",
        "\n",
        "**Task 2.1.1: Create Hash Base Class**\n",
        "- Location: `uba/utils/hash_utils.py`\n",
        "- Create a base `Hash` class with common functionality\n",
        "- Method: `compute()` - generates SHA-256 hash\n",
        "\n",
        "**Task 2.1.2: Create HashData Class**\n",
        "- Inherits from Hash base class\n",
        "- Takes raw bytes/string data as input\n",
        "- Returns hex-encoded hash string\n",
        "- Example: `HashData(b\"hello\").result` â†’ `\"2cf24dba5fb0a30e...\"`\n",
        "\n",
        "**Task 2.1.3: Create HashFile Class**\n",
        "- Inherits from Hash base class\n",
        "- Takes file path as input\n",
        "- Reads file and computes hash\n",
        "- Example: `HashFile(\"model.pkl\").result` â†’ `\"a5b9f...\"`\n",
        "\n",
        "**Task 2.1.4: Create HashLargeFile Class**\n",
        "- Inherits from Hash base class\n",
        "- Processes file in 4KB chunks (memory efficient)\n",
        "- Use for files > 100MB (large ML models)\n",
        "- Avoids loading entire file into memory\n",
        "- Example: `HashLargeFile(\"large_model.pkl\").result` â†’ `\"c7d8e...\"`\n",
        "\n",
        "**Task 2.1.5: Write Unit Tests**\n",
        "- Test with known input/output pairs\n",
        "- Test with various data types\n",
        "- Test file hashing with temp files\n",
        "- Test large file hashing with chunked processing\n",
        "\n",
        "### Verification Checklist:\n",
        "- [ ] HashData produces correct SHA-256 output\n",
        "- [ ] HashFile correctly hashes files\n",
        "- [ ] HashLargeFile handles large files efficiently (4KB chunks)\n",
        "- [ ] All unit tests pass\n",
        "- [ ] Edge cases handled (empty data, missing file)\n",
        "\n",
        "---\n",
        "\n",
        "## Step 2.2: Encoding Utilities\n",
        "\n",
        "### What This Does:\n",
        "Base64 encoding/decoding for data serialization.\n",
        "Used for: Model payload encoding, API data transfer, file encoding.\n",
        "\n",
        "### OpenUBA Reference:\n",
        "File: `core/encode.py` (84 lines - 100% reusable)\n",
        "\n",
        "### Implementation Tasks:\n",
        "\n",
        "**Task 2.2.1: Create Base64 Class**\n",
        "- Location: `uba/utils/encode_utils.py`\n",
        "- Method: `encode()` - convert bytes to base64 string\n",
        "- Method: `decode()` - convert base64 string to bytes\n",
        "\n",
        "**Task 2.2.2: Create B64EncodeFile Function**\n",
        "- Takes file path as input\n",
        "- Reads file and returns base64-encoded content\n",
        "- Used for model serialization\n",
        "\n",
        "**Task 2.2.3: Create B64DecodeFile Function**\n",
        "- Takes base64 string and output path\n",
        "- Decodes and writes to file\n",
        "- Used for model deserialization\n",
        "\n",
        "**Task 2.2.4: Write Unit Tests**\n",
        "- Test encode/decode round-trip\n",
        "- Test with binary files (images, pickles)\n",
        "- Test with text files\n",
        "\n",
        "### Verification Checklist:\n",
        "- [ ] Encode/decode round-trip produces original data\n",
        "- [ ] File operations work correctly\n",
        "- [ ] Binary data handled properly\n",
        "- [ ] All unit tests pass\n",
        "\n",
        "---\n",
        "\n",
        "## Step 2.3: Timestamp Utilities\n",
        "\n",
        "### What This Does:\n",
        "Standardized timestamp generation and formatting.\n",
        "Used for: Logging, audit trails, data timestamps.\n",
        "\n",
        "### OpenUBA Reference:\n",
        "File: `core/utility.py` (34 lines - Timestamp class) - **Already implemented in OpenUBA!**\n",
        "\n",
        "### OpenUBA Implementation (can copy directly):\n",
        "```python\n",
        "# From OpenUBA core/utility.py\n",
        "import time\n",
        "import datetime\n",
        "\n",
        "class Timestamp():\n",
        "    def __init__(self):\n",
        "        self.readable = datetime.datetime.fromtimestamp(\n",
        "            time.time()\n",
        "        ).strftime('%Y-%m-%d %H:%M:%S')\n",
        "```\n",
        "\n",
        "### Implementation Tasks:\n",
        "\n",
        "**Task 2.3.1: Create Timestamp Class**\n",
        "- Location: `uba/utils/timestamp.py`\n",
        "- Property: `readable` - human-readable format (e.g., \"2026-01-07 14:30:00\")\n",
        "- Property: `epoch` - Unix timestamp (seconds since 1970)\n",
        "- Property: `iso` - ISO 8601 format\n",
        "\n",
        "**Task 2.3.2: Add Timezone Support**\n",
        "- Default to UTC for consistency\n",
        "- Method to convert to local timezone\n",
        "- Store timezone info with timestamps\n",
        "\n",
        "**Task 2.3.3: Add Time Difference Calculation**\n",
        "- Method: `diff_seconds(other_timestamp)` - difference in seconds\n",
        "- Method: `diff_hours(other_timestamp)` - difference in hours\n",
        "- Used for session duration calculation\n",
        "\n",
        "**Task 2.3.4: Write Unit Tests**\n",
        "- Test all format outputs\n",
        "- Test timezone conversions\n",
        "- Test time difference calculations\n",
        "\n",
        "### Verification Checklist:\n",
        "- [ ] All timestamp formats correct\n",
        "- [ ] Timezone handling works\n",
        "- [ ] Time differences calculated correctly\n",
        "- [ ] All unit tests pass\n",
        "\n",
        "---\n",
        "\n",
        "## Step 2.4: Configuration System\n",
        "\n",
        "### What This Does:\n",
        "Centralized configuration management using environment variables.\n",
        "Used for: Database connections, API keys, feature flags.\n",
        "\n",
        "### Implementation Tasks:\n",
        "\n",
        "**Task 2.4.1: Create Settings Class**\n",
        "- Location: `uba/config/settings.py`\n",
        "- Use Pydantic `BaseSettings` for automatic env loading\n",
        "- Define all configuration parameters with defaults\n",
        "\n",
        "**Task 2.4.2: Define Configuration Parameters**\n",
        "\n",
        "| Parameter | Type | Default | Description |\n",
        "|-----------|------|---------|-------------|\n",
        "| `database_url` | str | - | PostgreSQL connection string |\n",
        "| `dspm_database_url` | str | - | DSPM database connection |\n",
        "| `debug` | bool | False | Enable debug mode |\n",
        "| `log_level` | str | \"INFO\" | Logging level |\n",
        "| `model_path` | str | \"./models\" | Where to store ML models |\n",
        "| `baseline_days` | int | 30 | Days for behavior baseline |\n",
        "| `anomaly_threshold` | float | 0.05 | Anomaly detection threshold |\n",
        "| `risk_weights` | dict | {...} | Risk score component weights |\n",
        "\n",
        "**Task 2.4.3: Create Constants File**\n",
        "- Location: `uba/config/constants.py`\n",
        "- Define enums for fixed values\n",
        "- Define severity levels, status types, etc.\n",
        "\n",
        "**Task 2.4.4: Create .env File Template**\n",
        "- Document all environment variables\n",
        "- Provide example values\n",
        "- Include comments explaining each\n",
        "\n",
        "### Verification Checklist:\n",
        "- [ ] Settings load from environment\n",
        "- [ ] Defaults work when env not set\n",
        "- [ ] All constants defined as enums\n",
        "- [ ] Configuration accessible throughout app\n",
        "\n",
        "---\n",
        "\n",
        "# ðŸ”§ PHASE 3: DATA LAYER\n",
        "\n",
        "## Goal: Build the data extraction and loading infrastructure\n",
        "\n",
        "---\n",
        "\n",
        "## âš ï¸ CRITICAL: Understanding DSPM Data Limitations\n",
        "\n",
        "**DSPM is a Data Security Posture Management tool, NOT a SIEM or log aggregator.**\n",
        "\n",
        "DSPM tracks **DATA ACCESS**, not **USER AUTHENTICATION**. This means:\n",
        "- âŒ NO login/logout timestamps\n",
        "- âŒ NO session duration data\n",
        "- âŒ NO device/browser information\n",
        "- âŒ NO geographic location data\n",
        "- âŒ NO failed login attempts\n",
        "\n",
        "**For full UBA functionality, you will need EXTERNAL log sources (covered in Phase 11.5).**\n",
        "\n",
        "---\n",
        "\n",
        "## Step 3.1: DSPM Database Connector\n",
        "\n",
        "### What This Does:\n",
        "Connects to the DSPM PostgreSQL database to extract behavioral data.\n",
        "This is your PRIMARY data source for UBA **data access** analysis.\n",
        "\n",
        "### DSPM Tables Actually Available:\n",
        "\n",
        "| Table | Data Available | UBA Use Case |\n",
        "|-------|---------------|--------------|\n",
        "| `policy_audit_log` | User actions, timestamps, IPs, event types | âœ… Activity pattern analysis |\n",
        "| `access_controls` | User-asset permission mappings (`user_or_role`, `asset_name`, `role`, `access`) | âœ… Access pattern analysis |\n",
        "| `identity_mappings` | User metadata, department, role, `issue` field (string: 'Over-privileged', 'Stale access', etc.), `severity` | âœ… User profiling & labels |\n",
        "| `assets_details` | Asset sensitivity, data classification | âœ… Risk weighting |\n",
        "| `policy_violations` | Policy breach events (LABELED DATA!) | âœ… ML training labels |\n",
        "| `user_sessions` | DSPM application login sessions (`ip_address`, `user_agent`, `device_info`, `location`, `last_activity`) | âš ï¸ DSPM app sessions only - NOT enterprise-wide auth logs |\n",
        "\n",
        "### âŒ Tables That DO NOT Exist in DSPM:\n",
        "\n",
        "| Assumed Table | Reality | Alternative |\n",
        "|---------------|---------|-------------|\n",
        "| `enterprise_auth_logs` | âŒ Does NOT exist | Need external IdP/SSO/SIEM integration |\n",
        "| `audit_logs_trail` | âŒ Does NOT exist | Use `policy_audit_log` instead |\n",
        "| `file_metadata` | âš ï¸ Part of `assets_details` | Query assets_details |\n",
        "\n",
        "**âš ï¸ IMPORTANT NOTE ON `user_sessions`:**\n",
        "- DSPM's `user_sessions` table tracks **DSPM application logins only**\n",
        "- It does NOT contain enterprise-wide authentication logs (VPN, SSO, workstation logins)\n",
        "- For enterprise-wide login behavior analysis, you need external IdP/SIEM integration\n",
        "\n",
        "### Implementation Tasks:\n",
        "\n",
        "**Task 3.1.1: Create DSPMConnector Class**\n",
        "- Location: `uba/connectors/dspm_connector.py`\n",
        "- Initialize with database connection string\n",
        "- Use SQLAlchemy for database operations\n",
        "- Handle connection pooling\n",
        "\n",
        "**Task 3.1.2: Implement Policy Audit Extraction**\n",
        "- Method: `get_policy_audit_logs(user_id=None, days=30)` â†’ DataFrame\n",
        "- Extracts: timestamp, user_id, action, ip_address, status, event_type\n",
        "- Returns pandas DataFrame for analysis\n",
        "- This is your PRIMARY behavioral data source!\n",
        "\n",
        "**Task 3.1.3: Implement Access Control Extraction**\n",
        "- Method: `get_access_controls(user_id=None)` â†’ DataFrame\n",
        "- Extracts: `user_or_role`, `asset_name`, `role`, `access`, `created_at`, `updated_at`\n",
        "- Optional filter by user\n",
        "- **NOTE:** `access_controls` does NOT have `last_accessed` - use `created_at`/`updated_at` instead\n",
        "\n",
        "**Task 3.1.4: Implement Identity Mapping Extraction**\n",
        "- Method: `get_identity_mappings(active_only=True)` â†’ DataFrame\n",
        "- Extracts: `identity_id`, `display_name`, `email`, `department`, `issue`, `severity`\n",
        "- **NOTE:** Table is `identity_mappings` (plural with 's')\n",
        "- The `issue` field is a STRING containing values like 'Over-privileged', 'Stale access', 'Inactive user'\n",
        "- Used for user profiling\n",
        "\n",
        "**Task 3.1.5: Implement Labeled Data Extraction (For ML Training)**\n",
        "- Method: `get_labeled_anomalies()` â†’ DataFrame\n",
        "- Sources:\n",
        "  - `policy_violations` â†’ Confirmed security events (positive labels)\n",
        "  - `identity_mappings` WHERE `issue = 'Over-privileged'` â†’ Risk indicators\n",
        "  - `identity_mappings` WHERE `issue = 'Stale access'` â†’ Unused access (risk indicator)\n",
        "- **NOTE:** Staleness and overprivilege info is in `identity_mappings.issue` (not access_controls)\n",
        "- These are your ML training labels!\n",
        "\n",
        "**Task 3.1.6: Create Bulk Extraction Methods**\n",
        "- Method: `get_all_behavioral_data(days=30)` â†’ dict of DataFrames\n",
        "- Efficiently extracts all available data in one call\n",
        "- Used for batch processing\n",
        "\n",
        "**Task 3.1.7: Write Integration Tests**\n",
        "- Test connection to DSPM database\n",
        "- Test each extraction method\n",
        "- Verify data types and shapes\n",
        "\n",
        "### Verification Checklist:\n",
        "- [ ] Can connect to DSPM PostgreSQL\n",
        "- [ ] All extraction methods return valid DataFrames\n",
        "- [ ] Column names match expected schema\n",
        "- [ ] Date filtering works correctly\n",
        "- [ ] Integration tests pass\n",
        "\n",
        "---\n",
        "\n",
        "## Step 3.2: CSV Connector\n",
        "\n",
        "### What This Does:\n",
        "Loads data from CSV files (for testing, external log ingestion).\n",
        "Based on OpenUBA's dataset.py patterns.\n",
        "\n",
        "### OpenUBA Reference:\n",
        "File: `core/dataset.py` - CSV class\n",
        "\n",
        "### Implementation Tasks:\n",
        "\n",
        "**Task 3.2.1: Create CSVConnector Class**\n",
        "- Location: `uba/connectors/csv_connector.py`\n",
        "- Method: `load(file_path, delimiter, header)` â†’ CoreDataFrame\n",
        "- Handle various delimiters (comma, tab, space)\n",
        "\n",
        "**Task 3.2.2: Add Schema Validation**\n",
        "- Validate loaded data against expected schema\n",
        "- Report missing or extra columns\n",
        "- Handle data type conversions\n",
        "\n",
        "**Task 3.2.3: Add Batch Loading**\n",
        "- Method: `load_directory(folder_path)` â†’ list of CoreDataFrames\n",
        "- Load all CSV files from a folder\n",
        "- Optionally concatenate into single DataFrame\n",
        "\n",
        "**Task 3.2.4: Write Unit Tests**\n",
        "- Test with various CSV formats\n",
        "- Test delimiter handling\n",
        "- Test with malformed files (error handling)\n",
        "\n",
        "### Verification Checklist:\n",
        "- [ ] Loads CSV files correctly\n",
        "- [ ] Handles different delimiters\n",
        "- [ ] Schema validation works\n",
        "- [ ] Batch loading works\n",
        "- [ ] Error handling for bad files\n",
        "\n",
        "---\n",
        "\n",
        "## Step 3.3: Parquet Connector\n",
        "\n",
        "### What This Does:\n",
        "Loads data from Parquet files (efficient columnar format for big data).\n",
        "\n",
        "### Implementation Tasks:\n",
        "\n",
        "**Task 3.3.1: Create ParquetConnector Class**\n",
        "- Location: `uba/connectors/parquet_connector.py`\n",
        "- Method: `load(file_path)` â†’ CoreDataFrame\n",
        "- Use pandas or pyarrow for reading\n",
        "\n",
        "**Task 3.3.2: Add Column Selection**\n",
        "- Method: `load(file_path, columns=[...])` - load only needed columns\n",
        "- More efficient for large files\n",
        "\n",
        "**Task 3.3.3: Add Partition Support**\n",
        "- Handle partitioned Parquet datasets\n",
        "- Common in data lake scenarios\n",
        "\n",
        "**Task 3.3.4: Write Unit Tests**\n",
        "- Test basic Parquet loading\n",
        "- Test column selection\n",
        "- Test with partitioned data\n",
        "\n",
        "### Verification Checklist:\n",
        "- [ ] Loads Parquet files correctly\n",
        "- [ ] Column selection works\n",
        "- [ ] Partitioned data handled\n",
        "- [ ] Memory efficient for large files\n",
        "\n",
        "---\n",
        "\n",
        "## Step 3.4: CoreDataFrame Implementation\n",
        "\n",
        "### What This Does:\n",
        "Wrapper around pandas DataFrame providing UBA-specific operations.\n",
        "Based on OpenUBA's CoreDataFrame concept.\n",
        "\n",
        "### OpenUBA Reference:\n",
        "File: `core/dataset.py` - CoreDataFrame class, Parser class, DatasetSession class\n",
        "\n",
        "### DatasetSession Class (Critical for data loading):\n",
        "The `DatasetSession` manages a data loading session:\n",
        "\n",
        "```python\n",
        "# Key methods from OpenUBA DatasetSession:\n",
        "dataset_session = DatasetSession(\"csv\")  # or \"es\", \"parquet\"\n",
        "\n",
        "# Load CSV file\n",
        "dataset_session.read_csv(data_folder, folder, location_type, delimiter)\n",
        "\n",
        "# Load from Elasticsearch  \n",
        "dataset_session.read_es_index(host, query)\n",
        "\n",
        "# Get loaded data\n",
        "dataset_session.get_csv_dataset()  # Returns Dataset object\n",
        "dataset_session.get_csv_size()     # Returns (rows, columns) tuple\n",
        "\n",
        "# Access the actual DataFrame\n",
        "df = dataset_session.csv_dataset.dataframe  # pandas DataFrame\n",
        "```\n",
        "\n",
        "### Parser Class (Also in dataset.py):\n",
        "The `Parser` class handles splitting raw log records into structured fields.\n",
        "- Method: `split_record(record, sep)` - splits a log line by delimiter\n",
        "- Uses `@PriorSplitRecord` decorator for pre-processing\n",
        "- Essential for CSV/log file parsing\n",
        "\n",
        "### Implementation Tasks:\n",
        "\n",
        "**Task 3.4.1: Create CoreDataFrame Class**\n",
        "- Location: `uba/analytics/core_dataframe.py`\n",
        "- Wraps pandas DataFrame\n",
        "- Property: `data` - access underlying DataFrame\n",
        "- Property: `shape` - get (rows, columns)\n",
        "\n",
        "**Task 3.4.2: Add User ID Operations**\n",
        "- Method: `get_unique_users(id_column)` â†’ list\n",
        "- Method: `filter_by_user(user_id, id_column)` â†’ CoreDataFrame\n",
        "- Essential for per-user analysis\n",
        "\n",
        "**Task 3.4.3: Add Time Operations**\n",
        "- Method: `filter_by_date_range(start, end, date_column)` â†’ CoreDataFrame\n",
        "- Method: `group_by_day(date_column)` â†’ dict of CoreDataFrames\n",
        "- Essential for temporal analysis\n",
        "\n",
        "**Task 3.4.4: Add Statistics Methods**\n",
        "- Method: `describe()` â†’ summary statistics\n",
        "- Method: `null_report()` â†’ null value counts per column\n",
        "- Useful for data quality checks\n",
        "\n",
        "**Task 3.4.5: Write Unit Tests**\n",
        "- Test all methods with sample data\n",
        "- Test edge cases (empty DataFrame, missing columns)\n",
        "\n",
        "### Verification Checklist:\n",
        "- [ ] Wraps pandas DataFrame correctly\n",
        "- [ ] User operations work\n",
        "- [ ] Time operations work\n",
        "- [ ] Statistics methods work\n",
        "- [ ] All unit tests pass\n",
        "\n",
        "---\n",
        "Main interface for analytics engine to get data.\n",
        "## Step 3.5: Data Loader Service\n",
        "\n",
        "### What This Does:\n",
        "Orchestrates data loading from various sources.\n",
        "Main interface for analytics engine to get data.\n",
        "\n",
        "### OpenUBA Reference:\n",
        "File: `core/model.py` - ModelDataLoader enum and loading logic\n",
        "\n",
        "### Implementation Tasks:\n",
        "\n",
        "**Task 3.5.1: Create DataLoader Class**\n",
        "- Location: `uba/analytics/data_loader.py`\n",
        "- Method: `load(source_type, source_config)` â†’ CoreDataFrame\n",
        "- Delegates to appropriate connector\n",
        "\n",
        "**Task 3.5.2: Define Data Source Types**\n",
        "- Enum: DSPM_POSTGRES, LOCAL_CSV, LOCAL_PARQUET, ELASTICSEARCH\n",
        "- Each type maps to a connector\n",
        "\n",
        "**Task 3.5.3: Create Data Source Configuration**\n",
        "- Define config schema for each source type\n",
        "- Validate configuration before loading\n",
        "\n",
        "**Task 3.5.4: Add Caching Layer**\n",
        "- Cache loaded data to avoid repeated queries\n",
        "- Time-based cache expiration\n",
        "- Memory-aware caching\n",
        "\n",
        "**Task 3.5.5: Write Integration Tests**\n",
        "- Test loading from each source type\n",
        "- Test caching behavior\n",
        "- Test error handling\n",
        "- [ ] Error handling is robust\n",
        "### Verification Checklist:\n",
        "- [ ] Can load from all source types\n",
        "- [ ] Configuration validation works\n",
        "- [ ] Caching reduces repeated loads\n",
        "- [ ] Error handling is robust\n",
        "## Goal: Implement user profile and behavior baseline tracking\n",
        "\n",
        "---\n",
        "# ðŸ”§ PHASE 4: USER & ENTITY MANAGEMENT\n",
        "## Step 4.1: User Profile Service\n",
        "## Goal: Implement user profile and behavior baseline tracking\n",
        "\n",
        "---\n",
        "Core of the UBA system - tracks WHO we're analyzing.\n",
        "## Step 4.1: User Profile Service\n",
        "\n",
        "### What This Does:\n",
        "Manages user profiles including behavior baselines and risk scores.\n",
        "Core of the UBA system - tracks WHO we're analyzing.\n",
        "\n",
        "### OpenUBA Reference:\n",
        "File: `core/user.py` - User, UserSet, ExtractAllUsersCSV\n",
        "\n",
        "### âš ï¸ OpenUBA User Extraction Pattern (IMPORTANT):\n",
        "\n",
        "OpenUBA uses `ExtractAllUsersCSV.get()` which is a **static method**:\n",
        "\n",
        "```python\n",
        "# From OpenUBA core/user.py\n",
        "@staticmethod\n",
        "def get(log_dataset_session: DatasetSession, log_metadata_obj: dict) -> UserSet:\n",
        "    # 1. Extract unique usernames from data\n",
        "    extracted_users = ExtractAllUsersCSV.extract_users(dataset_session, log_metadata_obj)\n",
        "    \n",
        "    # 2. Convert list to UserSet\n",
        "    user_set = ExtractAllUsersCSV.from_raw_list(extracted_users)\n",
        "    \n",
        "    # 3. Save discovered users\n",
        "    WriteUserSet.write(user_set)\n",
        "    return user_set\n",
        "\n",
        "@staticmethod\n",
        "def extract_users(dataset_session, log_metadata_obj) -> List:\n",
        "    # Get DataFrame from session\n",
        "    df = dataset_session.get_csv_dataset().get_dataframe().data\n",
        "    \n",
        "    # Use id_feature from config (e.g., \"cs-username\")\n",
        "    id_column = log_metadata_obj[\"id_feature\"]\n",
        "    \n",
        "    # Get unique values, fill NaN with \"NA\"\n",
        "    user_set = np.unique(df[id_column].fillna(\"NA\"))\n",
        "    return user_set\n",
        "```\n",
        "\n",
        "### Implementation Tasks:\n",
        "\n",
        "**Task 4.1.1: Create UserProfile Model**\n",
        "- This should already exist in uba_models.py from Phase 1\n",
        "- Fields: user_id, first_seen, last_seen, baseline_data, risk_score\n",
        "\n",
        "**Task 4.1.2: Create ProfileService Class**\n",
        "- Location: `uba/services/profile_service.py`\n",
        "- Initialize with database session\n",
        "- Manages all user profile operations\n",
        "\n",
        "**Task 4.1.3: Implement User Discovery**\n",
        "- Method: `discover_users_from_dspm()` â†’ list of user_ids\n",
        "- Queries DSPM user_sessions table\n",
        "- Returns all unique users\n",
        "\n",
        "**Task 4.1.4: Implement Profile Creation**\n",
        "- Method: `create_profile(user_id)` â†’ UserProfile\n",
        "- Creates new profile with empty baseline\n",
        "- Sets first_seen timestamp\n",
        "\n",
        "**Task 4.1.5: Implement Profile Retrieval**\n",
        "- Method: `get_profile(user_id)` â†’ UserProfile or None\n",
        "- Method: `get_all_profiles()` â†’ list of UserProfiles\n",
        "- Method: `get_profiles_by_risk(min_score)` â†’ list of UserProfiles\n",
        "\n",
        "**Task 4.1.6: Implement Profile Update**\n",
        "- Method: `update_baseline(user_id, baseline_data)`\n",
        "- Method: `update_risk_score(user_id, score, components)`\n",
        "- Both update last_seen timestamp\n",
        "\n",
        "**Task 4.1.7: Write Unit Tests**\n",
        "- Test CRUD operations\n",
        "- Test with mock database\n",
        "- Test concurrent updates\n",
        "\n",
        "### Verification Checklist:\n",
        "- [ ] User discovery finds all DSPM users\n",
        "- [ ] Profile CRUD operations work\n",
        "- [ ] Baseline data stored correctly as JSON\n",
        "- [ ] Risk score updates tracked\n",
        "- [ ] All unit tests pass\n",
        "\n",
        "---\n",
        "Baselines are the \"normal\" patterns we compare against.\n",
        "## Step 4.2: Behavior Baseline Storage\n",
        "### Implementation Tasks:\n",
        "### What This Does:\n",
        "Stores and manages behavioral baselines for each user.\n",
        "Baselines are the \"normal\" patterns we compare against.\n",
        "\n",
        "### Implementation Tasks:\n",
        "\n",
        "**Task 4.2.1: Define Baseline Data Structure**\n",
        "```\n",
        "baseline_data = {\n",
        "    \"login\": {\n",
        "        \"typical_hours\": [9, 10, 11, 12, 13, 14, 15, 16, 17],\n",
        "        \"typical_ips\": [\"192.168.1.x\", \"10.0.0.x\"],\n",
        "        \"typical_devices\": [\"Windows-Chrome\", \"MacOS-Safari\"],\n",
        "        \"avg_sessions_per_day\": 2.5,\n",
        "        \"std_sessions_per_day\": 1.2\n",
        "    },\n",
        "    \"access\": {\n",
        "        \"typical_assets\": [\"asset_1\", \"asset_2\", \"asset_3\"],\n",
        "        \"avg_assets_per_day\": 5,\n",
        "        \"typical_roles\": [\"viewer\", \"editor\"]\n",
        "    },\n",
        "    \"data\": {\n",
        "        \"avg_files_accessed\": 10,\n",
        "        \"avg_data_volume_mb\": 50,\n",
        "        \"typical_file_types\": [\".pdf\", \".xlsx\", \".docx\"]\n",
        "    },\n",
        "    \"computed_at\": \"2026-01-07T12:00:00Z\",\n",
        "    \"window_days\": 30\n",
        "}\n",
        "```\n",
        "\n",
        "**Task 4.2.2: Create BaselineManager Class**\n",
        "- Location: Part of `profile_service.py` or separate file\n",
        "- Method: `compute_baseline(user_id, days=30)` â†’ baseline_data\n",
        "- Uses feature extraction (Phase 5) to compute\n",
        "\n",
        "**Task 4.2.3: Implement Baseline Comparison**\n",
        "- Method: `compare_to_baseline(user_id, current_features)` â†’ deviations\n",
        "- Returns dict of how current behavior differs from baseline\n",
        "- Used by anomaly detection\n",
        "\n",
        "**Task 4.2.4: Implement Baseline Refresh**\n",
        "- Method: `refresh_baseline(user_id)` - recompute from recent data\n",
        "- Method: `refresh_all_baselines()` - batch refresh for all users\n",
        "- Should run on schedule (daily/weekly)\n",
        "\n",
        "**Task 4.2.5: Write Unit Tests**\n",
        "- Test baseline computation\n",
        "- Test comparison logic\n",
        "- Test refresh operations\n",
        "\n",
        "### Verification Checklist:\n",
        "- [ ] Baseline structure defined and documented\n",
        "- [ ] Compute creates valid baselines\n",
        "- [ ] Comparison identifies deviations\n",
        "- [ ] Refresh updates baselines correctly\n",
        "- [ ] All unit tests pass\n",
        "\n",
        "---\n",
        "Tracks what resources users interact with.\n",
        "## Step 4.3: Entity Service\n",
        "\n",
        "### What This Does:\n",
        "Manages non-user entities (assets, systems, services).\n",
        "Tracks what resources users interact with.\n",
        "\n",
        "### OpenUBA Reference:\n",
        "File: `core/entity.py` - Entity class (partial implementation)\n",
        "- Fields: entity_id, entity_type, name, metadata, first_seen\n",
        "### Implementation Tasks:\n",
        "\n",
        "**Task 4.3.1: Create Entity Model**\n",
        "- Add to uba_models.py if not present\n",
        "- Fields: entity_id, entity_type, name, metadata, first_seen\n",
        "\n",
        "**Task 4.3.2: Create EntityService Class**\n",
        "- Location: `uba/services/entity_service.py`\n",
        "- Manages entity discovery and tracking\n",
        "\n",
        "**Task 4.3.3: Implement Entity Discovery**\n",
        "- Method: `discover_entities_from_dspm()` â†’ list of entities\n",
        "- Sources: assets_details, file_metadata, table_metadata tables\n",
        "- Extracts unique assets, files, tables\n",
        "\n",
        "**Task 4.3.4: Implement Entity-User Mapping**\n",
        "- Method: `get_users_for_entity(entity_id)` â†’ list of user_ids\n",
        "- Method: `get_entities_for_user(user_id)` â†’ list of entity_ids\n",
        "- Based on access_controls table\n",
        "\n",
        "**Task 4.3.5: Implement Entity Risk**\n",
        "- Method: `compute_entity_risk(entity_id)` â†’ score\n",
        "- Based on: sensitivity level, number of users, violation count\n",
        "- High-risk entities need more monitoring\n",
        "\n",
        "**Task 4.3.6: Write Unit Tests**\n",
        "- Test discovery\n",
        "- Test mappings\n",
        "- Test risk calculation\n",
        "- [ ] All unit tests pass\n",
        "### Verification Checklist:\n",
        "- [ ] Entity discovery works\n",
        "- [ ] User-entity mappings accurate\n",
        "- [ ] Entity risk calculated\n",
        "- [ ] All unit tests pass\n",
        "\n",
        "---\n",
        "\n",
        "## ðŸ“ END OF PHASE 4\n",
        "\n",
        "### What You Have After Phase 4:\n",
        "1. âœ… All utility functions (hash, encode, timestamp)\n",
        "2. âœ… Configuration system\n",
        "3. âœ… Data connectors (DSPM, CSV, Parquet)\n",
        "4. âœ… CoreDataFrame for data manipulation\n",
        "5. âœ… User profile management\n",
        "6. âœ… Behavior baseline storage\n",
        "7. âœ… Entity management\n",
        "\n",
        "### How to Test Phases 2-4 Complete:\n",
        "\n",
        "```\n",
        "# Test utilities\n",
        "from uba.utils.hash_utils import HashData\n",
        "assert len(HashData(b\"test\").result) == 64\n",
        "assert sessions.shape[0] > 0\n",
        "# Test DSPM connector\n",
        "from uba.connectors.dspm_connector import DSPMConnector\n",
        "conn = DSPMConnector(settings.dspm_database_url)\n",
        "sessions = conn.get_user_sessions(days=7)\n",
        "assert sessions.shape[0] > 0\n",
        "\n",
        "\n",
        "# Test profile service\n",
        "```\n",
        "```\n",
        "\n",
        "from uba.services.profile_service import ProfileService\n",
        "# Test profile serviceassert len(users) > 0\n",
        "\n",
        "svc = ProfileService(db_session)\n",
        "from uba.services.profile_service import ProfileService```\n",
        "\n",
        "users = svc.discover_users_from_dspm()\n",
        "svc = ProfileService(db_session)users = svc.discover_users_from_dspm()\n",
        "assert len(users) > 0"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7a4397fe",
      "metadata": {
        "id": "7a4397fe"
      },
      "source": [
        "---\n",
        "\n",
        "# ðŸ§  PHASE 5: FEATURE EXTRACTION\n",
        "\n",
        "## Goal: Extract meaningful behavioral features from raw data for ML models\n",
        "\n",
        "---\n",
        "\n",
        "## What is Feature Extraction?\n",
        "\n",
        "```\n",
        "RAW DATA                          FEATURES                        ML MODEL\n",
        "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
        "â”‚ Login: 9:00 AM  â”‚              â”‚ login_hour: 9   â”‚              â”‚                 â”‚\n",
        "â”‚ Login: 9:15 AM  â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â–º   â”‚ login_count: 5  â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â–º   â”‚  Anomaly Score  â”‚\n",
        "â”‚ Login: 9:30 AM  â”‚   EXTRACT    â”‚ unique_ips: 2   â”‚   PREDICT    â”‚     0.85        â”‚\n",
        "â”‚ Login: 2:00 AM  â”‚              â”‚ hour_std: 2.1   â”‚              â”‚                 â”‚\n",
        "â”‚ Login: 9:05 AM  â”‚              â”‚ off_hours: 1    â”‚              â”‚                 â”‚\n",
        "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
        "        â–²                                â–²                                â–²\n",
        "   Raw events               Numerical features              Anomaly detection\n",
        "   from DSPM                for ML input                    output\n",
        "```\n",
        "\n",
        "Features are NUMERICAL VALUES that ML models can understand.\n",
        "Raw logs are text - models need numbers!\n",
        "\n",
        "---\n",
        "\n",
        "## Step 5.1: Feature Extractor Base Class\n",
        "\n",
        "### What This Does:\n",
        "Provides common interface for all feature extractors.\n",
        "\n",
        "### Implementation Tasks:\n",
        "\n",
        "**Task 5.1.1: Create Base FeatureExtractor Class**\n",
        "- Location: `uba/analytics/feature_extractor.py`\n",
        "- Abstract base class that all extractors inherit from\n",
        "- Defines common interface\n",
        "\n",
        "**Task 5.1.2: Define Common Methods**\n",
        "- Method: `extract(user_id, data)` â†’ dict of features (ABSTRACT)\n",
        "- Method: `get_feature_names()` â†’ list of feature names\n",
        "- Method: `validate_features(features)` â†’ bool\n",
        "\n",
        "**Task 5.1.3: Add Feature Normalization**\n",
        "- Method: `normalize(features)` â†’ normalized features\n",
        "- Scales features to 0-1 range\n",
        "- Important for ML models to work correctly\n",
        "\n",
        "**Task 5.1.4: Add Feature Storage**\n",
        "- Method: `cache_features(user_id, features)` - save to database\n",
        "- Method: `get_cached_features(user_id)` - retrieve from database\n",
        "- Avoids recomputing same features\n",
        "\n",
        "### Verification Checklist:\n",
        "- [ ] Base class defined with abstract methods\n",
        "- [ ] Normalization produces 0-1 range values\n",
        "- [ ] Feature caching works\n",
        "- [ ] Subclasses can inherit properly\n",
        "\n",
        "---\n",
        "\n",
        "## Step 5.2: Login Feature Extractor\n",
        "\n",
        "### âœ… DSPM HAS LOGIN DATA! (DSPM Application Sessions Only)\n",
        "\n",
        "**âš ï¸ IMPORTANT CLARIFICATION:**\n",
        "DSPM's `user_sessions` table tracks **logins to the DSPM application itself**, NOT enterprise-wide authentication (VPN, SSO, workstation logins). This is useful for DSPM app usage analysis but NOT for enterprise-wide login behavior analysis.\n",
        "\n",
        "**DSPM provides `user_sessions` table with login data:**\n",
        "- `ip_address` - Client IP (for location features)\n",
        "- `user_agent` - Browser/device info (for device features)\n",
        "- `created_at` - Login timestamp (for timing features)\n",
        "- `last_activity` - Session activity (for duration features)\n",
        "- `device_info` (JSONB) - Detailed device metadata\n",
        "- `location` (JSONB) - Geolocation data\n",
        "\n",
        "**For Enterprise-Wide Login Analysis:**\n",
        "- Integrate with Identity Provider (Okta, Azure AD, etc.)\n",
        "- Integrate with SIEM (Splunk, Elastic, etc.)\n",
        "- Use DSPM `user_sessions` as a supplementary data source\n",
        "\n",
        "### Data Source Options:\n",
        "| Source | Integration | Priority |\n",
        "|--------|-------------|----------|\n",
        "| DSPM `user_sessions` table | Direct SQL query | âœ… **Primary** |\n",
        "| Identity Provider (Okta, Azure AD) | API connector | Optional enhancement |\n",
        "| SIEM (Splunk, Elastic) | API connector | Optional enhancement |\n",
        "\n",
        "**For MVP:**\n",
        "- Use DSPM `user_sessions` table directly\n",
        "- Optional: Enrich with IdP data for additional context\n",
        "\n",
        "### What This Does:\n",
        "Extracts behavioral features from user login patterns.\n",
        "Detects: unusual login times, new locations, new devices.\n",
        "\n",
        "### Data Source:\n",
        "**DSPM table**: `user_sessions` (in `app/models/user.py`)\n",
        "âœ… **This extractor CAN use DSPM data directly!**\n",
        "\n",
        "### Features to Extract:\n",
        "\n",
        "| Feature Name | Formula | What It Detects |\n",
        "|--------------|---------|-----------------|\n",
        "| `login_count_daily` | COUNT(logins) / days | Activity level |\n",
        "| `login_hour_mean` | AVG(hour of login) | Typical work hours |\n",
        "| `login_hour_std` | STDDEV(hour of login) | Schedule consistency |\n",
        "| `unique_ips_30d` | COUNT(DISTINCT ip_address) | Location variety |\n",
        "| `unique_devices_30d` | COUNT(DISTINCT device_info) | Device variety |\n",
        "| `unique_locations_30d` | COUNT(DISTINCT location) | Travel patterns |\n",
        "| `off_hours_ratio` | logins_outside_9to5 / total | After-hours access |\n",
        "| `weekend_ratio` | weekend_logins / total | Weekend activity |\n",
        "| `session_duration_mean` | AVG(session_length) | Work duration |\n",
        "| `session_duration_std` | STDDEV(session_length) | Duration consistency |\n",
        "| `failed_login_ratio` | failed / total_attempts | Authentication issues |\n",
        "| `new_ip_flag` | 1 if IP not in baseline | New location alert |\n",
        "| `new_device_flag` | 1 if device not in baseline | New device alert |\n",
        "| `location_entropy` | -Î£(p * log(p)) | Location randomness |\n",
        "\n",
        "### Implementation Tasks:\n",
        "\n",
        "**Task 5.2.1: Create LoginFeatureExtractor Class**\n",
        "- Location: `uba/analytics/feature_extractor.py` (or separate file)\n",
        "- Inherits from base FeatureExtractor\n",
        "- Takes DSPM `user_sessions` DataFrame as input\n",
        "\n",
        "**Task 5.2.2: Implement Time-based Features**\n",
        "- Extract hour from login timestamps\n",
        "- Calculate mean, std, off-hours ratio\n",
        "- Handle timezone conversions\n",
        "\n",
        "**Task 5.2.3: Implement Device/Location Features**\n",
        "- Count unique IPs, devices, locations\n",
        "- Calculate entropy for randomness\n",
        "- Compare against baseline for flags\n",
        "\n",
        "**Task 5.2.4: Implement Session Features**\n",
        "- Calculate session durations\n",
        "- Compute mean and std\n",
        "- Identify unusually long/short sessions\n",
        "\n",
        "**Task 5.2.5: Write Unit Tests**\n",
        "- Test with known data, verify calculations\n",
        "- Test edge cases (single login, no data)\n",
        "- Test normalization\n",
        "\n",
        "### Verification Checklist:\n",
        "- [ ] DSPM `user_sessions` query working\n",
        "- [ ] All 14 features extracted correctly\n",
        "- [ ] Time calculations handle timezones\n",
        "- [ ] Entropy calculation correct\n",
        "- [ ] Baseline comparison works\n",
        "- [ ] All unit tests pass\n",
        "\n",
        "---\n",
        "\n",
        "## Step 5.3: Access Feature Extractor\n",
        "\n",
        "### What This Does:\n",
        "Extracts features from data access patterns.\n",
        "Detects: unusual data access, privilege escalation, new assets.\n",
        "\n",
        "### Data Source:\n",
        "DSPM tables: `access_controls`, `policy_audit_log`, `assets_details`\n",
        "âœ… **This extractor can use DSPM data directly!**\n",
        "\n",
        "### Features to Extract:\n",
        "\n",
        "| Feature Name | Formula | What It Detects |\n",
        "|--------------|---------|-----------------|\n",
        "| `assets_accessed_daily` | COUNT(DISTINCT asset) / days | Access volume |\n",
        "| `unique_assets_30d` | COUNT(DISTINCT asset_name) | Asset variety |\n",
        "| `sensitive_asset_ratio` | sensitive_assets / total | Sensitive data access |\n",
        "| `new_asset_ratio` | new_assets / total | First-time access |\n",
        "| `access_hour_mean` | AVG(hour of access) | Typical access time |\n",
        "| `access_hour_std` | STDDEV(hour of access) | Time consistency |\n",
        "| `permission_level_max` | MAX(role_weight) | Highest privilege used |\n",
        "| `permission_variety` | COUNT(DISTINCT role) | Role diversity |\n",
        "| `read_write_ratio` | writes / reads | Write activity level |\n",
        "| `cross_department_ratio` | other_dept_assets / total | Cross-boundary access |\n",
        "| `policy_violation_count` | COUNT(violations) | Policy issues |\n",
        "| `access_burst_count` | sessions > 2*mean | Unusual spikes |\n",
        "\n",
        "### Implementation Tasks:\n",
        "\n",
        "**Task 5.3.1: Create AccessFeatureExtractor Class**\n",
        "- Inherits from base FeatureExtractor\n",
        "- Combines data from multiple tables\n",
        "\n",
        "**Task 5.3.2: Implement Volume Features**\n",
        "- Count assets per day\n",
        "- Calculate variety metrics\n",
        "- Track first-time access\n",
        "\n",
        "**Task 5.3.3: Implement Permission Features**\n",
        "- Map roles to numeric weights (viewer=1, editor=2, admin=3)\n",
        "- Track maximum and variety\n",
        "- Identify privilege escalation\n",
        "\n",
        "**Task 5.3.4: Implement Pattern Features**\n",
        "- Calculate read/write ratios\n",
        "- Identify cross-department access\n",
        "- Detect access bursts\n",
        "\n",
        "**Task 5.3.5: Write Unit Tests**\n",
        "- Test all 12 features\n",
        "- Test with multi-table joins\n",
        "- Test edge cases\n",
        "\n",
        "### Verification Checklist:\n",
        "- [ ] All 12 features extracted correctly\n",
        "- [ ] Multi-table join works\n",
        "- [ ] Permission weighting applied\n",
        "- [ ] Burst detection works\n",
        "- [ ] All unit tests pass\n",
        "\n",
        "---\n",
        "\n",
        "## Step 5.4: Data Transfer Feature Extractor\n",
        "\n",
        "### What This Does:\n",
        "Extracts features from data movement patterns.\n",
        "Detects: data exfiltration, unusual downloads, bulk transfers.\n",
        "\n",
        "### Data Source:\n",
        "DSPM tables: `assets_details`, `policy_audit_log`\n",
        "âš ï¸ **Limited data available** - DSPM tracks access, not detailed file transfers\n",
        "\n",
        "### Features to Extract:\n",
        "\n",
        "| Feature Name | Formula | What It Detects | DSPM Available? |\n",
        "|--------------|---------|-----------------|-----------------|\n",
        "| `files_accessed_daily` | COUNT(files) / days | File activity | âš ï¸ Limited |\n",
        "| `data_volume_daily_mb` | SUM(file_size) / days / 1024^2 | Data volume | âš ï¸ Limited |\n",
        "| `data_volume_std` | STDDEV(daily_volume) | Volume consistency | âš ï¸ Limited |\n",
        "| `large_file_count` | COUNT(files > 100MB) | Large file access | âŒ No |\n",
        "| `file_type_variety` | COUNT(DISTINCT extension) | File type diversity | âŒ No |\n",
        "| `sensitive_file_ratio` | sensitive_files / total | Sensitive data | âœ… Yes |\n",
        "| `external_transfer_count` | COUNT(external destinations) | External sends | âŒ No |\n",
        "| `download_upload_ratio` | downloads / uploads | Download heavy? | âŒ No |\n",
        "| `bulk_operation_count` | operations > 10 files/hour | Bulk activity | âš ï¸ Limited |\n",
        "| `after_hours_transfer` | off_hours_transfers / total | Night transfers | âœ… Yes |\n",
        "| `new_file_type_flag` | 1 if new extension | New file types | âŒ No |\n",
        "| `volume_spike_flag` | 1 if volume > 3*baseline | Volume spike | âš ï¸ Limited |\n",
        "\n",
        "### âš ï¸ Note on Data Availability:\n",
        "Many transfer features require DLP or file-level monitoring data.\n",
        "DSPM provides asset-level access, not file-level transfer details.\n",
        "For full transfer monitoring, consider integrating with DLP tools.\n",
        "\n",
        "### Implementation Tasks:\n",
        "\n",
        "**Task 5.4.1: Create DataTransferFeatureExtractor Class**\n",
        "- Inherits from base FeatureExtractor\n",
        "- Focuses on file/data movement patterns\n",
        "\n",
        "**Task 5.4.2: Implement Volume Features**\n",
        "- Calculate daily volumes\n",
        "- Track large files\n",
        "- Compute statistics\n",
        "\n",
        "**Task 5.4.3: Implement Transfer Direction Features**\n",
        "- Identify external vs internal\n",
        "- Calculate download/upload ratios\n",
        "- Track bulk operations\n",
        "\n",
        "**Task 5.4.4: Implement Anomaly Flags**\n",
        "- Compare to baseline\n",
        "- Set flags for spikes\n",
        "- Detect new patterns\n",
        "\n",
        "**Task 5.4.5: Write Unit Tests**\n",
        "- Test all 12 features\n",
        "- Test volume calculations\n",
        "- Test flag logic\n",
        "\n",
        "### Verification Checklist:\n",
        "- [ ] All 12 features extracted correctly\n",
        "- [ ] Volume calculations accurate\n",
        "- [ ] External detection works\n",
        "- [ ] Spike detection works\n",
        "- [ ] All unit tests pass\n",
        "\n",
        "---\n",
        "\n",
        "## Step 5.5: Time Feature Extractor\n",
        "\n",
        "### What This Does:\n",
        "Extracts time-based behavioral patterns.\n",
        "Detects: schedule changes, unusual timing, pattern breaks.\n",
        "\n",
        "### Data Source:\n",
        "All DSPM behavioral tables (using timestamps)\n",
        "\n",
        "### Features to Extract:\n",
        "\n",
        "| Feature Name | Formula | What It Detects |\n",
        "|--------------|---------|-----------------|\n",
        "| `activity_hours` | list of active hours | Work schedule |\n",
        "| `peak_hour` | MODE(activity_hour) | Most active time |\n",
        "| `activity_span` | MAX(hour) - MIN(hour) | Work day length |\n",
        "| `weekend_activity_ratio` | weekend / total | Weekend work |\n",
        "| `night_activity_ratio` | 10pm-6am / total | Night activity |\n",
        "| `monday_activity` | monday_count / total | Start of week |\n",
        "| `friday_activity` | friday_count / total | End of week |\n",
        "| `activity_regularity` | 1 - (std / mean) | Schedule consistency |\n",
        "| `gap_max_hours` | MAX(time between activities) | Longest break |\n",
        "| `session_time_mean` | AVG(session_duration) | Typical session |\n",
        "| `first_activity_hour` | MODE(first daily activity) | Start time |\n",
        "| `last_activity_hour` | MODE(last daily activity) | End time |\n",
        "\n",
        "### Implementation Tasks:\n",
        "\n",
        "**Task 5.5.1: Create TimeFeatureExtractor Class**\n",
        "- Inherits from base FeatureExtractor\n",
        "- Analyzes temporal patterns\n",
        "\n",
        "**Task 5.5.2: Implement Hour Distribution**\n",
        "- Calculate activity per hour\n",
        "- Find peak hours\n",
        "- Compute span\n",
        "\n",
        "**Task 5.5.3: Implement Day-of-Week Patterns**\n",
        "- Calculate per-day ratios\n",
        "- Identify weekend patterns\n",
        "- Track week patterns\n",
        "\n",
        "**Task 5.5.4: Implement Regularity Metrics**\n",
        "- Calculate activity gaps\n",
        "- Compute regularity score\n",
        "- Track schedule changes\n",
        "\n",
        "**Task 5.5.5: Write Unit Tests**\n",
        "- Test all 12 features\n",
        "- Test with various schedules\n",
        "- Test edge cases\n",
        "\n",
        "### Verification Checklist:\n",
        "- [ ] All 12 features extracted correctly\n",
        "- [ ] Hour calculations correct\n",
        "- [ ] Day-of-week patterns work\n",
        "- [ ] Regularity metric meaningful\n",
        "- [ ] All unit tests pass\n",
        "\n",
        "---\n",
        "\n",
        "## Step 5.6: Feature Pipeline\n",
        "\n",
        "### What This Does:\n",
        "Orchestrates all feature extractors into a single pipeline.\n",
        "\n",
        "### Implementation Tasks:\n",
        "\n",
        "**Task 5.6.1: Create FeaturePipeline Class**\n",
        "- Location: `uba/analytics/feature_extractor.py`\n",
        "- Combines all extractors\n",
        "- Method: `extract_all_features(user_id)` â†’ dict\n",
        "\n",
        "**Task 5.6.2: Define Feature Groups**\n",
        "- Create a mapping of feature group names to their extractor classes\n",
        "- Groups: \"login\", \"access\", \"data_transfer\", \"time\"\n",
        "- Each group maps to its corresponding FeatureExtractor class\n",
        "- This allows the pipeline to dynamically load and use extractors\n",
        "\n",
        "**Task 5.6.3: Implement Batch Extraction**\n",
        "- Method: `extract_for_all_users()` â†’ DataFrame\n",
        "- Efficiently processes all users\n",
        "- Returns features as rows\n",
        "\n",
        "**Task 5.6.4: Add Feature Selection**\n",
        "- Method: `select_features(feature_names)` - use only specific features\n",
        "- Useful for model-specific feature sets\n",
        "\n",
        "**Task 5.6.5: Write Integration Tests**\n",
        "- Test full pipeline\n",
        "- Test batch extraction\n",
        "- Test feature selection\n",
        "\n",
        "### Verification Checklist:\n",
        "- [ ] Pipeline combines all extractors\n",
        "- [ ] Batch extraction works\n",
        "- [ ] Feature selection works\n",
        "- [ ] No missing values in output\n",
        "- [ ] All integration tests pass\n",
        "\n",
        "---\n",
        "\n",
        "# ðŸ“Š PHASE 5.5: TRAINING DATA PREPARATION\n",
        "\n",
        "## Goal: Prepare labeled and unlabeled data for ML model training\n",
        "\n",
        "---\n",
        "\n",
        "## âš ï¸ CRITICAL: Understanding Your Data Situation\n",
        "\n",
        "**Most UBA data is UNLABELED!**\n",
        "\n",
        "Unlike supervised learning problems (spam detection, image classification),\n",
        "behavioral anomalies are rarely labeled. You won't have a dataset of\n",
        "\"1000 normal users and 100 malicious users.\"\n",
        "\n",
        "### What You Have:\n",
        "\n",
        "| Data Type | Source | Label Status | Volume |\n",
        "|-----------|--------|--------------|--------|\n",
        "| Normal behavior | DSPM `policy_audit_log` | Assumed normal | High |\n",
        "| Policy violations | DSPM `policy_violations` | âœ… LABELED (anomaly) | Low |\n",
        "| Over-privileged users | DSPM `identity_mappings` WHERE `issue = 'Over-privileged'` | âœ… LABELED (risk) | Medium |\n",
        "| Stale access | DSPM `identity_mappings` WHERE `issue = 'Stale access'` | âœ… LABELED (risk) | Medium |\n",
        "\n",
        "---\n",
        "\n",
        "## Step 5.5.1: Identify Available Labels\n",
        "\n",
        "### From DSPM Database:\n",
        "\n",
        "**Positive Labels (Anomalies):**\n",
        "- Query the `policy_violations` table for confirmed security events\n",
        "- Filter by status = 'confirmed' to get verified anomalies\n",
        "- Extract: user_id, timestamp, violation_type, severity\n",
        "- Query `identity_mappings` table for users WHERE `issue = 'Over-privileged'`\n",
        "- These provide risk indicators for labeling\n",
        "\n",
        "**Weak Labels (Risk Indicators):**\n",
        "- Query `identity_mappings` table WHERE `issue = 'Stale access'`\n",
        "- This indicates potential security risks (unused permissions)\n",
        "- **NOTE:** Stale access info is in `identity_mappings.issue`, not in `access_controls`\n",
        "\n",
        "---\n",
        "\n",
        "## Step 5.5.2: Data Volume Requirements\n",
        "\n",
        "| Model Type | Minimum Records | Recommended | Time Period |\n",
        "|------------|-----------------|-------------|-------------|\n",
        "| Isolation Forest (unsupervised) | 1,000 | 10,000+ | 30 days |\n",
        "| Statistical Baseline | 500 per user | 2,000 per user | 60-90 days |\n",
        "| One-Class SVM | 5,000 | 50,000+ | 90 days |\n",
        "| Deep Learning | 100,000 | 500,000+ | 6 months |\n",
        "\n",
        "### Reality Check:\n",
        "- **If you have < 1,000 records**: Use rule-based detection only\n",
        "- **If you have 1,000-10,000 records**: Use Isolation Forest\n",
        "- **If you have 10,000+ records**: Consider One-Class SVM\n",
        "- **If you have 100,000+ records**: Consider deep learning\n",
        "\n",
        "---\n",
        "\n",
        "## Step 5.5.3: Labeling Strategy\n",
        "\n",
        "### Strategy 1: Use DSPM Labels Directly\n",
        "- Create a method in your DSPM connector to extract labeled anomalies\n",
        "- This method should return: user_id, timestamp, and label (1=anomaly, 0=normal)\n",
        "- Use policy_violations as positive labels (confirmed anomalies)\n",
        "\n",
        "### Strategy 2: Expert Rules as Weak Labels\n",
        "- Define a set of expert rules that indicate potential anomalies\n",
        "- Examples of weak label rules:\n",
        "  - If off_hours_ratio > 0.5 â†’ likely anomaly (confidence: 70%)\n",
        "  - If unique_ips_30d > 10 â†’ likely anomaly (confidence: 60%)\n",
        "  - If sensitive_asset_ratio > 0.8 â†’ likely anomaly (confidence: 80%)\n",
        "- These create \"weak labels\" for semi-supervised learning\n",
        "\n",
        "### Strategy 3: Unsupervised First, Label Later\n",
        "1. Train Isolation Forest on all data (unsupervised)\n",
        "2. Get top 5% anomaly scores\n",
        "3. Have security team review these\n",
        "4. Use reviewed cases as labels for supervised model\n",
        "\n",
        "---\n",
        "\n",
        "## Step 5.5.4: Train/Validation/Test Split\n",
        "\n",
        "### Visual Representation:\n",
        "```\n",
        "TOTAL DATA\n",
        "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
        "â”‚                                                         â”‚\n",
        "â”‚  TRAINING SET (70%)    VAL (15%)      TEST (15%)       â”‚\n",
        "â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤ â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤ â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤  â”‚\n",
        "â”‚                                                         â”‚\n",
        "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
        "\n",
        "Split by TIME, not randomly!\n",
        "- Training: Oldest 70% of data\n",
        "- Validation: Next 15%\n",
        "- Test: Most recent 15%\n",
        "\n",
        "This prevents data leakage from future to past.\n",
        "```\n",
        "\n",
        "### How to Implement Time-Based Split:\n",
        "1. Sort your dataframe by timestamp column (oldest first)\n",
        "2. Calculate the total number of records\n",
        "3. Training set: Take the first 70% of records\n",
        "4. Validation set: Take the next 15% of records\n",
        "5. Test set: Take the final 15% of records\n",
        "6. Return all three datasets separately\n",
        "\n",
        "**Important:** Never shuffle the data randomly - this causes data leakage where future information leaks into training data.\n",
        "\n",
        "---\n",
        "\n",
        "## Step 5.5.5: Handle Class Imbalance\n",
        "\n",
        "**Problem:** Anomalies are rare (1-5% of data)\n",
        "**Impact:** Model will predict \"normal\" for everything\n",
        "\n",
        "### Solutions:\n",
        "\n",
        "| Technique | When to Use | How |\n",
        "|-----------|-------------|-----|\n",
        "| **Contamination parameter** | Isolation Forest | Set `contamination=0.05` |\n",
        "| **Class weights** | Supervised models | Set `class_weight='balanced'` |\n",
        "| **SMOTE** | Small datasets | Generate synthetic anomalies |\n",
        "| **Threshold tuning** | Any model | Lower decision threshold |\n",
        "\n",
        "---\n",
        "\n",
        "## Step 5.5.6: Data Quality Checks\n",
        "\n",
        "### Before Training, Verify These Conditions:\n",
        "\n",
        "| Check | What to Verify | Minimum Requirement |\n",
        "|-------|---------------|---------------------|\n",
        "| Record count | Total number of data points | At least 1,000 records |\n",
        "| Null values | No missing values in features | Zero null values |\n",
        "| Date range | Sufficient time span covered | At least 30 days |\n",
        "| User diversity | Enough different users | At least 10 unique users |\n",
        "| Feature variance | Features have meaningful variation | Standard deviation > 0.01 for all features |\n",
        "\n",
        "### How to Perform Quality Checks:\n",
        "1. Count total records - must be >= 1,000\n",
        "2. Check for null values in all columns - must be zero\n",
        "3. Calculate date range (max - min timestamp) - must be >= 30 days\n",
        "4. Count unique users - must be >= 10\n",
        "5. Calculate standard deviation for each feature - must be > 0.01 (no constant features)\n",
        "\n",
        "If any check fails, investigate and fix before training. Constant features (std=0) provide no information and should be removed.\n",
        "\n",
        "### Verification Checklist:\n",
        "- [ ] Minimum data volume achieved (1,000+ records)\n",
        "- [ ] Labels identified and extracted\n",
        "- [ ] Time-based split created (no data leakage)\n",
        "- [ ] Class imbalance addressed\n",
        "- [ ] Data quality checks passed\n",
        "- [ ] Feature variance verified (no constant features)\n",
        "\n",
        "---\n",
        "\n",
        "# ðŸ¤– PHASE 6: ML MODEL DEVELOPMENT\n",
        "\n",
        "## Goal: Build, train, validate, and deploy ML models for anomaly detection\n",
        "\n",
        "---\n",
        "\n",
        "## ML Model Development Workflow\n",
        "\n",
        "```\n",
        "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
        "â”‚                        ML MODEL DEVELOPMENT LIFECYCLE                            â”‚\n",
        "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
        "â”‚                                                                                  â”‚\n",
        "â”‚   STEP 1: DATA PREPARATION                                                       â”‚\n",
        "â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                          â”‚\n",
        "â”‚   â”‚ Raw Data    â”‚ â†’ â”‚ Features    â”‚ â†’ â”‚ Train/Test  â”‚                          â”‚\n",
        "â”‚   â”‚ from DSPM   â”‚   â”‚ Extracted   â”‚   â”‚ Split       â”‚                          â”‚\n",
        "â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                          â”‚\n",
        "â”‚                                                                                  â”‚\n",
        "â”‚   STEP 2: MODEL TRAINING                                                         â”‚\n",
        "â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                          â”‚\n",
        "â”‚   â”‚ Choose      â”‚ â†’ â”‚ Train on    â”‚ â†’ â”‚ Validate    â”‚                          â”‚\n",
        "â”‚   â”‚ Algorithm   â”‚   â”‚ Train Set   â”‚   â”‚ on Val Set  â”‚                          â”‚\n",
        "â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                          â”‚\n",
        "â”‚                                                                                  â”‚\n",
        "â”‚   STEP 3: MODEL EVALUATION                                                       â”‚\n",
        "â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                          â”‚\n",
        "â”‚   â”‚ Test on     â”‚ â†’ â”‚ Calculate   â”‚ â†’ â”‚ Compare     â”‚                          â”‚\n",
        "â”‚   â”‚ Test Set    â”‚   â”‚ Metrics     â”‚   â”‚ to Baseline â”‚                          â”‚\n",
        "â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                          â”‚\n",
        "â”‚                                                                                  â”‚\n",
        "â”‚   STEP 4: MODEL DEPLOYMENT                                                       â”‚\n",
        "â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                          â”‚\n",
        "â”‚   â”‚ Save Model  â”‚ â†’ â”‚ Register    â”‚ â†’ â”‚ Deploy to   â”‚                          â”‚\n",
        "â”‚   â”‚ to Disk     â”‚   â”‚ in Registry â”‚   â”‚ Production  â”‚                          â”‚\n",
        "â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                          â”‚\n",
        "â”‚                                                                                  â”‚\n",
        "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## Step 6.1: Base Model Interface\n",
        "\n",
        "### What This Does:\n",
        "Defines the interface all UBA models must follow.\n",
        "Ensures consistency across all model types.\n",
        "\n",
        "### OpenUBA Reference:\n",
        "File: `core/model_library/*/MODEL.py` - all have `execute()` function\n",
        "\n",
        "### âš ï¸ CRITICAL: Two Model Patterns\n",
        "\n",
        "**Pattern 1: OpenUBA Function-Based (for compatibility)**\n",
        "```\n",
        "model_library/model_name/\n",
        "â”œâ”€â”€ __init__.py    # from .MODEL import execute\n",
        "â””â”€â”€ MODEL.py       # def execute() -> dict\n",
        "```\n",
        "\n",
        "**Pattern 2: UBA_PRO Class-Based (for ML models)**\n",
        "```python\n",
        "class BaseModel(ABC):\n",
        "    @abstractmethod\n",
        "    def train(self, X, y=None): pass\n",
        "    @abstractmethod\n",
        "    def predict(self, X): pass\n",
        "    @abstractmethod\n",
        "    def save(self, path): pass\n",
        "    @abstractmethod\n",
        "    def load(self, path): pass\n",
        "```\n",
        "\n",
        "**You need BOTH patterns - use a wrapper to bridge them!**\n",
        "\n",
        "### Implementation Tasks:\n",
        "\n",
        "**Task 6.1.1: Create BaseModel Abstract Class**\n",
        "- Location: `uba/analytics/model_library/base_model.py`\n",
        "- All UBA_PRO models inherit from this\n",
        "- Defines required class methods for ML workflow\n",
        "\n",
        "**Task 6.1.2: Define Required Class Methods (UBA_PRO Pattern)**\n",
        "- Every model MUST implement these abstract methods:\n",
        "  - `train(X_train, y_train=None)`: Train the model on data\n",
        "  - `predict(X)`: Make predictions on new data\n",
        "  - `save(path)`: Save model to disk\n",
        "  - `load(path)`: Load model from disk\n",
        "  - `get_feature_names()`: Return list of required features\n",
        "- These methods ensure all models have a consistent interface\n",
        "- The y_train parameter is optional (for unsupervised models)\n",
        "\n",
        "**Task 6.1.3: Create OpenUBA-Compatible Wrapper**\n",
        "- Every model folder MUST have:\n",
        "  - `MODEL.py` with `def execute() -> dict` function\n",
        "  - `__init__.py` with `from .MODEL import execute`\n",
        "- The `execute()` function wraps your class-based model:\n",
        "\n",
        "```python\n",
        "# MODEL.py - OpenUBA-compatible wrapper\n",
        "from .login_anomaly_model import LoginAnomalyModel\n",
        "\n",
        "_model_instance = None\n",
        "\n",
        "def execute(data=None) -> dict:\n",
        "    \\\"\\\"\\\"OpenUBA-compatible entry point for ModelEngine\\\"\\\"\\\"\n",
        "    global _model_instance\n",
        "    \n",
        "    # Lazy load model\n",
        "    if _model_instance is None:\n",
        "        _model_instance = LoginAnomalyModel()\n",
        "        _model_instance.load(\"saved_model.pkl\")\n",
        "    \n",
        "    # Run prediction\n",
        "    results = _model_instance.predict(data)\n",
        "    return {\"user_risks\": results}\n",
        "```\n",
        "\n",
        "**Task 6.1.4: Add Common Methods**\n",
        "- Method: `validate_input(X)` - check features present\n",
        "- Method: `get_model_info()` - return metadata\n",
        "- Method: `compute_metrics(y_true, y_pred)` - evaluation\n",
        "\n",
        "**Task 6.1.5: Add Model Registry Integration**\n",
        "- Method: `register(registry)` - add to registry\n",
        "- Method: `get_version()` - return version string\n",
        "\n",
        "**Task 6.1.6: Write Unit Tests**\n",
        "- Test abstract methods enforced\n",
        "- Test `execute()` wrapper works\n",
        "- Test inheritance\n",
        "\n",
        "### Verification Checklist:\n",
        "- [ ] Abstract class defined correctly\n",
        "- [ ] All required methods documented\n",
        "- [ ] `execute()` wrapper created for each model\n",
        "- [ ] `__init__.py` exports `execute` function\n",
        "- [ ] Registry integration works\n",
        "- [ ] Subclasses must implement abstract methods\n",
        "\n",
        "---\n",
        "\n",
        "## Step 6.2: Login Anomaly Model\n",
        "\n",
        "### âš ï¸ CRITICAL DATA REQUIREMENT\n",
        "\n",
        "| Data Type | Required | DSPM Availability | Solution |\n",
        "|-----------|----------|-------------------|----------|\n",
        "| Login timestamps | âœ… Yes | âŒ NOT AVAILABLE | External source required |\n",
        "| IP addresses | âœ… Yes | âŒ NOT AVAILABLE | External source required |\n",
        "| Device info | Recommended | âŒ NOT AVAILABLE | External source required |\n",
        "| Geo-location | Optional | âŒ NOT AVAILABLE | External source required |\n",
        "\n",
        "**âš ï¸ DSPM does NOT have login session data!**\n",
        "\n",
        "This model requires external authentication log integration (Phase 11.5).\n",
        "Until that integration is complete, you have two options:\n",
        "\n",
        "1. **SKIP this model** - Move to Access Anomaly Model (Step 6.3) which CAN work with DSPM data\n",
        "2. **Use synthetic data** - For development/testing only (not production)\n",
        "\n",
        "### What This Does:\n",
        "Detects anomalous login behavior using Isolation Forest.\n",
        "This is typically your FIRST and MOST IMPORTANT model - **IF you have login data**.\n",
        "\n",
        "### Algorithm: Isolation Forest\n",
        "- Unsupervised anomaly detection\n",
        "- Works by isolating observations\n",
        "- Anomalies are easier to isolate (shorter path)\n",
        "- No labels needed (perfect for starting out)\n",
        "\n",
        "### Features Used:\n",
        "From LoginFeatureExtractor (14 features)\n",
        "\n",
        "### Implementation Tasks:\n",
        "\n",
        "**Task 6.2.1: Create LoginAnomalyModel Class**\n",
        "- Location: `uba/analytics/model_library/login_anomaly.py`\n",
        "- Inherits from BaseModel\n",
        "- Uses sklearn IsolationForest\n",
        "\n",
        "**Task 6.2.2: Define Model Parameters**\n",
        "```\n",
        "DEFAULT_PARAMS = {\n",
        "    \"n_estimators\": 100,      # Number of trees\n",
        "    \"contamination\": 0.05,    # Expected anomaly rate (5%)\n",
        "    \"max_samples\": \"auto\",    # Samples per tree\n",
        "    \"random_state\": 42        # Reproducibility\n",
        "}\n",
        "```\n",
        "\n",
        "**Task 6.2.3: Implement train() Method**\n",
        "- Accept feature DataFrame\n",
        "- Validate features present\n",
        "- Fit IsolationForest\n",
        "- Store fitted model\n",
        "\n",
        "**Task 6.2.4: Implement predict() Method**\n",
        "- Accept new feature DataFrame\n",
        "- Return anomaly scores (-1 = anomaly, 1 = normal)\n",
        "- Also return probability scores (0-1)\n",
        "\n",
        "**Task 6.2.5: Implement save()/load() Methods**\n",
        "- Use joblib for model serialization\n",
        "- Save to specified path\n",
        "- Include metadata (version, date, features)\n",
        "\n",
        "**Task 6.2.6: Write Unit Tests**\n",
        "- Test training with sample data\n",
        "- Test prediction output format\n",
        "- Test save/load round-trip\n",
        "\n",
        "### Training Process:\n",
        "\n",
        "**TRAINING STEPS FOR LOGIN ANOMALY MODEL:**\n",
        "\n",
        "âš ï¸ PREREQUISITE: External authentication logs must be integrated! See Phase 11.5: External Log Integration\n",
        "\n",
        "**Step 1: COLLECT DATA**\n",
        "- Query from EXTERNAL source (not DSPM)\n",
        "- Options: Okta API, Azure AD logs, LDAP logs, SSO logs\n",
        "- Need minimum 1,500 sessions, 50 users\n",
        "\n",
        "**Step 2: EXTRACT FEATURES**\n",
        "- Use LoginFeatureExtractor\n",
        "- Get 14 features per user\n",
        "- Store as DataFrame\n",
        "\n",
        "**Step 3: PREPARE DATA**\n",
        "- Handle missing values (fill with median)\n",
        "- Normalize features to 0-1 range\n",
        "- No train/test split for unsupervised!\n",
        "\n",
        "**Step 4: TRAIN MODEL**\n",
        "- Create IsolationForest instance\n",
        "- Set contamination parameter (e.g., 0.05 for 5% anomaly rate)\n",
        "- Fit on all feature data\n",
        "- Model learns \"normal\" patterns\n",
        "\n",
        "**Step 5: VALIDATE MODEL**\n",
        "- Use DSPM labeled data (policy_violations)\n",
        "- Check if known anomalies get high scores\n",
        "- Compute precision/recall if labels available\n",
        "\n",
        "**Step 6: SAVE MODEL**\n",
        "- Serialize with joblib\n",
        "- Store in uba/models/login_anomaly_v1.pkl\n",
        "- Register in model registry\n",
        "\n",
        "### Verification Checklist:\n",
        "- [ ] Model trains without errors\n",
        "- [ ] Predictions return -1/1 values\n",
        "- [ ] Probability scores in 0-1 range\n",
        "- [ ] Save/load preserves model\n",
        "- [ ] Performance acceptable on labeled data\n",
        "\n",
        "---\n",
        "\n",
        "## Step 6.3: Access Anomaly Model\n",
        "\n",
        "### What This Does:\n",
        "Detects anomalous data access patterns.\n",
        "Identifies: unusual data access, first-time access to sensitive data.\n",
        "\n",
        "### Algorithm Options:\n",
        "- **Isolation Forest** (recommended to start)\n",
        "- **One-Class SVM** (alternative)\n",
        "- **Autoencoder** (for complex patterns)\n",
        "\n",
        "### Features Used:\n",
        "From AccessFeatureExtractor (12 features)\n",
        "\n",
        "### Implementation Tasks:\n",
        "\n",
        "**Task 6.3.1: Create AccessAnomalyModel Class**\n",
        "- Location: `uba/analytics/model_library/access_anomaly.py`\n",
        "- Inherits from BaseModel\n",
        "- Similar structure to LoginAnomalyModel\n",
        "\n",
        "**Task 6.3.2: Implement All Required Methods**\n",
        "- train(), predict(), save(), load()\n",
        "- Same pattern as LoginAnomalyModel\n",
        "\n",
        "**Task 6.3.3: Add Access-Specific Logic**\n",
        "- Weight sensitive asset access higher\n",
        "- Consider first-time access flags\n",
        "- Incorporate policy violation history\n",
        "\n",
        "**Task 6.3.4: Write Unit Tests**\n",
        "- Test with access pattern data\n",
        "- Verify sensitive asset weighting\n",
        "- Test prediction outputs\n",
        "\n",
        "### Verification Checklist:\n",
        "- [ ] Model trains on access features\n",
        "- [ ] Sensitive assets weighted correctly\n",
        "- [ ] Predictions make sense\n",
        "- [ ] All unit tests pass\n",
        "\n",
        "---\n",
        "\n",
        "## Step 6.4: Data Transfer Anomaly Model\n",
        "\n",
        "### What This Does:\n",
        "Detects anomalous data movement (potential exfiltration).\n",
        "Identifies: bulk downloads, unusual file access, external transfers.\n",
        "\n",
        "### Algorithm: Isolation Forest + Rule-based Hybrid\n",
        "- ML model for general anomalies\n",
        "- Rules for known bad patterns (large external transfer)\n",
        "\n",
        "### Features Used:\n",
        "From DataTransferFeatureExtractor (12 features)\n",
        "\n",
        "### Implementation Tasks:\n",
        "\n",
        "**Task 6.4.1: Create TransferAnomalyModel Class**\n",
        "- Location: `uba/analytics/model_library/transfer_anomaly.py`\n",
        "- Inherits from BaseModel\n",
        "- Hybrid ML + rules approach\n",
        "\n",
        "**Task 6.4.2: Implement ML Component**\n",
        "- Isolation Forest on transfer features\n",
        "- Detects unusual patterns\n",
        "\n",
        "**Task 6.4.3: Implement Rule Component**\n",
        "- Rule: external_transfer > threshold â†’ flag\n",
        "- Rule: volume_spike_flag == 1 â†’ flag\n",
        "- Rule: after_hours_transfer > 0.5 â†’ flag\n",
        "\n",
        "**Task 6.4.4: Combine Scores**\n",
        "- ML score: 0-1 (probability)\n",
        "- Rule score: 0-1 (how many rules triggered)\n",
        "- Final: weighted combination\n",
        "\n",
        "**Task 6.4.5: Write Unit Tests**\n",
        "- Test ML component\n",
        "- Test rule triggers\n",
        "- Test combined scoring\n",
        "\n",
        "### Verification Checklist:\n",
        "- [ ] ML component works\n",
        "- [ ] Rules trigger correctly\n",
        "- [ ] Combined scoring reasonable\n",
        "- [ ] High-risk transfers flagged\n",
        "\n",
        "---\n",
        "\n",
        "## Step 6.5: Time Anomaly Model\n",
        "\n",
        "### What This Does:\n",
        "Detects schedule/timing anomalies.\n",
        "Identifies: work hour changes, unusual activity times.\n",
        "\n",
        "### Algorithm: Statistical + Isolation Forest\n",
        "- Compare current time patterns to baseline\n",
        "- Flag significant deviations\n",
        "\n",
        "### Features Used:\n",
        "From TimeFeatureExtractor (12 features)\n",
        "\n",
        "### Implementation Tasks:\n",
        "\n",
        "**Task 6.5.1: Create TimeAnomalyModel Class**\n",
        "- Location: `uba/analytics/model_library/time_anomaly.py`\n",
        "- Inherits from BaseModel\n",
        "\n",
        "**Task 6.5.2: Implement Baseline Comparison**\n",
        "- Compare current hour distribution to baseline\n",
        "- Use statistical tests (chi-square, KS test)\n",
        "- Compute deviation score\n",
        "\n",
        "**Task 6.5.3: Implement Pattern Detection**\n",
        "- Detect new work hours\n",
        "- Detect schedule shifts\n",
        "- Flag night/weekend activity increases\n",
        "\n",
        "**Task 6.5.4: Write Unit Tests**\n",
        "- Test with various schedules\n",
        "- Test deviation detection\n",
        "- Test pattern changes\n",
        "\n",
        "### Verification Checklist:\n",
        "- [ ] Schedule comparison works\n",
        "- [ ] Deviations detected\n",
        "- [ ] Pattern changes flagged\n",
        "- [ ] All unit tests pass\n",
        "\n",
        "---\n",
        "\n",
        "## Step 6.6: Model Training Pipeline\n",
        "\n",
        "### What This Does:\n",
        "Orchestrates training of all models.\n",
        "\n",
        "### Implementation Tasks:\n",
        "\n",
        "**Task 6.6.1: Create ModelTrainer Class**\n",
        "- Location: `uba/analytics/engine.py` or separate file\n",
        "- Manages training workflow\n",
        "\n",
        "**Task 6.6.2: Implement Training Workflow**\n",
        "\n",
        "**TRAINING WORKFLOW STEPS:**\n",
        "\n",
        "1. **Load configuration** - Read which models to train from settings\n",
        "2. **For each model:**\n",
        "   - a. Load data from DSPM using the appropriate connector\n",
        "   - b. Extract features using the model's required feature extractor\n",
        "   - c. Prepare data (normalize values, handle missing values)\n",
        "   - d. Train the model\n",
        "   - e. Validate model using labeled data (if available)\n",
        "   - f. Save model to disk\n",
        "   - g. Register model in model registry with metadata\n",
        "3. **Log training metrics** - Record precision, recall, F1 scores\n",
        "4. **Report completion** - Generate training summary report\n",
        "\n",
        "**Task 6.6.3: Implement Validation Logic**\n",
        "- Use DSPM labeled data (policy_violations, identity_mappings)\n",
        "- Compute: precision, recall, F1-score (if labels available)\n",
        "- For unlabeled: use contamination estimate\n",
        "\n",
        "**Task 6.6.4: Implement Model Registry**\n",
        "- Store model metadata in database (uba_model_registry table)\n",
        "- Track: model_name, version, path, trained_at, metrics\n",
        "\n",
        "**Task 6.6.5: Add Retraining Schedule**\n",
        "- Models should be retrained periodically\n",
        "- Weekly or monthly depending on data volume\n",
        "- Detect model drift (performance degradation)\n",
        "\n",
        "### Verification Checklist:\n",
        "- [ ] Training pipeline runs end-to-end\n",
        "- [ ] All models trained and saved\n",
        "- [ ] Validation metrics computed\n",
        "- [ ] Models registered in database\n",
        "- [ ] Retraining can be triggered\n",
        "\n",
        "---\n",
        "\n",
        "## Step 6.7: Model Validation & Testing\n",
        "\n",
        "### What This Does:\n",
        "Ensures models perform correctly before deployment.\n",
        "\n",
        "### Validation Types:\n",
        "\n",
        "**1. Holdout Validation**\n",
        "- Split data: 80% train, 20% test\n",
        "- Train on 80%, evaluate on 20%\n",
        "- Use for supervised models\n",
        "\n",
        "**2. Cross-Validation**\n",
        "- Split into k folds (e.g., 5)\n",
        "- Train on k-1 folds, test on 1\n",
        "- Repeat k times, average results\n",
        "\n",
        "**3. Temporal Validation**\n",
        "- Train on older data\n",
        "- Test on newer data\n",
        "- More realistic for time-series\n",
        "\n",
        "**4. Labeled Data Validation**\n",
        "- Use policy_violations as ground truth\n",
        "- Check if model flags known anomalies\n",
        "- Most important validation!\n",
        "\n",
        "### Metrics to Compute:\n",
        "\n",
        "| Metric | Formula | What It Measures |\n",
        "|--------|---------|------------------|\n",
        "| Precision | TP / (TP + FP) | How many flagged are real anomalies |\n",
        "| Recall | TP / (TP + FN) | How many anomalies we catch |\n",
        "| F1-Score | 2 * (P * R) / (P + R) | Balance of precision/recall |\n",
        "| AUC-ROC | Area under ROC curve | Overall ranking quality |\n",
        "| False Positive Rate | FP / (FP + TN) | Alert fatigue measure |\n",
        "\n",
        "### Implementation Tasks:\n",
        "\n",
        "**Task 6.7.1: Create ModelValidator Class**\n",
        "- Implements all validation types\n",
        "- Computes all metrics\n",
        "\n",
        "**Task 6.7.2: Implement Labeled Validation**\n",
        "- Query policy_violations from DSPM\n",
        "- Match to user features\n",
        "- Evaluate if model detects\n",
        "\n",
        "**Task 6.7.3: Create Validation Report**\n",
        "- Generate report with all metrics\n",
        "- Include confusion matrix\n",
        "- Add recommendations\n",
        "\n",
        "**Task 6.7.4: Define Acceptance Criteria**\n",
        "\n",
        "**MINIMUM MODEL PERFORMANCE REQUIREMENTS:**\n",
        "\n",
        "| Metric | Minimum Threshold | Meaning |\n",
        "|--------|-------------------|---------|\n",
        "| Precision | > 0.3 (30%) | At least 30% of alerts are real anomalies |\n",
        "| Recall | > 0.7 (70%) | Catch at least 70% of actual anomalies |\n",
        "| False Positive Rate | < 0.1 (10%) | Less than 10% false alarms |\n",
        "\n",
        "**If model fails criteria:** Investigate root cause, adjust parameters, and retrain.\n",
        "\n",
        "### Verification Checklist:\n",
        "- [ ] All validation types implemented\n",
        "- [ ] Metrics computed correctly\n",
        "- [ ] Report generation works\n",
        "- [ ] Acceptance criteria defined\n",
        "- [ ] Failing models flagged\n",
        "\n",
        "---\n",
        "\n",
        "## ðŸ“ END OF PHASES 5-6\n",
        "\n",
        "### What You Have After Phase 6:\n",
        "1. âœ… Feature extraction pipeline (4 extractors, 50+ features)\n",
        "2. âœ… 4 trained ML models (login, access, transfer, time)\n",
        "3. âœ… Model training pipeline\n",
        "4. âœ… Model validation system\n",
        "5. âœ… Model registry in database\n",
        "6. âœ… Saved model files on disk\n",
        "\n",
        "### How to Test Phases 5-6 Complete:\n",
        "\n",
        "**Test 1: Feature Extraction**\n",
        "- Create a FeaturePipeline instance\n",
        "- Call extract_all_features() for a test user\n",
        "- Verify you get 40+ features back\n",
        "- Check that no features are null\n",
        "\n",
        "**Test 2: Model Training**\n",
        "- Create a model instance (e.g., LoginAnomalyModel)\n",
        "- Call train() with your training features\n",
        "- Call predict() on test features\n",
        "- Verify predictions match the number of input samples\n",
        "\n",
        "**Test 3: Model Persistence**\n",
        "- Save a trained model to disk\n",
        "- Create a new model instance\n",
        "- Load the saved model\n",
        "- Verify predictions are identical to original model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ac27f77f",
      "metadata": {
        "id": "ac27f77f"
      },
      "source": [
        "---\n",
        "\n",
        "# âš™ï¸ PHASE 7: ANALYTICS ENGINE\n",
        "\n",
        "## Goal: Build the orchestration engine that runs ML models and calculates risk scores\n",
        "\n",
        "---\n",
        "\n",
        "## âš ï¸ CRITICAL: Two Separate Engines in OpenUBA\n",
        "\n",
        "**OpenUBA has TWO distinct engines that run SEQUENTIALLY:**\n",
        "\n",
        "| Engine | File | Purpose | Runs |\n",
        "|--------|------|---------|------|\n",
        "| **ProcessEngine** | `core/process.py` | DATA INGESTION - loads logs, extracts users | FIRST |\n",
        "| **ModelEngine** | `core/model.py` | MODEL EXECUTION - runs ML models | SECOND |\n",
        "\n",
        "### Execution Order:\n",
        "```\n",
        "1. ProcessEngine.execute()\n",
        "   â”œâ”€â”€ Read scheme.json (data sources)\n",
        "   â”œâ”€â”€ For each source_group:\n",
        "   â”‚   â”œâ”€â”€ Load log files (CSV, Parquet, etc.)\n",
        "   â”‚   â”œâ”€â”€ Extract users via ExtractAllUsersCSV\n",
        "   â”‚   â””â”€â”€ Save users to storage/users/\n",
        "   â””â”€â”€ Return True when complete\n",
        "\n",
        "2. ModelEngine.execute()  \n",
        "   â”œâ”€â”€ Read models.json (model config)\n",
        "   â”œâ”€â”€ For each model_group:\n",
        "   â”‚   â”œâ”€â”€ Load data using data_loader_type\n",
        "   â”‚   â”œâ”€â”€ For each enabled model:\n",
        "   â”‚   â”‚   â”œâ”€â”€ Create ModelSession\n",
        "   â”‚   â”‚   â”œâ”€â”€ Verify model hash\n",
        "   â”‚   â”‚   â”œâ”€â”€ Call MODEL.execute()\n",
        "   â”‚   â”‚   â””â”€â”€ Process return_type\n",
        "   â”‚   â””â”€â”€ Evaluate inline rules\n",
        "   â””â”€â”€ Return results\n",
        "```\n",
        "\n",
        "### Why Two Engines?\n",
        "- **Separation of concerns**: Data loading vs. model execution\n",
        "- **Different schedules**: ProcessEngine might run hourly, ModelEngine daily\n",
        "- **Failure isolation**: Data loading failure doesn't crash model execution\n",
        "\n",
        "---\n",
        "\n",
        "## Analytics Engine Architecture\n",
        "\n",
        "```\n",
        "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
        "â”‚                           ANALYTICS ENGINE FLOW                                  â”‚\n",
        "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
        "â”‚                                                                                  â”‚\n",
        "â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                                               â”‚\n",
        "â”‚   â”‚  SCHEDULER   â”‚  â—„â”€â”€ Triggers execution (every hour, daily, etc.)            â”‚\n",
        "â”‚   â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜                                                               â”‚\n",
        "â”‚          â”‚                                                                       â”‚\n",
        "â”‚          â–¼                                                                       â”‚\n",
        "â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                                               â”‚\n",
        "â”‚   â”‚PROCESS ENGINEâ”‚  â—„â”€â”€ FIRST: Loads logs, extracts users (scheme.json)         â”‚\n",
        "â”‚   â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜                                                               â”‚\n",
        "â”‚          â”‚                                                                       â”‚\n",
        "â”‚          â–¼                                                                       â”‚\n",
        "â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                                               â”‚\n",
        "â”‚   â”‚ MODEL ENGINE â”‚  â—„â”€â”€ SECOND: Loads data for models (models.json)             â”‚\n",
        "â”‚   â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜                                                               â”‚\n",
        "â”‚          â”‚                                                                       â”‚\n",
        "â”‚          â–¼                                                                       â”‚\n",
        "â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                                               â”‚\n",
        "â”‚   â”‚  FEATURE     â”‚  â—„â”€â”€ Extracts features from raw data                         â”‚\n",
        "â”‚   â”‚  EXTRACTOR   â”‚                                                               â”‚\n",
        "â”‚   â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜                                                               â”‚\n",
        "â”‚          â”‚                                                                       â”‚\n",
        "â”‚          â–¼                                                                       â”‚\n",
        "â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                          â”‚\n",
        "â”‚   â”‚              MODEL EXECUTION ENGINE               â”‚                          â”‚\n",
        "â”‚   â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚                          â”‚\n",
        "â”‚   â”‚  â”‚ Login   â”‚ â”‚ Access  â”‚ â”‚Transfer â”‚ â”‚  Time   â”‚ â”‚                          â”‚\n",
        "â”‚   â”‚  â”‚ Model   â”‚ â”‚ Model   â”‚ â”‚ Model   â”‚ â”‚ Model   â”‚ â”‚                          â”‚\n",
        "â”‚   â”‚  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜ â”‚                          â”‚\n",
        "â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”˜                          â”‚\n",
        "â”‚           â”‚           â”‚           â”‚           â”‚                                  â”‚\n",
        "â”‚           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                  â”‚\n",
        "â”‚                             â”‚                                                    â”‚\n",
        "â”‚                             â–¼                                                    â”‚\n",
        "â”‚                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                              â”‚\n",
        "â”‚                    â”‚    RISK      â”‚  â—„â”€â”€ Aggregates all model scores            â”‚\n",
        "â”‚                    â”‚  CALCULATOR  â”‚                                              â”‚\n",
        "â”‚                    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜                                              â”‚\n",
        "â”‚                           â”‚                                                      â”‚\n",
        "â”‚                           â–¼                                                      â”‚\n",
        "â”‚                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                              â”‚\n",
        "â”‚                    â”‚   ANOMALY    â”‚  â—„â”€â”€ Stores detected anomalies              â”‚\n",
        "â”‚                    â”‚   SERVICE    â”‚                                              â”‚\n",
        "â”‚                    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜                                              â”‚\n",
        "â”‚                           â”‚                                                      â”‚\n",
        "â”‚                           â–¼                                                      â”‚\n",
        "â”‚                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                              â”‚\n",
        "â”‚                    â”‚    ALERT     â”‚  â—„â”€â”€ Generates alerts for high scores       â”‚\n",
        "â”‚                    â”‚   SERVICE    â”‚                                              â”‚\n",
        "â”‚                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                              â”‚\n",
        "â”‚                                                                                  â”‚\n",
        "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## Step 7.1: Model Execution Engine\n",
        "\n",
        "### What This Does:\n",
        "Loads and executes all ML models on user data.\n",
        "This is the CORE of your UBA system - runs the actual analysis.\n",
        "\n",
        "### OpenUBA Reference:\n",
        "File: `core/model.py` - ModelEngine class\n",
        "\n",
        "### âš ï¸ ACTUAL OpenUBA ModelEngine Flow:\n",
        "\n",
        "```python\n",
        "# Simplified from OpenUBA model.py ModelEngine.execute()\n",
        "\n",
        "1. Load model config from storage/models.json\n",
        "2. For each model_group_key in model_configuration:\n",
        "   \n",
        "   a. Get model_group data\n",
        "   b. Get data_loader_type (e.g., \"local_pandas_csv\")\n",
        "   c. Get data_loader_context (file path, separator, etc.)\n",
        "   \n",
        "   d. Load data based on data_loader_type:\n",
        "      - LOCAL_PANDAS_CSV â†’ model_modules.LocalPandasCSV()\n",
        "      - LOCAL_PANDAS_PARQUET â†’ model_modules.LocalPandasParquet()\n",
        "      - ES_GENERIC â†’ model_modules.ESGeneric()\n",
        "      - etc.\n",
        "   \n",
        "   e. For each model in model_group[\"models\"]:\n",
        "      - Check if model[\"enabled\"] == True\n",
        "      - Create ModelSession(metadata, library)\n",
        "      - Call model_session.start_job(loaded_data)\n",
        "      \n",
        "      f. ModelSession.start_job():\n",
        "         - Check if model is installed locally (folder exists)\n",
        "         - If not installed â†’ download from LibraryAPI\n",
        "         - Verify file hash matches config\n",
        "         - Run model via library.run_model()\n",
        "         \n",
        "      g. library.run_model():\n",
        "         - sys.path.insert(0, model_path)\n",
        "         - import MODEL\n",
        "         - result = MODEL.execute()  # <-- YOUR CODE RUNS HERE\n",
        "         - sys.path.remove(model_path)\n",
        "         - del sys.modules[\"MODEL\"]\n",
        "         - return result\n",
        "      \n",
        "   h. Process return_type (e.g., \"user_risks\")\n",
        "```\n",
        "\n",
        "### Implementation Tasks:\n",
        "\n",
        "**Task 7.1.1: Create AnalyticsEngine Class**\n",
        "- Location: `uba/analytics/engine.py`\n",
        "- Main orchestrator for all analytics\n",
        "- Manages model loading, execution, and result processing\n",
        "\n",
        "**Task 7.1.2: Implement Model Loading (OpenUBA Pattern)**\n",
        "- Read `storage/models.json` configuration\n",
        "- For each model_group, identify `data_loader_type`\n",
        "- Use appropriate model_module to load data\n",
        "- Store as CoreDataFrame\n",
        "\n",
        "**Task 7.1.3: Implement ModelSession Pattern**\n",
        "- Create ModelSession class (from OpenUBA)\n",
        "- Check if model is installed locally (`is_installed()`)\n",
        "- Verify model hash before execution\n",
        "- Execute via dynamic import: `import MODEL; MODEL.execute()`\n",
        "\n",
        "**Task 7.1.4: Implement Single User Analysis**\n",
        "- Method: `analyze_user(user_id)` â†’ dict of scores\n",
        "- Extract features for user\n",
        "- Run all models on features\n",
        "- Return individual model scores\n",
        "\n",
        "**Task 7.1.5: Implement Batch Analysis**\n",
        "- Method: `analyze_all_users()` â†’ DataFrame of scores\n",
        "- Efficiently process all users\n",
        "- Use parallel processing if possible\n",
        "- Return DataFrame with user_id and all scores\n",
        "\n",
        "**Task 7.1.6: Implement Model Result Aggregation**\n",
        "- Collect results from all models\n",
        "- Handle `return_type` from config:\n",
        "  - `\"user_risks\"` â†’ dict of user â†’ risk score\n",
        "- Pass to risk calculator\n",
        "\n",
        "**Task 7.1.7: Add Execution Logging**\n",
        "- Log to `storage/model_sessions.json` (OpenUBA pattern)\n",
        "- Include timestamp, model name, status\n",
        "- Store in uba_model_runs table\n",
        "\n",
        "### Verification Checklist:\n",
        "- [ ] Models load correctly from disk\n",
        "- [ ] ModelSession verifies hash before execution\n",
        "- [ ] `MODEL.execute()` is called correctly\n",
        "- [ ] Single user analysis works\n",
        "- [ ] Batch analysis completes\n",
        "- [ ] Results stored in database\n",
        "- [ ] Execution logged\n",
        "\n",
        "---\n",
        "\n",
        "## Step 7.2: Risk Score Calculator\n",
        "\n",
        "### What This Does:\n",
        "Aggregates individual model scores into a single risk score per user.\n",
        "This is what security analysts will see and act upon.\n",
        "\n",
        "### OpenUBA Reference:\n",
        "File: `core/risk.py` - RiskScore class (placeholder - you implement fully)\n",
        "\n",
        "### Risk Score Formula:\n",
        "\n",
        "```\n",
        "RISK SCORE CALCULATION:\n",
        "\n",
        "Individual Model Scores (each 0-1 range):\n",
        "â”œâ”€â”€ login_score      = Login Anomaly Model output\n",
        "â”œâ”€â”€ access_score     = Access Anomaly Model output  \n",
        "â”œâ”€â”€ transfer_score   = Data Transfer Model output\n",
        "â””â”€â”€ time_score       = Time Anomaly Model output\n",
        "\n",
        "Weights (configurable, must sum to 1.0):\n",
        "â”œâ”€â”€ login_weight     = 0.25 (25%)\n",
        "â”œâ”€â”€ access_weight    = 0.30 (30%)  â—„â”€â”€ Most important for DSPM\n",
        "â”œâ”€â”€ transfer_weight  = 0.30 (30%)  â—„â”€â”€ Data exfiltration focus\n",
        "â””â”€â”€ time_weight      = 0.15 (15%)\n",
        "\n",
        "COMPOSITE RISK SCORE =\n",
        "    (login_score Ã— login_weight) +\n",
        "    (access_score Ã— access_weight) +\n",
        "    (transfer_score Ã— transfer_weight) +\n",
        "    (time_score Ã— time_weight)\n",
        "\n",
        "Final Score: 0.0 (no risk) to 1.0 (critical risk)\n",
        "\n",
        "Mapped to 0-100 for display:\n",
        "â”œâ”€â”€ 0-25:   LOW risk (green)\n",
        "â”œâ”€â”€ 26-50:  MEDIUM risk (yellow)\n",
        "â”œâ”€â”€ 51-75:  HIGH risk (orange)\n",
        "â””â”€â”€ 76-100: CRITICAL risk (red)\n",
        "```\n",
        "\n",
        "### Implementation Tasks:\n",
        "\n",
        "**Task 7.2.1: Create RiskCalculator Class**\n",
        "- Location: `uba/analytics/risk_calculator.py`\n",
        "- Accepts individual model scores\n",
        "- Produces composite risk score\n",
        "\n",
        "**Task 7.2.2: Define Risk Weights**\n",
        "- Store weights in configuration\n",
        "- Allow customization per deployment\n",
        "- Weights must sum to 1.0\n",
        "\n",
        "**Task 7.2.3: Implement Score Calculation**\n",
        "- Method: `calculate(model_scores)` â†’ composite score\n",
        "- Apply weights to each model score\n",
        "- Return 0-1 normalized score\n",
        "\n",
        "**Task 7.2.4: Implement Score Classification**\n",
        "- Method: `classify(score)` â†’ risk level (LOW/MEDIUM/HIGH/CRITICAL)\n",
        "- Map numeric score to category\n",
        "- Thresholds configurable\n",
        "\n",
        "**Task 7.2.5: Implement Historical Tracking**\n",
        "- Store each score calculation in uba_risk_scores table\n",
        "- Track: user_id, score, components, calculated_at\n",
        "- Enable trend analysis\n",
        "\n",
        "**Task 7.2.6: Add Score Explanation**\n",
        "- Method: `explain(user_id)` â†’ dict of contributing factors\n",
        "- Show which models contributed most\n",
        "- Helpful for security analysts\n",
        "\n",
        "### Verification Checklist:\n",
        "- [ ] Weights sum to 1.0\n",
        "- [ ] Composite score in 0-1 range\n",
        "- [ ] Classification works correctly\n",
        "- [ ] Scores stored in database\n",
        "- [ ] Explanation provides insights\n",
        "\n",
        "---\n",
        "\n",
        "## Step 7.3: Anomaly Aggregator\n",
        "\n",
        "### What This Does:\n",
        "Collects and stores detected anomalies for further processing.\n",
        "\n",
        "### OpenUBA Reference:\n",
        "File: `core/anomaly.py` - AnomalyJob class (placeholder)\n",
        "\n",
        "### Implementation Tasks:\n",
        "\n",
        "**Task 7.3.1: Create AnomalyService Class**\n",
        "- Location: `uba/services/anomaly_service.py`\n",
        "- Manages anomaly lifecycle\n",
        "\n",
        "**Task 7.3.2: Define Anomaly Structure**\n",
        "```\n",
        "anomaly = {\n",
        "    \"id\": \"uuid\",\n",
        "    \"user_id\": \"user123\",\n",
        "    \"anomaly_type\": \"login_anomaly\",  # Which model detected\n",
        "    \"score\": 0.85,                     # Raw model score\n",
        "    \"features\": {...},                 # Features that triggered\n",
        "    \"details\": {\n",
        "        \"reason\": \"Login from new location\",\n",
        "        \"baseline_value\": \"192.168.1.x\",\n",
        "        \"current_value\": \"45.67.89.x\"\n",
        "    },\n",
        "    \"detected_at\": \"2026-01-07T14:30:00Z\",\n",
        "    \"status\": \"new\"  # new, reviewed, false_positive, confirmed\n",
        "}\n",
        "```\n",
        "\n",
        "**Task 7.3.3: Implement Anomaly Detection Logic**\n",
        "- Method: `detect_anomalies(user_id, model_scores)` â†’ list of anomalies\n",
        "- Threshold: score > 0.5 (configurable)\n",
        "- Create anomaly record for each threshold breach\n",
        "\n",
        "**Task 7.3.4: Implement Anomaly Storage**\n",
        "- Store in uba_anomalies table\n",
        "- Include all relevant context\n",
        "- Link to user profile\n",
        "\n",
        "**Task 7.3.5: Implement Anomaly Retrieval**\n",
        "- Method: `get_anomalies(user_id=None, status=None)` â†’ list\n",
        "- Filter by user, status, date range\n",
        "- Sort by score (highest first)\n",
        "\n",
        "**Task 7.3.6: Implement Status Updates**\n",
        "- Method: `update_status(anomaly_id, status, notes)`\n",
        "- Allow marking as reviewed/false_positive/confirmed\n",
        "- Track who updated and when\n",
        "\n",
        "### Verification Checklist:\n",
        "- [ ] Anomalies detected when score > threshold\n",
        "- [ ] Stored with full context\n",
        "- [ ] Retrieval filtering works\n",
        "- [ ] Status updates tracked\n",
        "\n",
        "---\n",
        "\n",
        "## Step 7.4: Scheduler Integration\n",
        "\n",
        "### What This Does:\n",
        "Runs the analytics engine on a schedule (not just on-demand).\n",
        "\n",
        "### OpenUBA Reference:\n",
        "File: `core/core.py` - scheduler_run function\n",
        "\n",
        "### Implementation Tasks:\n",
        "\n",
        "**Task 7.4.1: Create Scheduler Configuration**\n",
        "- Define schedule for each operation\n",
        "- Example: risk calculation every hour\n",
        "- Example: model retraining weekly\n",
        "\n",
        "**Task 7.4.2: Implement Scheduled Jobs**\n",
        "\n",
        "| Job Name | Frequency | What It Does |\n",
        "|----------|-----------|--------------|\n",
        "| `analyze_all_users` | Every hour | Run all models on all users |\n",
        "| `refresh_baselines` | Daily | Update user behavior baselines |\n",
        "| `retrain_models` | Weekly | Retrain ML models with new data |\n",
        "| `cleanup_old_data` | Daily | Archive old anomalies/scores |\n",
        "\n",
        "**Task 7.4.3: Integrate with DSPM Scheduler**\n",
        "- DSPM already has a scheduler (APScheduler)\n",
        "- Add UBA jobs to existing scheduler\n",
        "- Or create separate UBA scheduler\n",
        "\n",
        "**Task 7.4.4: Add Job Monitoring**\n",
        "- Log job start/end times\n",
        "- Track success/failure\n",
        "- Alert on failures\n",
        "\n",
        "### Verification Checklist:\n",
        "- [ ] Jobs run on schedule\n",
        "- [ ] Results stored after each run\n",
        "- [ ] Failures logged and alerted\n",
        "- [ ] Can manually trigger jobs\n",
        "\n",
        "---\n",
        "\n",
        "# ðŸš¨ PHASE 8: ALERT & CASE SYSTEM\n",
        "\n",
        "## Goal: Generate alerts from anomalies and manage investigation cases\n",
        "\n",
        "---\n",
        "\n",
        "## Alert System Architecture\n",
        "\n",
        "```\n",
        "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
        "â”‚                            ALERT SYSTEM FLOW                                     â”‚\n",
        "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
        "â”‚                                                                                  â”‚\n",
        "â”‚   ANOMALIES                    ALERTS                      CASES                 â”‚\n",
        "â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”                 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”                 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”‚\n",
        "â”‚   â”‚Anomaly 1â”‚â”€â”               â”‚ Alert 1 â”‚â”€â”               â”‚         â”‚           â”‚\n",
        "â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚               â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚               â”‚  Case   â”‚           â”‚\n",
        "â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚  AGGREGATE    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚  ESCALATE     â”‚    1    â”‚           â”‚\n",
        "â”‚   â”‚Anomaly 2â”‚â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º â”‚ Alert 2 â”‚â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º â”‚         â”‚           â”‚\n",
        "â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚               â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚               â”‚(Multipleâ”‚           â”‚\n",
        "â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚               â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚               â”‚ alerts) â”‚           â”‚\n",
        "â”‚   â”‚Anomaly 3â”‚â”€â”˜               â”‚ Alert 3 â”‚â”€â”˜               â”‚         â”‚           â”‚\n",
        "â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜           â”‚\n",
        "â”‚                                                                                  â”‚\n",
        "â”‚   Multiple anomalies          One alert per               Case groups            â”‚\n",
        "â”‚   can trigger one             significant event           related alerts         â”‚\n",
        "â”‚   alert (aggregation)                                     for investigation      â”‚\n",
        "â”‚                                                                                  â”‚\n",
        "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## Step 8.1: Alert Generation Service\n",
        "\n",
        "### What This Does:\n",
        "Creates alerts when anomalies exceed thresholds.\n",
        "Not every anomaly becomes an alert - only significant ones.\n",
        "\n",
        "### OpenUBA Reference:\n",
        "File: `core/alert.py` (placeholder - you implement fully)\n",
        "\n",
        "### Implementation Tasks:\n",
        "\n",
        "**Task 8.1.1: Create AlertService Class**\n",
        "- Location: `uba/services/alert_service.py`\n",
        "- Manages alert creation and lifecycle\n",
        "\n",
        "**Task 8.1.2: Define Alert Structure**\n",
        "```\n",
        "alert = {\n",
        "    \"id\": \"uuid\",\n",
        "    \"user_id\": \"user123\",\n",
        "    \"title\": \"Unusual Login Pattern Detected\",\n",
        "    \"description\": \"User logged in from new location at unusual time\",\n",
        "    \"severity\": \"HIGH\",  # LOW, MEDIUM, HIGH, CRITICAL\n",
        "    \"risk_score\": 78,\n",
        "    \"anomaly_ids\": [\"anomaly1\", \"anomaly2\"],  # Related anomalies\n",
        "    \"source_models\": [\"login_anomaly\", \"time_anomaly\"],\n",
        "    \"evidence\": {\n",
        "        \"new_ip\": \"45.67.89.x\",\n",
        "        \"login_time\": \"03:45 AM\",\n",
        "        \"baseline_hours\": \"9 AM - 6 PM\"\n",
        "    },\n",
        "    \"status\": \"open\",  # open, acknowledged, investigating, resolved, false_positive\n",
        "    \"created_at\": \"2026-01-07T14:30:00Z\",\n",
        "    \"assigned_to\": null,\n",
        "    \"resolution_notes\": null\n",
        "}\n",
        "```\n",
        "\n",
        "**Task 8.1.3: Implement Alert Generation Logic**\n",
        "- Method: `generate_alert(user_id, anomalies, risk_score)` â†’ Alert\n",
        "- Threshold: risk_score > 50 OR critical anomaly\n",
        "- Aggregate related anomalies into single alert\n",
        "\n",
        "**Task 8.1.4: Implement Alert Deduplication**\n",
        "- Don't create duplicate alerts for same issue\n",
        "- Check if similar alert exists in last 24 hours\n",
        "- Update existing alert instead of creating new\n",
        "\n",
        "**Task 8.1.5: Implement Severity Classification**\n",
        "```\n",
        "SEVERITY MAPPING:\n",
        "\n",
        "Risk Score 0-25:   No alert (too low)\n",
        "Risk Score 26-50:  LOW severity alert\n",
        "Risk Score 51-75:  MEDIUM severity alert\n",
        "Risk Score 76-90:  HIGH severity alert\n",
        "Risk Score 91-100: CRITICAL severity alert\n",
        "\n",
        "Override to CRITICAL if:\n",
        "- External data transfer detected\n",
        "- Privilege escalation detected\n",
        "- Policy violation with HIGH severity\n",
        "```\n",
        "\n",
        "**Task 8.1.6: Implement Alert Notification**\n",
        "- Method: `notify(alert)` - send notification\n",
        "- Integration with DSPM notification system\n",
        "- Email, Slack, or in-app notification\n",
        "\n",
        "### Verification Checklist:\n",
        "- [ ] Alerts created for high-risk users\n",
        "- [ ] Deduplication prevents spam\n",
        "- [ ] Severity matches risk level\n",
        "- [ ] Notifications sent\n",
        "\n",
        "---\n",
        "\n",
        "## Step 8.2: Alert Priority System\n",
        "\n",
        "### What This Does:\n",
        "Ranks alerts so analysts focus on most important first.\n",
        "\n",
        "### Implementation Tasks:\n",
        "\n",
        "**Task 8.2.1: Define Priority Score**\n",
        "```\n",
        "PRIORITY SCORE CALCULATION:\n",
        "\n",
        "Base Score = Risk Score (0-100)\n",
        "\n",
        "Modifiers:\n",
        "+ 20 if user has admin privileges\n",
        "+ 15 if sensitive data accessed\n",
        "+ 10 if external transfer involved\n",
        "+ 10 if repeat offender (previous alerts)\n",
        "+ 5 if business hours violation\n",
        "- 10 if user is on travel (known exception)\n",
        "\n",
        "Final Priority = Base + Modifiers (capped at 100)\n",
        "```\n",
        "\n",
        "**Task 8.2.2: Implement Priority Calculator**\n",
        "- Method: `calculate_priority(alert)` â†’ priority score\n",
        "- Consider user context\n",
        "- Consider asset sensitivity\n",
        "\n",
        "**Task 8.2.3: Implement Alert Queue**\n",
        "- Method: `get_alert_queue()` â†’ ordered list\n",
        "- Ordered by priority (highest first)\n",
        "- Filter by status, assignee, date\n",
        "\n",
        "**Task 8.2.4: Add SLA Tracking**\n",
        "- Define response time by severity\n",
        "- CRITICAL: 1 hour, HIGH: 4 hours, MEDIUM: 24 hours\n",
        "- Track if SLA breached\n",
        "\n",
        "### Verification Checklist:\n",
        "- [ ] Priority score calculated correctly\n",
        "- [ ] Modifiers applied\n",
        "- [ ] Queue ordered by priority\n",
        "- [ ] SLA tracking works\n",
        "\n",
        "---\n",
        "\n",
        "## Step 8.3: Case Management Service\n",
        "\n",
        "### What This Does:\n",
        "Creates and manages investigation cases for groups of related alerts.\n",
        "\n",
        "### OpenUBA Reference:\n",
        "File: `core/case.py` (placeholder - you implement fully)\n",
        "\n",
        "### Implementation Tasks:\n",
        "\n",
        "**Task 8.3.1: Create CaseService Class**\n",
        "- Location: `uba/services/case_service.py`\n",
        "- Manages case lifecycle\n",
        "\n",
        "**Task 8.3.2: Define Case Structure**\n",
        "```\n",
        "case = {\n",
        "    \"id\": \"uuid\",\n",
        "    \"title\": \"Suspicious Activity - John Doe\",\n",
        "    \"description\": \"Multiple anomalies detected over 3 days\",\n",
        "    \"user_id\": \"user123\",\n",
        "    \"alert_ids\": [\"alert1\", \"alert2\", \"alert3\"],\n",
        "    \"severity\": \"HIGH\",\n",
        "    \"status\": \"open\",  # open, investigating, pending_review, closed\n",
        "    \"assignee\": \"analyst@company.com\",\n",
        "    \"created_at\": \"2026-01-07T14:30:00Z\",\n",
        "    \"updated_at\": \"2026-01-07T15:00:00Z\",\n",
        "    \"timeline\": [\n",
        "        {\"action\": \"created\", \"by\": \"system\", \"at\": \"...\"},\n",
        "        {\"action\": \"assigned\", \"by\": \"manager\", \"to\": \"analyst\", \"at\": \"...\"},\n",
        "        {\"action\": \"note_added\", \"by\": \"analyst\", \"note\": \"Investigating...\", \"at\": \"...\"}\n",
        "    ],\n",
        "    \"resolution\": {\n",
        "        \"outcome\": null,  # confirmed_threat, false_positive, inconclusive\n",
        "        \"notes\": null,\n",
        "        \"resolved_by\": null,\n",
        "        \"resolved_at\": null\n",
        "    }\n",
        "}\n",
        "```\n",
        "\n",
        "**Task 8.3.3: Implement Case Creation**\n",
        "- Method: `create_case(alerts)` â†’ Case\n",
        "- Automatically create for CRITICAL alerts\n",
        "- Or manually create by analyst\n",
        "\n",
        "**Task 8.3.4: Implement Alert-to-Case Grouping**\n",
        "- Method: `auto_group_alerts()` - group related alerts\n",
        "- Same user within time window â†’ same case\n",
        "- Similar anomaly types â†’ same case\n",
        "\n",
        "**Task 8.3.5: Implement Case Workflow**\n",
        "- Status transitions: open â†’ investigating â†’ pending_review â†’ closed\n",
        "- Track all actions in timeline\n",
        "- Require resolution notes when closing\n",
        "\n",
        "**Task 8.3.6: Implement Case Assignment**\n",
        "- Method: `assign_case(case_id, assignee)`\n",
        "- Round-robin or manual assignment\n",
        "- Notify assignee\n",
        "\n",
        "**Task 8.3.7: Write Unit Tests**\n",
        "- Test case creation\n",
        "- Test alert grouping\n",
        "- Test workflow transitions\n",
        "\n",
        "### Verification Checklist:\n",
        "- [ ] Cases created from alerts\n",
        "- [ ] Alert grouping works\n",
        "- [ ] Workflow transitions enforced\n",
        "- [ ] Timeline tracks all actions\n",
        "- [ ] Resolution required to close\n",
        "\n",
        "---\n",
        "\n",
        "# ðŸ“ PHASE 9: RULE ENGINE\n",
        "\n",
        "## Goal: Implement rule-based detection for known threat patterns\n",
        "\n",
        "---\n",
        "\n",
        "## Rules vs ML Models\n",
        "\n",
        "```\n",
        "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
        "â”‚                        RULES vs ML MODELS                                        â”‚\n",
        "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
        "â”‚                                                                                  â”‚\n",
        "â”‚   RULES (Deterministic)              ML MODELS (Probabilistic)                  â”‚\n",
        "â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                    â”‚\n",
        "â”‚   â”‚ IF condition THEN   â”‚            â”‚ Based on patterns   â”‚                    â”‚\n",
        "â”‚   â”‚    action           â”‚            â”‚ learned from data   â”‚                    â”‚\n",
        "â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                    â”‚\n",
        "â”‚                                                                                  â”‚\n",
        "â”‚   EXAMPLES:                          EXAMPLES:                                   â”‚\n",
        "â”‚   â€¢ Login from blocked IP            â€¢ Unusual login pattern                    â”‚\n",
        "â”‚   â€¢ 5+ failed logins                 â€¢ Anomalous data access                    â”‚\n",
        "â”‚   â€¢ Access after hours               â€¢ Behavior drift over time                 â”‚\n",
        "â”‚   â€¢ Known bad file hash              â€¢ Subtle insider threat                    â”‚\n",
        "â”‚                                                                                  â”‚\n",
        "â”‚   PROS:                              PROS:                                       â”‚\n",
        "â”‚   â€¢ Predictable                      â€¢ Catches unknown patterns                 â”‚\n",
        "â”‚   â€¢ Easy to explain                  â€¢ Adapts to new threats                    â”‚\n",
        "â”‚   â€¢ No training needed               â€¢ Handles complex patterns                 â”‚\n",
        "â”‚                                                                                  â”‚\n",
        "â”‚   CONS:                              CONS:                                       â”‚\n",
        "â”‚   â€¢ Only catches known patterns      â€¢ Harder to explain                        â”‚\n",
        "â”‚   â€¢ Needs manual updates             â€¢ Needs training data                      â”‚\n",
        "â”‚   â€¢ Can be bypassed                  â€¢ Can have false positives                 â”‚\n",
        "â”‚                                                                                  â”‚\n",
        "â”‚   BEST APPROACH: USE BOTH TOGETHER!                                             â”‚\n",
        "â”‚                                                                                  â”‚\n",
        "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## Step 9.1: Rule Definition System\n",
        "\n",
        "### What This Does:\n",
        "Allows defining detection rules that trigger alerts.\n",
        "\n",
        "### OpenUBA Reference:\n",
        "File: `core/rule.py` (placeholder - you implement fully)\n",
        "File: `core/storage/models.json` - has rule examples in model_group_context\n",
        "\n",
        "### Implementation Tasks:\n",
        "\n",
        "**Task 9.1.1: Create RuleService Class**\n",
        "- Location: `uba/services/rule_service.py`\n",
        "- Manages rule CRUD and execution\n",
        "\n",
        "**Task 9.1.2: Define Rule Structure**\n",
        "```\n",
        "rule = {\n",
        "    \"id\": \"uuid\",\n",
        "    \"name\": \"After Hours Login\",\n",
        "    \"description\": \"Detect logins outside business hours\",\n",
        "    \"enabled\": true,\n",
        "    \"severity\": \"MEDIUM\",\n",
        "    \"conditions\": [\n",
        "        {\n",
        "            \"field\": \"login_hour\",\n",
        "            \"operator\": \"not_between\",\n",
        "            \"value\": [9, 17]  # Outside 9 AM - 5 PM\n",
        "        }\n",
        "    ],\n",
        "    \"logic\": \"AND\",  # AND = all conditions, OR = any condition\n",
        "    \"exceptions\": [\n",
        "        {\n",
        "            \"field\": \"user_role\",\n",
        "            \"operator\": \"equals\",\n",
        "            \"value\": \"on_call\"  # On-call users excluded\n",
        "        }\n",
        "    ],\n",
        "    \"actions\": [\"create_anomaly\", \"create_alert\"],\n",
        "    \"score\": 30,  # Risk points to add\n",
        "    \"mitre_technique\": \"T1078\",  # Valid Accounts\n",
        "    \"created_by\": \"admin\",\n",
        "    \"created_at\": \"2026-01-01T00:00:00Z\"\n",
        "}\n",
        "```\n",
        "\n",
        "**Task 9.1.3: Define Operators**\n",
        "```\n",
        "SUPPORTED OPERATORS:\n",
        "\n",
        "Comparison:\n",
        "â€¢ equals, not_equals\n",
        "â€¢ greater_than, less_than\n",
        "â€¢ greater_or_equal, less_or_equal\n",
        "â€¢ between, not_between\n",
        "\n",
        "String:\n",
        "â€¢ contains, not_contains\n",
        "â€¢ starts_with, ends_with\n",
        "â€¢ matches (regex)\n",
        "\n",
        "List:\n",
        "â€¢ in, not_in\n",
        "\n",
        "Special:\n",
        "â€¢ is_null, is_not_null\n",
        "â€¢ is_new (not in baseline)\n",
        "â€¢ deviates_from_baseline (> N std)\n",
        "```\n",
        "\n",
        "**Task 9.1.4: Implement Rule CRUD**\n",
        "- Method: `create_rule(rule_data)` â†’ Rule\n",
        "- Method: `update_rule(rule_id, updates)` â†’ Rule\n",
        "- Method: `delete_rule(rule_id)` â†’ bool\n",
        "- Method: `get_rules(enabled=True)` â†’ list\n",
        "\n",
        "**Task 9.1.5: Store Rules in Database**\n",
        "- Table: uba_rules\n",
        "- Include version tracking\n",
        "- Audit log for changes\n",
        "\n",
        "### Verification Checklist:\n",
        "- [ ] Rule structure validated\n",
        "- [ ] All operators defined\n",
        "- [ ] CRUD operations work\n",
        "- [ ] Rules stored in database\n",
        "\n",
        "---\n",
        "\n",
        "## Step 9.2: Rule Evaluation Engine\n",
        "\n",
        "### What This Does:\n",
        "Evaluates rules against user data to detect matches.\n",
        "\n",
        "### Implementation Tasks:\n",
        "\n",
        "**Task 9.2.1: Create RuleEvaluator Class**\n",
        "- Location: Part of rule_service.py or separate file\n",
        "- Evaluates rules against data\n",
        "\n",
        "**Task 9.2.2: Implement Condition Evaluation**\n",
        "- Method: `evaluate_condition(condition, data)` â†’ bool\n",
        "- Handle all operator types\n",
        "- Return True if condition matches\n",
        "\n",
        "**Task 9.2.3: Implement Rule Evaluation**\n",
        "- Method: `evaluate_rule(rule, user_data)` â†’ bool\n",
        "- Evaluate all conditions\n",
        "- Apply AND/OR logic\n",
        "- Check exceptions\n",
        "\n",
        "**Task 9.2.4: Implement Batch Evaluation**\n",
        "- Method: `evaluate_all_rules(user_id)` â†’ list of matched rules\n",
        "- Run all enabled rules against user\n",
        "- Return list of matches with details\n",
        "\n",
        "**Task 9.2.5: Implement Rule Actions**\n",
        "- When rule matches, execute actions:\n",
        "  - `create_anomaly`: Create anomaly record\n",
        "  - `create_alert`: Create alert\n",
        "  - `add_risk_score`: Add points to risk\n",
        "  - `notify`: Send notification\n",
        "\n",
        "**Task 9.2.6: Add Rule Performance Tracking**\n",
        "- Track how often each rule fires\n",
        "- Track false positive rate\n",
        "- Identify noisy rules\n",
        "\n",
        "### Verification Checklist:\n",
        "- [ ] Conditions evaluate correctly\n",
        "- [ ] AND/OR logic works\n",
        "- [ ] Exceptions prevent matches\n",
        "- [ ] Actions execute on match\n",
        "- [ ] Performance tracked\n",
        "\n",
        "---\n",
        "\n",
        "## Step 9.3: Built-in Rule Library\n",
        "\n",
        "### What This Does:\n",
        "Provides pre-built rules for common threat patterns.\n",
        "\n",
        "### Default Rules to Create:\n",
        "\n",
        "| Rule Name | Condition | Severity | MITRE |\n",
        "|-----------|-----------|----------|-------|\n",
        "| After Hours Login | login_hour NOT IN [9-17] | MEDIUM | T1078 |\n",
        "| Failed Login Spike | failed_logins > 5 in 10 min | HIGH | T1110 |\n",
        "| New Location Login | location NOT IN baseline | MEDIUM | T1078 |\n",
        "| New Device Login | device NOT IN baseline | LOW | T1078 |\n",
        "| Sensitive Data Access | sensitive_asset_ratio > 0.5 | HIGH | T1530 |\n",
        "| Bulk File Download | files_accessed > 100 in 1 hour | HIGH | T1567 |\n",
        "| External Data Transfer | external_transfer = true | CRITICAL | T1041 |\n",
        "| Privilege Escalation | role_change to admin | CRITICAL | T1078.001 |\n",
        "| Weekend Activity | day_of_week IN [Sat, Sun] | LOW | T1078 |\n",
        "| Dormant Account Login | days_since_last_login > 90 | MEDIUM | T1078.001 |\n",
        "\n",
        "**Task 9.3.1: Create Default Rules**\n",
        "- Implement each rule from table above\n",
        "- Store in database on first startup\n",
        "- Mark as system rules (not deletable)\n",
        "\n",
        "**Task 9.3.2: Allow Custom Rules**\n",
        "- Users can create additional rules\n",
        "- Custom rules can override system rules\n",
        "- Mark as user-created\n",
        "\n",
        "### Verification Checklist:\n",
        "- [ ] All 10 default rules created\n",
        "- [ ] Rules fire correctly\n",
        "- [ ] Custom rules work\n",
        "- [ ] Can enable/disable rules\n",
        "\n",
        "---\n",
        "\n",
        "## Step 9.4: MITRE ATT&CK Mapping\n",
        "\n",
        "### What This Does:\n",
        "Maps detections to MITRE ATT&CK framework for standardized reporting.\n",
        "\n",
        "### What is MITRE ATT&CK?\n",
        "- Industry standard framework for cyber attacks\n",
        "- Organizes attacks into Tactics (goals) and Techniques (methods)\n",
        "- Helps communicate threats consistently\n",
        "\n",
        "### Relevant MITRE Techniques for UBA:\n",
        "\n",
        "| Technique ID | Name | UBA Detection |\n",
        "|--------------|------|---------------|\n",
        "| T1078 | Valid Accounts | Unusual login patterns |\n",
        "| T1078.001 | Valid Accounts: Default Accounts | Dormant account usage |\n",
        "| T1110 | Brute Force | Failed login attempts |\n",
        "| T1530 | Data from Cloud Storage | Unusual data access |\n",
        "| T1567 | Exfiltration Over Web Service | Bulk data transfer |\n",
        "| T1041 | Exfiltration Over C2 Channel | External transfers |\n",
        "| T1485 | Data Destruction | Unusual delete patterns |\n",
        "| T1565 | Data Manipulation | Unusual modification |\n",
        "\n",
        "### Implementation Tasks:\n",
        "\n",
        "**Task 9.4.1: Create MITRE Mapping Configuration**\n",
        "- Location: `uba/config/mitre_mapping.py`\n",
        "- Map each detection to technique ID\n",
        "- Include tactic and description\n",
        "\n",
        "**Task 9.4.2: Include in Alerts/Cases**\n",
        "- Add mitre_technique field to alerts\n",
        "- Show in case details\n",
        "- Enable filtering by technique\n",
        "\n",
        "**Task 9.4.3: Generate MITRE Report**\n",
        "- Method: `get_mitre_coverage()` â†’ coverage report\n",
        "- Show which techniques you can detect\n",
        "- Identify gaps in coverage\n",
        "\n",
        "### Verification Checklist:\n",
        "- [ ] Mapping configuration created\n",
        "- [ ] Alerts include MITRE reference\n",
        "- [ ] Coverage report generated\n",
        "- [ ] Filtering by technique works\n",
        "\n",
        "---\n",
        "\n",
        "## ðŸ“ END OF PHASES 7-9\n",
        "\n",
        "### What You Have After Phase 9:\n",
        "1. âœ… Analytics engine running models on schedule\n",
        "2. âœ… Risk score calculator with weighted aggregation\n",
        "3. âœ… Anomaly detection and storage\n",
        "4. âœ… Alert generation with severity classification\n",
        "5. âœ… Case management for investigations\n",
        "6. âœ… Rule engine with 10+ built-in rules\n",
        "7. âœ… MITRE ATT&CK mapping\n",
        "\n",
        "### How to Test Phases 7-9 Complete:\n",
        "\n",
        "**Test 1: Analytics Engine**\n",
        "- Create an AnalyticsEngine instance\n",
        "- Call analyze_user() for a test user\n",
        "- Verify the response contains a \"risk_score\" key\n",
        "- Check that scores are in valid range (0-100)\n",
        "\n",
        "**Test 2: Alert Service**\n",
        "- Create an AlertService instance with database session\n",
        "- Call generate_alert() with a user_id, anomalies list, and risk_score=80\n",
        "- Verify an alert is created with severity \"HIGH\"\n",
        "- Check alert is stored in database\n",
        "\n",
        "**Test 3: Rule Engine**\n",
        "- Create a RuleService instance with database session\n",
        "- Call evaluate_all_rules() for a test user\n",
        "- Verify it returns a list of matched rules\n",
        "- Check that default rules are properly loaded"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6a0f289b",
      "metadata": {
        "id": "6a0f289b"
      },
      "source": [
        "---\n",
        "\n",
        "# ðŸŒ PHASE 10: API ROUTES & ENDPOINTS\n",
        "\n",
        "## Goal: Build all REST API endpoints that expose UBA functionality\n",
        "\n",
        "---\n",
        "\n",
        "## API Architecture Overview\n",
        "\n",
        "```\n",
        "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
        "â”‚                           UBA_PRO API STRUCTURE                                  â”‚\n",
        "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
        "â”‚                                                                                  â”‚\n",
        "â”‚   BASE URL: /api/v1/uba                                                          â”‚\n",
        "â”‚                                                                                  â”‚\n",
        "â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚\n",
        "â”‚   â”‚ DASHBOARD                                                                â”‚   â”‚\n",
        "â”‚   â”‚ GET  /dashboard/summary          â†’ Overall UBA statistics               â”‚   â”‚\n",
        "â”‚   â”‚ GET  /dashboard/risk-distribution â†’ Risk level breakdown                â”‚   â”‚\n",
        "â”‚   â”‚ GET  /dashboard/trends           â†’ Risk trends over time                â”‚   â”‚\n",
        "â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚\n",
        "â”‚                                                                                  â”‚\n",
        "â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚\n",
        "â”‚   â”‚ PROFILES (User Behavioral Profiles)                                      â”‚   â”‚\n",
        "â”‚   â”‚ GET  /profiles                   â†’ List all user profiles               â”‚   â”‚\n",
        "â”‚   â”‚ GET  /profiles/{user_id}         â†’ Get specific user profile            â”‚   â”‚\n",
        "â”‚   â”‚ GET  /profiles/{user_id}/timeline â†’ User activity timeline              â”‚   â”‚\n",
        "â”‚   â”‚ GET  /profiles/{user_id}/risk-history â†’ Historical risk scores          â”‚   â”‚\n",
        "â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚\n",
        "â”‚                                                                                  â”‚\n",
        "â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚\n",
        "â”‚   â”‚ RISK                                                                     â”‚   â”‚\n",
        "â”‚   â”‚ GET  /risk/scores                â†’ All current risk scores              â”‚   â”‚\n",
        "â”‚   â”‚ GET  /risk/high-risk-users       â†’ Users with score > 75                â”‚   â”‚\n",
        "â”‚   â”‚ POST /risk/calculate/{user_id}   â†’ Trigger risk recalculation           â”‚   â”‚\n",
        "â”‚   â”‚ GET  /risk/trends/{user_id}      â†’ Risk score history                   â”‚   â”‚\n",
        "â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚\n",
        "â”‚                                                                                  â”‚\n",
        "â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚\n",
        "â”‚   â”‚ ANOMALIES                                                                â”‚   â”‚\n",
        "â”‚   â”‚ GET  /anomalies                  â†’ List all anomalies                   â”‚   â”‚\n",
        "â”‚   â”‚ GET  /anomalies/{id}             â†’ Get anomaly details                  â”‚   â”‚\n",
        "â”‚   â”‚ PUT  /anomalies/{id}/status      â†’ Update anomaly status                â”‚   â”‚\n",
        "â”‚   â”‚ GET  /anomalies/by-user/{user_id} â†’ Anomalies for specific user         â”‚   â”‚\n",
        "â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚\n",
        "â”‚                                                                                  â”‚\n",
        "â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚\n",
        "â”‚   â”‚ ALERTS                                                                   â”‚   â”‚\n",
        "â”‚   â”‚ GET  /alerts                     â†’ List all alerts                      â”‚   â”‚\n",
        "â”‚   â”‚ GET  /alerts/{id}                â†’ Get alert details                    â”‚   â”‚\n",
        "â”‚   â”‚ PUT  /alerts/{id}/status         â†’ Update alert status                  â”‚   â”‚\n",
        "â”‚   â”‚ PUT  /alerts/{id}/assign         â†’ Assign to analyst                    â”‚   â”‚\n",
        "â”‚   â”‚ GET  /alerts/queue               â†’ Priority-ordered alert queue         â”‚   â”‚\n",
        "â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚\n",
        "â”‚                                                                                  â”‚\n",
        "â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚\n",
        "â”‚   â”‚ CASES                                                                    â”‚   â”‚\n",
        "â”‚   â”‚ GET  /cases                      â†’ List all cases                       â”‚   â”‚\n",
        "â”‚   â”‚ POST /cases                      â†’ Create new case                      â”‚   â”‚\n",
        "â”‚   â”‚ GET  /cases/{id}                 â†’ Get case details                     â”‚   â”‚\n",
        "â”‚   â”‚ PUT  /cases/{id}                 â†’ Update case                          â”‚   â”‚\n",
        "â”‚   â”‚ PUT  /cases/{id}/status          â†’ Change case status                   â”‚   â”‚\n",
        "â”‚   â”‚ POST /cases/{id}/notes           â†’ Add investigation note               â”‚   â”‚\n",
        "â”‚   â”‚ PUT  /cases/{id}/resolve         â†’ Close case with resolution           â”‚   â”‚\n",
        "â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚\n",
        "â”‚                                                                                  â”‚\n",
        "â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚\n",
        "â”‚   â”‚ MODELS                                                                   â”‚   â”‚\n",
        "â”‚   â”‚ GET  /models                     â†’ List all ML models                   â”‚   â”‚\n",
        "â”‚   â”‚ GET  /models/{id}                â†’ Get model details                    â”‚   â”‚\n",
        "â”‚   â”‚ POST /models/{id}/train          â†’ Trigger model training               â”‚   â”‚\n",
        "â”‚   â”‚ GET  /models/{id}/performance    â†’ Model metrics                        â”‚   â”‚\n",
        "â”‚   â”‚ PUT  /models/{id}/enable         â†’ Enable/disable model                 â”‚   â”‚\n",
        "â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚\n",
        "â”‚                                                                                  â”‚\n",
        "â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚\n",
        "â”‚   â”‚ RULES                                                                    â”‚   â”‚\n",
        "â”‚   â”‚ GET  /rules                      â†’ List all rules                       â”‚   â”‚\n",
        "â”‚   â”‚ POST /rules                      â†’ Create new rule                      â”‚   â”‚\n",
        "â”‚   â”‚ GET  /rules/{id}                 â†’ Get rule details                     â”‚   â”‚\n",
        "â”‚   â”‚ PUT  /rules/{id}                 â†’ Update rule                          â”‚   â”‚\n",
        "â”‚   â”‚ DELETE /rules/{id}               â†’ Delete rule                          â”‚   â”‚\n",
        "â”‚   â”‚ PUT  /rules/{id}/enable          â†’ Enable/disable rule                  â”‚   â”‚\n",
        "â”‚   â”‚ GET  /rules/{id}/stats           â†’ Rule firing statistics               â”‚   â”‚\n",
        "â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚\n",
        "â”‚                                                                                  â”‚\n",
        "â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚\n",
        "â”‚   â”‚ BEHAVIORS (Real-time behavior tracking)                                  â”‚   â”‚\n",
        "â”‚   â”‚ GET  /behaviors/{user_id}        â†’ Current user behavior snapshot       â”‚   â”‚\n",
        "â”‚   â”‚ GET  /behaviors/{user_id}/baseline â†’ User's baseline behavior           â”‚   â”‚\n",
        "â”‚   â”‚ GET  /behaviors/compare          â†’ Compare users/periods                â”‚   â”‚\n",
        "â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚\n",
        "â”‚                                                                                  â”‚\n",
        "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## Step 10.1: Create Route Files\n",
        "\n",
        "### Implementation Tasks:\n",
        "\n",
        "**Task 10.1.1: Create Dashboard Routes**\n",
        "- Location: `uba/routes/dashboard.py`\n",
        "- Endpoints: summary, risk-distribution, trends\n",
        "\n",
        "**Dashboard Route Structure:**\n",
        "- Create an APIRouter with prefix \"/dashboard\"\n",
        "- Implement GET /summary endpoint:\n",
        "  - Returns: total_users, high_risk_users, open_alerts, open_cases, anomalies_today, average_risk_score\n",
        "- Implement GET /risk-distribution endpoint:\n",
        "  - Returns count of users in each risk category (low, medium, high, critical)\n",
        "- Implement GET /trends endpoint:\n",
        "  - Accept days parameter (default 30)\n",
        "  - Return daily risk trends for charting\n",
        "\n",
        "**Task 10.1.2: Create Profile Routes**\n",
        "- Location: `uba/routes/profiles.py`\n",
        "- User behavioral profiles and history\n",
        "\n",
        "**Task 10.1.3: Create Risk Routes**\n",
        "- Location: `uba/routes/risk.py`\n",
        "- Risk scores and calculations\n",
        "\n",
        "**Task 10.1.4: Create Anomaly Routes**\n",
        "- Location: `uba/routes/anomalies.py`\n",
        "- Anomaly listing and management\n",
        "\n",
        "**Task 10.1.5: Create Alert Routes**\n",
        "- Location: `uba/routes/alerts.py`\n",
        "- Alert queue and management\n",
        "\n",
        "**Task 10.1.6: Create Case Routes**\n",
        "- Location: `uba/routes/cases.py`\n",
        "- Case management endpoints\n",
        "\n",
        "**Task 10.1.7: Create Model Routes**\n",
        "- Location: `uba/routes/models.py`\n",
        "- ML model management\n",
        "\n",
        "**Task 10.1.8: Create Rule Routes**\n",
        "- Location: `uba/routes/rules.py`\n",
        "- Rule CRUD operations\n",
        "\n",
        "**Task 10.1.9: Create Behavior Routes**\n",
        "- Location: `uba/routes/behaviors.py`\n",
        "- Real-time behavior data\n",
        "\n",
        "---\n",
        "\n",
        "## Step 10.2: Create Pydantic Schemas\n",
        "\n",
        "### What Are Schemas?\n",
        "Pydantic schemas define the structure of API requests and responses.\n",
        "They provide validation and documentation.\n",
        "\n",
        "### Implementation Tasks:\n",
        "\n",
        "**Task 10.2.1: Create Base Schemas**\n",
        "- Location: `uba/schemas/base.py`\n",
        "- Create PaginatedResponse schema with: items, total, page, page_size\n",
        "- Create StatusUpdate schema with: status, notes (optional)\n",
        "\n",
        "**Task 10.2.2: Create Profile Schemas**\n",
        "- Location: `uba/schemas/profile.py`\n",
        "- Create UserProfileResponse with fields:\n",
        "  - user_id, email, department, role, risk_score, risk_level\n",
        "  - last_activity, anomaly_count, alert_count, baseline_established\n",
        "\n",
        "**Task 10.2.3: Create Risk Schemas**\n",
        "- Location: `uba/schemas/risk.py`\n",
        "- Create RiskScoreResponse with: user_id, composite_score, risk_level, components, calculated_at\n",
        "- Create RiskTrendPoint with: date, score\n",
        "\n",
        "**Task 10.2.4: Create Anomaly Schemas**\n",
        "- Location: `uba/schemas/anomaly.py`\n",
        "- Create AnomalyResponse with: id, user_id, anomaly_type, score, details, status, detected_at\n",
        "\n",
        "**Task 10.2.5: Create Alert Schemas**\n",
        "- Location: `uba/schemas/alert.py`\n",
        "- Create AlertResponse with: id, user_id, title, description, severity, priority_score, status, assigned_to, created_at, mitre_technique\n",
        "- Create AlertAssignment with: assignee\n",
        "\n",
        "**Task 10.2.6: Create Case Schemas**\n",
        "- Location: `uba/schemas/case.py`\n",
        "- Create CaseCreate with: title, description, user_id, alert_ids\n",
        "- Create CaseResponse with: id, title, user_id, alert_count, severity, status, assignee, created_at\n",
        "- Create CaseNote with: content\n",
        "- Create CaseResolution with: outcome (confirmed_threat/false_positive/inconclusive), notes\n",
        "\n",
        "**Task 10.2.7: Create Rule Schemas**\n",
        "- Location: `uba/schemas/rule.py`\n",
        "- Create RuleCondition with: field, operator, value\n",
        "- Create RuleCreate with: name, description, conditions, logic, severity, score, enabled, mitre_technique\n",
        "\n",
        "---\n",
        "\n",
        "## Step 10.3: Register Routes in Main App\n",
        "\n",
        "### Implementation Tasks:\n",
        "\n",
        "**Task 10.3.1: Create Router Registry**\n",
        "- Location: `uba/routes/__init__.py`\n",
        "- Import all route modules (dashboard, profiles, risk, anomalies, alerts, cases, models, rules, behaviors)\n",
        "- Create a main uba_router with prefix \"/api/v1/uba\"\n",
        "- Include all sub-routers using include_router()\n",
        "\n",
        "**Task 10.3.2: Register in DSPM Main App**\n",
        "- Import uba_router in DSPM's main.py\n",
        "- Call app.include_router(uba_router) to register all UBA endpoints\n",
        "\n",
        "---\n",
        "\n",
        "## Step 10.4: Add Authentication & Authorization\n",
        "\n",
        "### Implementation Tasks:\n",
        "\n",
        "**Task 10.4.1: Integrate with DSPM's Keycloak Auth**\n",
        "- Use DSPM's existing `get_current_user` dependency\n",
        "- Require authentication on all UBA endpoints\n",
        "\n",
        "**Task 10.4.2: Define UBA Permissions**\n",
        "\n",
        "| Permission | Description | Roles |\n",
        "|------------|-------------|-------|\n",
        "| uba:view | View dashboards, profiles, scores | All users |\n",
        "| uba:analyze | Trigger analysis, view details | Analysts |\n",
        "| uba:manage_alerts | Update alerts, assign | Analysts, Managers |\n",
        "| uba:manage_cases | Create, update, close cases | Analysts, Managers |\n",
        "| uba:manage_rules | Create, update, delete rules | Admins |\n",
        "| uba:manage_models | Train, enable/disable models | Admins |\n",
        "\n",
        "**Task 10.4.3: Create Permission Decorator**\n",
        "- Create a require_permission decorator function\n",
        "- It should check if current_user has the required permission\n",
        "- Raise HTTP 403 error if permission denied\n",
        "- Use functools.wraps to preserve function metadata\n",
        "\n",
        "### Verification Checklist:\n",
        "- [ ] All routes created\n",
        "- [ ] Schemas validate input/output\n",
        "- [ ] Routes registered in app\n",
        "- [ ] Authentication required\n",
        "- [ ] Permissions enforced\n",
        "\n",
        "---\n",
        "\n",
        "# ðŸ”— PHASE 11: DSPM INTEGRATION\n",
        "\n",
        "## Goal: Connect UBA_PRO to existing DSPM data and workflows\n",
        "\n",
        "---\n",
        "\n",
        "## Integration Architecture\n",
        "\n",
        "```\n",
        "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
        "â”‚                         DSPM + UBA INTEGRATION                                   â”‚\n",
        "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
        "â”‚                                                                                  â”‚\n",
        "â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚\n",
        "â”‚   â”‚                          DSPM CORE                                       â”‚   â”‚\n",
        "â”‚   â”‚                                                                          â”‚   â”‚\n",
        "â”‚   â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚   â”‚\n",
        "â”‚   â”‚  â”‚   Users    â”‚  â”‚  Sessions  â”‚  â”‚   Policy   â”‚  â”‚   Assets   â”‚        â”‚   â”‚\n",
        "â”‚   â”‚  â”‚            â”‚  â”‚            â”‚  â”‚   Audit    â”‚  â”‚            â”‚        â”‚   â”‚\n",
        "â”‚   â”‚  â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜        â”‚   â”‚\n",
        "â”‚   â”‚        â”‚               â”‚               â”‚               â”‚               â”‚   â”‚\n",
        "â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚\n",
        "â”‚            â”‚               â”‚               â”‚               â”‚                    â”‚\n",
        "â”‚            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                    â”‚\n",
        "â”‚                                    â”‚                                             â”‚\n",
        "â”‚                                    â–¼                                             â”‚\n",
        "â”‚            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                    â”‚\n",
        "â”‚            â”‚           DSPM INTEGRATION SERVICE            â”‚                    â”‚\n",
        "â”‚            â”‚                                               â”‚                    â”‚\n",
        "â”‚            â”‚  â€¢ Reads DSPM tables for behavioral data      â”‚                    â”‚\n",
        "â”‚            â”‚  â€¢ Transforms to UBA format                   â”‚                    â”‚\n",
        "â”‚            â”‚  â€¢ Handles real-time event streaming          â”‚                    â”‚\n",
        "â”‚            â”‚  â€¢ Syncs user information                     â”‚                    â”‚\n",
        "â”‚            â”‚                                               â”‚                    â”‚\n",
        "â”‚            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                    â”‚\n",
        "â”‚                                    â”‚                                             â”‚\n",
        "â”‚                                    â–¼                                             â”‚\n",
        "â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚\n",
        "â”‚   â”‚                           UBA_PRO                                        â”‚   â”‚\n",
        "â”‚   â”‚                                                                          â”‚   â”‚\n",
        "â”‚   â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚   â”‚\n",
        "â”‚   â”‚  â”‚  Profiles  â”‚  â”‚   Risk     â”‚  â”‚  Anomaly   â”‚  â”‚   Alert    â”‚        â”‚   â”‚\n",
        "â”‚   â”‚  â”‚            â”‚  â”‚   Engine   â”‚  â”‚   Engine   â”‚  â”‚   Engine   â”‚        â”‚   â”‚\n",
        "â”‚   â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚   â”‚\n",
        "â”‚   â”‚                                                                          â”‚   â”‚\n",
        "â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚\n",
        "â”‚                                                                                  â”‚\n",
        "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## Step 11.1: DSPM Data Integration Service\n",
        "\n",
        "### What This Does:\n",
        "Reads behavioral data from DSPM tables and transforms for UBA analysis.\n",
        "\n",
        "### Implementation Tasks:\n",
        "\n",
        "**Task 11.1.1: Create DSPMIntegrationService**\n",
        "- Location: `uba/services/dspm_integration.py`\n",
        "- Main interface between DSPM and UBA\n",
        "\n",
        "**Task 11.1.2: Implement User Sync**\n",
        "- Method: `sync_users()` â†’ sync DSPM users to UBA profiles\n",
        "- Map DSPM user fields to UBA profile fields\n",
        "- Run daily to catch new users\n",
        "\n",
        "```python\n",
        "class DSPMIntegrationService:\n",
        "    def __init__(self, db: Session):\n",
        "        self.db = db\n",
        "    \n",
        "    def sync_users(self):\n",
        "        \"\"\"\n",
        "        Sync users from DSPM identity tables to UBA profiles\n",
        "        \"\"\"\n",
        "        # Get users from DSPM\n",
        "        dspm_users = self.db.query(DSPMUser).all()\n",
        "        \n",
        "        for user in dspm_users:\n",
        "            # Check if profile exists\n",
        "            profile = self.db.query(UBAProfile).filter_by(\n",
        "                dspm_user_id=user.id\n",
        "            ).first()\n",
        "            \n",
        "            if not profile:\n",
        "                # Create new profile\n",
        "                profile = UBAProfile(\n",
        "                    dspm_user_id=user.id,\n",
        "                    email=user.email,\n",
        "                    department=user.department,\n",
        "                    # ... other fields\n",
        "                )\n",
        "                self.db.add(profile)\n",
        "        \n",
        "        self.db.commit()\n",
        "```\n",
        "\n",
        "**Task 11.1.3: Implement Session Data Extraction**\n",
        "\n",
        "âš ï¸ **NOTE:** DSPM does NOT have a `user_sessions` table!\n",
        "\n",
        "- Method: `get_user_sessions(user_id, days=30)` â†’ session data\n",
        "- **Requires External Data Source** (see Phase 11.5: External Log Integration)\n",
        "- Options: Okta API, Azure AD logs, LDAP authentication logs\n",
        "- Until external integration: Use `policy_audit_log` timestamps as proxy for activity sessions\n",
        "\n",
        "**Task 11.1.4: Implement Policy Audit Extraction**\n",
        "- Method: `get_policy_events(user_id, days=30)` â†’ policy events\n",
        "- Query DSPM's policy_audit_log table\n",
        "- Extract behavioral signals\n",
        "\n",
        "**Task 11.1.5: Implement Access Pattern Extraction**\n",
        "- Method: `get_access_patterns(user_id, days=30)` â†’ access data\n",
        "- Query DSPM's access_controls and related tables\n",
        "- Build access behavior profile\n",
        "\n",
        "---\n",
        "\n",
        "## Step 11.2: Data Transformation Layer\n",
        "\n",
        "### What This Does:\n",
        "Converts DSPM data format to UBA feature format.\n",
        "\n",
        "### Implementation Tasks:\n",
        "\n",
        "**Task 11.2.1: Create Data Transformers**\n",
        "- Location: `uba/services/transformers.py`\n",
        "âš ï¸ **REQUIRES EXTERNAL DATA** - See Phase 11.5\n",
        "\n",
        "```python\n",
        "def transform_sessions(sessions: List[ExternalAuthSession]) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Transform EXTERNAL authentication sessions to UBA format\n",
        "    \n",
        "    NOTE: This data comes from EXTERNAL sources (Okta, Azure AD, etc.)\n",
        "    NOT from DSPM database!\n",
        "    \n",
        "    Input (External Auth format - varies by source):\n",
        "    {\n",
        "        \"id\": \"...\",\n",
        "        \"user_id\": \"...\",\n",
        "        \"ip_address\": \"192.168.1.100\",\n",
        "        \"user_agent\": \"Mozilla/5.0...\",\n",
        "        \"location\": \"New York\",\n",
        "        \"created_at\": \"2026-01-07T10:00:00Z\"\n",
        "    }\n",
        "    \n",
        "    Output (UBA format):\n",
        "    {\n",
        "        \"user_id\": \"...\",\n",
        "        \"login_hour\": 10,\n",
        "        \"login_day\": 2,  # Tuesday\n",
        "        \"ip_address\": \"192.168.1.100\",\n",
        "        \"ip_is_new\": False,\n",
        "        \"location\": \"New York\",\n",
        "        \"location_is_new\": False,\n",
        "        \"device_fingerprint\": \"abc123\",\n",
        "        \"device_is_new\": False\n",
        "    }\n",
        "    \"\"\"\n",
        "    # Implementation requires external log integration first\n",
        "    pass\n",
        "```\n",
        "\n",
        "**Task 11.2.3: Policy Event Transformer**\n",
        "\n",
        "```python\n",
        "def transform_policy_events(events: List[PolicyAuditLog]) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Transform policy events to UBA format\n",
        "    \n",
        "    Input (DSPM format):\n",
        "    {\n",
        "        \"event_type\": \"data_access\",\n",
        "        \"severity\": \"medium\",\n",
        "        \"resource_accessed\": \"sensitive_file.csv\"\n",
        "    }\n",
        "    \n",
        "    Output (UBA format):\n",
        "    {\n",
        "        \"user_id\": \"...\",\n",
        "        \"event_count\": 45,\n",
        "        \"high_severity_count\": 3,\n",
        "        \"sensitive_access_count\": 12,\n",
        "        \"unique_resources\": 8\n",
        "    }\n",
        "    \"\"\"\n",
        "    pass\n",
        "```\n",
        "\n",
        "**Task 11.2.4: Access Pattern Transformer**\n",
        "\n",
        "```python\n",
        "def transform_access_patterns(accesses: List[AccessControl]) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Transform access patterns to UBA format\n",
        "    \"\"\"\n",
        "    pass\n",
        "```\n",
        "\n",
        "---\n",
        "## Step 11.3: Real-time Event Integration\n",
        "## Step 11.3: Real-time Event Integration\n",
        "### What This Does:\n",
        "### What This Does:\n",
        "Processes DSPM events in real-time for immediate analysis.\n",
        "\n",
        "### Implementation Tasks:\n",
        "\n",
        "**Task 11.3.1: Create Event Handler**\n",
        "- Location: `uba/services/event_handler.py`\n",
        "- Process incoming DSPM events\n",
        "\n",
        "**Task 11.3.2: Define Event Types**\n",
        "\n",
        "| DSPM Event | UBA Action | Available in DSPM? |\n",
        "|------------|------------|-------------------|\n",
        "| user_login | Update session data, check login rules | âŒ NO - requires external integration |\n",
        "| data_access | Update access patterns, check access rules | âœ… YES (via policy_audit_log) |\n",
        "| policy_violation | Immediate anomaly check, possible alert | âœ… YES (policy_violations table) |\n",
        "| privilege_change | Check privilege escalation rules | âœ… YES (identity_mappings changes) |\n",
        "| file_transfer | Check data exfiltration rules | âš ï¸ PARTIAL (assets_details only) |\n",
        "\n",
        "âš ï¸ **NOTE:** `user_login` events are NOT available in DSPM!\n",
        "See Phase 11.6: External Log Integration for how to obtain this data.\n",
        "\n",
        "**Task 11.3.3: Implement Event Processing**\n",
        "\n",
        "```python\n",
        "class UBAEventHandler:\n",
        "    def handle_event(self, event: dict):\n",
        "        \"\"\"\n",
        "        Process a DSPM event in real-time\n",
        "        \"\"\"\n",
        "        event_type = event.get(\"type\")\n",
        "        \n",
        "        if event_type == \"user_login\":\n",
        "            self._handle_login(event)\n",
        "        elif event_type == \"data_access\":\n",
        "            self._handle_access(event)\n",
        "        elif event_type == \"policy_violation\":\n",
        "            self._handle_violation(event)\n",
        "        # ... other event types\n",
        "    \n",
        "    def _handle_login(self, event):\n",
        "        # 1. Get user profile\n",
        "        # 2. Check login rules\n",
        "        # 3. Update session data\n",
        "        # 4. Trigger quick risk check if rules match\n",
        "        pass\n",
        "```\n",
        "**Task 11.3.4: Integrate with DSPM Event System**\n",
        "**Task 11.3.4: Integrate with DSPM Event System**\n",
        "- Hook into DSPM's event publishing\n",
        "- Subscribe to relevant event types\n",
        "- Process asynchronously\n",
        "\n",
        "---\n",
        "## Step 11.4: DSPM UI Integration\n",
        "## Step 11.4: DSPM UI Integration\n",
        "### What This Does:\n",
        "### What This Does:\n",
        "Adds UBA components to existing DSPM dashboard.\n",
        "\n",
        "### Implementation Tasks:\n",
        "**Task 11.4.1: Add UBA Widget to DSPM Dashboard**\n",
        "**Task 11.4.1: Add UBA Widget to DSPM Dashboard**\n",
        "- Show high-risk users summary\n",
        "- Show recent alerts count\n",
        "- Link to full UBA dashboard\n",
        "\n",
        "**Task 11.4.2: Add Risk Indicator to User Pages**\n",
        "- Show risk score on user profile pages\n",
        "- Add link to detailed UBA profile\n",
        "\n",
        "**Task 11.4.3: Integrate Alerts with DSPM Notifications**\n",
        "- UBA alerts appear in DSPM notification center\n",
        "- Use same notification UI components\n",
        "\n",
        "**Task 11.4.4: Add UBA Section to Navigation**\n",
        "- Add \"User Analytics\" or \"Behavior Analysis\" menu item\n",
        "- Link to UBA dashboard and sub-pages\n",
        "\n",
        "---\n",
        "## Step 11.5: Policy Violation Integration\n",
        "## Step 11.5: Policy Violation Integration\n",
        "### What This Does:\n",
        "### What This Does:\n",
        "Uses DSPM's existing policy violations as LABELED DATA for training.\n",
        "### This is CRITICAL for Training:\n",
        "### This is CRITICAL for Training:\n",
        "\n",
        "```\n",
        "DSPM Policy Violations = LABELED TRAINING DATA\n",
        "policy_violations table:\n",
        "policy_violations table:\n",
        "â”œâ”€â”€ status = \"open\"          â†’ Potential anomaly (unlabeled)\n",
        "â”œâ”€â”€ status = \"resolved\"      â†’ Confirmed issue (positive label)\n",
        "â”œâ”€â”€ status = \"false_positive\" â†’ Not an issue (negative label)\n",
        "\n",
        "USE THIS FOR:\n",
        "1. Initial model training (historical violations)\n",
        "2. Continuous learning (new labeled violations)\n",
        "3. Model evaluation (how many we catch)\n",
        "```\n",
        "\n",
        "### Implementation Tasks:\n",
        "**Task 11.5.1: Create Training Data Extractor**\n",
        "**Task 11.5.1: Create Training Data Extractor**\n",
        "- Extract labeled violations from DSPM\n",
        "- Transform to ML training format\n",
        "- Create training/test split\n",
        "**Task 11.5.2: Implement Feedback Loop**\n",
        "**Task 11.5.2: Implement Feedback Loop**\n",
        "- When analyst marks alert as false positive â†’ feed back to model\n",
        "- When analyst confirms threat â†’ feed back to model\n",
        "- Continuous model improvement\n",
        "\n",
        "### Verification Checklist:\n",
        "- [ ] User sync works\n",
        "- [ ] Session data extracted correctly\n",
        "- [ ] Policy events extracted correctly\n",
        "- [ ] Real-time events processed\n",
        "- [ ] UI integration complete\n",
        "- [ ] Training data extracted from violations\n",
        "\n",
        "---\n",
        "\n",
        "# ðŸ”Œ PHASE 11.5: EXTERNAL LOG INTEGRATION\n",
        "\n",
        "## Goal: Connect external data sources to enable login/session analysis\n",
        "\n",
        "---\n",
        "\n",
        "## âš ï¸ WHY THIS PHASE IS CRITICAL\n",
        "\n",
        "**DSPM does NOT store user login/session data.**\n",
        "\n",
        "The following UBA features REQUIRE external data sources:\n",
        "\n",
        "| Feature | Requires | External Source Options |\n",
        "|---------|----------|------------------------|\n",
        "| Login Anomaly Detection | Session logs | Okta, Azure AD, Auth0, LDAP |\n",
        "| IP-based analysis | IP addresses | Firewall logs, VPN logs |\n",
        "| Time-based analysis | Login timestamps | SSO provider logs |\n",
        "| Device fingerprinting | Device info | MDM, SSO provider |\n",
        "| Geo-location analysis | Location data | SSO provider, VPN |\n",
        "\n",
        "**Without this phase, Login Anomaly Model (Step 6.2) cannot function!**\n",
        "\n",
        "---\n",
        "\n",
        "## Step 11.5.1: Choose External Data Sources\n",
        "\n",
        "### Option A: SSO Provider Integration (RECOMMENDED)\n",
        "\n",
        "```\n",
        "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
        "â”‚                     SSO PROVIDER INTEGRATION                                     â”‚\n",
        "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
        "â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    API Call    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    ETL    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚\n",
        "â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    API Call    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    ETL    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚\n",
        "â”‚   â”‚   Okta/      â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’â”‚  UBA_PRO     â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â†’â”‚  PostgreSQL  â”‚  â”‚\n",
        "â”‚   â”‚   Azure AD   â”‚                â”‚  Connector   â”‚           â”‚  (sessions)  â”‚  â”‚\n",
        "â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚\n",
        "â”‚                                                                                  â”‚\n",
        "â”‚   Provides:                                                                      â”‚\n",
        "â”‚   âœ… User login events         âœ… Device info                                   â”‚\n",
        "â”‚   âœ… IP addresses              âœ… User agent                                    â”‚\n",
        "â”‚   âœ… Timestamps                âœ… Success/failure                               â”‚\n",
        "â”‚   âœ… MFA status                âœ… Geo-location                                  â”‚\n",
        "â”‚                                                                                  â”‚\n",
        "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
        "```\n",
        "\n",
        "### Option B: SIEM Integration\n",
        "- Splunk API\n",
        "- Splunk API\n",
        "- Elastic SIEM\n",
        "- QRadar\n",
        "- LogRhythm\n",
        "\n",
        "### Option C: Direct Log Ingestion\n",
        "\n",
        "- Syslog collector\n",
        "- AWS CloudWatch Logs\n",
        "- Azure Monitor Logs\n",
        "\n",
        "---\n",
        "\n",
        "## Step 11.5.2: Create External Connector Interface\n",
        "### Implementation Tasks:\n",
        "### Implementation Tasks:\n",
        "\n",
        "**Task 11.5.2.1: Create Base Connector**\n",
        "- Location: `uba/connectors/external_auth_connector.py`\n",
        "```python\n",
        "```python\n",
        "from abc import ABC, abstractmethod\n",
        "from typing import List, Dict, Any\n",
        "from datetime import datetime\n",
        "\n",
        "class ExternalAuthConnector(ABC):\n",
        "    \\\"\\\"\\\"\n",
        "    Base class for external authentication log connectors.\n",
        "    Implement this for each external data source.\n",
        "    \\\"\\\"\\\"\n",
        "    @abstractmethod\n",
        "    def connect(self) -> bool:\n",
        "    def connect(self) -> bool:\n",
        "        \\\"\\\"\\\"Establish connection to external source\\\"\\\"\\\"\n",
        "        pass\n",
        "    \n",
        "    @abstractmethod\n",
        "    def get_login_events(\n",
        "        self,\n",
        "        user_id: str = None,\n",
        "        start_date: datetime = None,\n",
        "        end_date: datetime = None\n",
        "    ) -> List[Dict[str, Any]]:\n",
        "        \\\"\\\"\\\"\n",
        "        Retrieve login events from external source.\n",
        "        \n",
        "        Returns list of events with standardized fields:\n",
        "        - user_id: str\n",
        "        - timestamp: datetime\n",
        "        - ip_address: str\n",
        "        - user_agent: str (optional)\n",
        "        - device_id: str (optional)\n",
        "        - geo_location: dict (optional)\n",
        "        - success: bool\n",
        "        - mfa_used: bool (optional)\n",
        "        \\\"\\\"\\\"\n",
        "    \n",
        "    \n",
        "    @abstractmethod\n",
        "    def health_check(self) -> bool:\n",
        "        \\\"\\\"\\\"Check if connection is healthy\\\"\\\"\\\"\n",
        "        pass\n",
        "```\n",
        "\n",
        "**Task 11.5.2.2: Create Okta Connector (Example)**\n",
        "- Location: `uba/connectors/okta_connector.py`\n",
        "\n",
        "```python\n",
        "import requests\n",
        "from uba.connectors.external_auth_connector import ExternalAuthConnector\n",
        "\n",
        "class OktaConnector(ExternalAuthConnector):\n",
        "    \\\"\\\"\\\"\n",
        "    Connector for Okta authentication logs.\n",
        "    Requires: OKTA_DOMAIN, OKTA_API_TOKEN in environment.\n",
        "    \\\"\\\"\\\"\n",
        "    def __init__(self, domain: str, api_token: str):\n",
        "    def __init__(self, domain: str, api_token: str):\n",
        "        self.domain = domain\n",
        "        self.api_token = api_token\n",
        "        self.base_url = f\\\"https://{domain}/api/v1\\\"\n",
        "    \n",
        "    def connect(self) -> bool:\n",
        "        # Verify credentials\n",
        "        return self.health_check()\n",
        "    \n",
        "    def get_login_events(self, user_id=None, start_date=None, end_date=None):\n",
        "        # Call Okta System Log API\n",
        "        # Filter by eventType eq \\\"user.session.start\\\"\n",
        "        # Transform to standard format\n",
        "        pass\n",
        "    \n",
        "    def health_check(self) -> bool:\n",
        "        try:\n",
        "            response = requests.get(\n",
        "                f\\\"{self.base_url}/users/me\\\",\n",
        "                headers={\\\"Authorization\\\": f\\\"SSWS {self.api_token}\\\"}\n",
        "            )\n",
        "            return response.status_code == 200\n",
        "        except:\n",
        "            return False\n",
        "```\n",
        "**Task 11.5.2.3: Create Azure AD Connector (Example)**\n",
        "**Task 11.5.2.3: Create Azure AD Connector (Example)**\n",
        "- Location: `uba/connectors/azure_ad_connector.py`\n",
        "- Uses Microsoft Graph API\n",
        "- Requires: tenant_id, client_id, client_secret\n",
        "**Task 11.5.2.4: Create Mock Connector (For Testing)**\n",
        "**Task 11.5.2.4: Create Mock Connector (For Testing)**\n",
        "- Location: `uba/connectors/mock_auth_connector.py`\n",
        "- Generates synthetic login data\n",
        "- USE FOR DEVELOPMENT ONLY\n",
        "\n",
        "---\n",
        "\n",
        "## Step 11.5.3: Create Sessions Table\n",
        "\n",
        "**Since DSPM doesn't have user_sessions, UBA_PRO must create it:**\n",
        "### Implementation Tasks:\n",
        "### Implementation Tasks:\n",
        "\n",
        "**Task 11.5.3.1: Add Sessions Model**\n",
        "- Location: `uba/models/sessions.py`\n",
        "\n",
        "```python\n",
        "from sqlalchemy import Column, String, DateTime, Boolean, JSON\n",
        "from uba.models.base import Base\n",
        "\n",
        "class UserSession(Base):\n",
        "    __tablename__ = \\\"uba_user_sessions\\\"\n",
        "    \n",
        "    id = Column(String, primary_key=True)\n",
        "    user_id = Column(String, index=True, nullable=False)\n",
        "    timestamp = Column(DateTime, index=True, nullable=False)\n",
        "    ip_address = Column(String)\n",
        "    user_agent = Column(String)\n",
        "    device_id = Column(String)\n",
        "    geo_location = Column(JSON)  # {\\\"city\\\": \\\"...\\\", \\\"country\\\": \\\"...\\\"}\n",
        "    success = Column(Boolean, default=True)\n",
        "    mfa_used = Column(Boolean)\n",
        "    source = Column(String)  # \\\"okta\\\", \\\"azure_ad\\\", etc.\n",
        "    raw_event = Column(JSON)  # Store original event for debugging\n",
        "```\n",
        "\n",
        "**Task 11.5.3.2: Create Migration**\n",
        "- Add Alembic migration for uba_user_sessions table\n",
        "- Run migration: `alembic upgrade head`\n",
        "\n",
        "---\n",
        "\n",
        "## Step 11.5.4: Create ETL Service\n",
        "### Implementation Tasks:\n",
        "### Implementation Tasks:\n",
        "\n",
        "**Task 11.5.4.1: Create Session ETL Service**\n",
        "- Location: `uba/services/session_etl_service.py`\n",
        "\n",
        "```python\n",
        "class SessionETLService:\n",
        "    \\\"\\\"\\\"\n",
        "    Extract, Transform, Load session data from external sources.\n",
        "    \\\"\\\"\\\"\n",
        "    \n",
        "    def __init__(self, connector: ExternalAuthConnector, db: Session):\n",
        "        self.connector = connector\n",
        "        self.db = db\n",
        "    \n",
        "    def sync_sessions(self, days: int = 7):\n",
        "        \\\"\\\"\\\"\n",
        "        Sync sessions from external source to local database.\n",
        "        Run as scheduled job (hourly or daily).\n",
        "        \\\"\\\"\\\"\n",
        "        # 1. Get last sync timestamp\n",
        "        # 2. Fetch new events from connector\n",
        "        # 3. Transform to UserSession objects\n",
        "        # 4. Insert into database\n",
        "        # 5. Update last sync timestamp\n",
        "        pass\n",
        "```\n",
        "**Task 11.5.4.2: Create Scheduled Job**\n",
        "**Task 11.5.4.2: Create Scheduled Job**\n",
        "- Add to scheduler (APScheduler or similar)\n",
        "- Run every 1 hour\n",
        "- Configurable via settings\n",
        "\n",
        "---\n",
        "## Step 11.5.5: Configuration\n",
        "## Step 11.5.5: Configuration\n",
        "\n",
        "**Task 11.5.5.1: Add External Auth Settings**\n",
        "- Location: `uba/config/settings.py`\n",
        "```python\n",
        "```python\n",
        "# External Authentication Log Settings\n",
        "EXTERNAL_AUTH_ENABLED: bool = False\n",
        "EXTERNAL_AUTH_PROVIDER: str = \\\"okta\\\"  # okta, azure_ad, mock\n",
        "\n",
        "# Okta Settings (if using Okta)\n",
        "OKTA_DOMAIN: str = \\\"\\\"\n",
        "OKTA_API_TOKEN: str = \\\"\\\"\n",
        "# Azure AD Settings (if using Azure)\n",
        "# Azure AD Settings (if using Azure)\n",
        "AZURE_TENANT_ID: str = \\\"\\\"\n",
        "AZURE_CLIENT_ID: str = \\\"\\\"\n",
        "AZURE_CLIENT_SECRET: str = \\\"\\\"\n",
        "# ETL Settings\n",
        "# ETL Settings\n",
        "SESSION_SYNC_INTERVAL_MINUTES: int = 60\n",
        "SESSION_SYNC_LOOKBACK_DAYS: int = 30\n",
        "```\n",
        "\n",
        "---\n",
        "## âš ï¸ IMPORTANT NOTES\n",
        "## âš ï¸ IMPORTANT NOTES\n",
        "\n",
        "1. **This phase is OPTIONAL for initial deployment** - You can deploy UBA_PRO\n",
        "   without external logs, but Login Anomaly Detection will not work.\n",
        "\n",
        "2. **Start with Mock Connector** - Use mock data for development and testing,\n",
        "   then switch to real connector in production.\n",
        "\n",
        "3. **Data Volume Considerations** - External auth logs can be HUGE.\n",
        "   Implement pagination and consider data retention policies.\n",
        "\n",
        "4. **Security** - Store API tokens securely (environment variables, vault).\n",
        "   Never commit credentials to code.\n",
        "\n",
        "### Verification Checklist:\n",
        "- [ ] External connector implemented (at least mock)\n",
        "- [ ] Sessions table created\n",
        "- [ ] ETL service syncs data correctly\n",
        "- [ ] Scheduled job runs on schedule\n",
        "- [ ] Settings configurable via environment\n",
        "- [ ] Health check endpoint for external connection\n",
        "\n",
        "---\n",
        "\n",
        "# ðŸ§ª PHASE 12: TESTING & DEPLOYMENT\n",
        "\n",
        "## Goal: Comprehensive testing and production deployment\n",
        "\n",
        "---\n",
        "\n",
        "## Testing Strategy\n",
        "\n",
        "```\n",
        "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
        "â”‚                           TESTING PYRAMID                                        â”‚\n",
        "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
        "â”‚                                                                                  â”‚\n",
        "â”‚                              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                       â”‚\n",
        "â”‚                              â”‚   E2E     â”‚  â—„â”€â”€ Slow, expensive, few            â”‚\n",
        "â”‚                              â”‚   Tests   â”‚      (10-20 tests)                   â”‚\n",
        "â”‚                              â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜                                       â”‚\n",
        "â”‚                           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”                                    â”‚\n",
        "â”‚                           â”‚  Integration    â”‚  â—„â”€â”€ Medium speed                 â”‚\n",
        "â”‚                           â”‚     Tests       â”‚      (50-100 tests)               â”‚\n",
        "â”‚                           â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                    â”‚\n",
        "â”‚                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                            â”‚\n",
        "â”‚                    â”‚         Unit Tests            â”‚  â—„â”€â”€ Fast, many            â”‚\n",
        "â”‚                    â”‚                               â”‚      (200+ tests)          â”‚\n",
        "â”‚                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                            â”‚\n",
        "â”‚                                                                                  â”‚\n",
        "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
        "```\n",
        "\n",
        "---\n",
        "## Step 12.1: Unit Testing\n",
        "\n",
        "### What to Test:\n",
        "Every individual function and class method.\n",
        "\n",
        "### OpenUBA Reference Test Files:\n",
        "OpenUBA includes test files you can use as starting points:\n",
        "- `core/user_test.py` - User management tests\n",
        "- `core/model_test.py` - Model system tests\n",
        "- `core/hash_test.py` - Hash utility tests\n",
        "- `core/encode_test.py` - Encoding utility tests\n",
        "- `core/entity_test.py` - Entity management tests\n",
        "- `core/dataset_test.py` - Dataset loading tests\n",
        "- `core/display_test.py` - Display service tests\n",
        "- `core/process_test.py` - Process engine tests\n",
        "\n",
        "### Implementation Tasks:\n",
        "\n",
        "**Task 12.1.1: Test Feature Extraction**\n",
        "- Location: `uba/tests/test_feature_extractor.py`\n",
        "\n",
        "```python\n",
        "import pytest\n",
        "from uba.analytics.feature_extractor import FeatureExtractor\n",
        "\n",
        "class TestFeatureExtractor:\n",
        "    \n",
        "    def test_extract_login_features(self):\n",
        "        \"\"\"Test login feature extraction\"\"\"\n",
        "        extractor = FeatureExtractor()\n",
        "        sessions = [...]  # Mock session data\n",
        "        \n",
        "        features = extractor.extract_login_features(sessions)\n",
        "        \n",
        "        assert \"login_count\" in features\n",
        "        assert \"unique_ips\" in features\n",
        "        assert features[\"login_count\"] >= 0\n",
        "    \n",
        "    def test_empty_input(self):\n",
        "        \"\"\"Test handling of empty data\"\"\"\n",
        "        extractor = FeatureExtractor()\n",
        "        features = extractor.extract_login_features([])\n",
        "        \n",
        "        assert features[\"login_count\"] == 0\n",
        "```\n",
        "\n",
        "**Task 12.1.2: Test Risk Calculator**\n",
        "- Location: `uba/tests/test_risk_calculator.py`\n",
        "\n",
        "```python\n",
        "class TestRiskCalculator:\n",
        "    \n",
        "    def test_calculate_composite_score(self):\n",
        "        calculator = RiskCalculator()\n",
        "        scores = {\n",
        "            \"login\": 0.8,\n",
        "            \"access\": 0.6,\n",
        "            \"transfer\": 0.4,\n",
        "            \"time\": 0.2\n",
        "        }\n",
        "        \n",
        "        result = calculator.calculate(scores)\n",
        "        assert 0 <= result <= 1\n",
        "    \n",
        "    def test_classify_risk_level(self):\n",
        "        calculator = RiskCalculator()\n",
        "        \n",
        "        assert calculator.classify(0.1) == \"LOW\"\n",
        "        assert calculator.classify(0.4) == \"MEDIUM\"\n",
        "        assert calculator.classify(0.7) == \"HIGH\"\n",
        "        assert calculator.classify(0.9) == \"CRITICAL\"\n",
        "```\n",
        "\n",
        "**Task 12.1.3: Test ML Models**\n",
        "- Test model training\n",
        "- Test prediction\n",
        "- Test serialization/deserialization\n",
        "\n",
        "**Task 12.1.4: Test Services**\n",
        "- Test each service method\n",
        "- Mock database sessions\n",
        "- Test error handling\n",
        "\n",
        "**Task 12.1.5: Test Rule Engine**\n",
        "- Test each operator\n",
        "- Test condition evaluation\n",
        "- Test rule matching\n",
        "\n",
        "---\n",
        "## Step 12.2: Integration Testing\n",
        "\n",
        "### What to Test:\n",
        "Multiple components working together.\n",
        "\n",
        "### Implementation Tasks:\n",
        "\n",
        "**Task 12.2.1: Test API Endpoints**\n",
        "- Location: `uba/tests/test_api.py`\n",
        "\n",
        "```python\n",
        "from fastapi.testclient import TestClient\n",
        "from uba.main import app\n",
        "\n",
        "client = TestClient(app)\n",
        "\n",
        "class TestDashboardAPI:\n",
        "    \n",
        "    def test_get_summary(self, auth_headers):\n",
        "        response = client.get(\n",
        "            \"/api/v1/uba/dashboard/summary\",\n",
        "            headers=auth_headers\n",
        "        )\n",
        "        \n",
        "        assert response.status_code == 200\n",
        "        data = response.json()\n",
        "        assert \"total_users\" in data\n",
        "        assert \"high_risk_users\" in data\n",
        "\n",
        "class TestAlertAPI:\n",
        "    \n",
        "    def test_create_and_update_alert(self, auth_headers):\n",
        "        # Create scenario that generates alert\n",
        "        # Verify alert was created\n",
        "        # Update alert status\n",
        "        # Verify status changed\n",
        "        pass\n",
        "```\n",
        "**Task 12.2.2: Test Database Operations**\n",
        "**Task 12.2.2: Test Database Operations**\n",
        "- Test CRUD for all models\n",
        "- Test relationships\n",
        "- Test constraints\n",
        "**Task 12.2.3: Test DSPM Integration**\n",
        "**Task 12.2.3: Test DSPM Integration**\n",
        "- Test data extraction\n",
        "- Test transformation\n",
        "- Test sync operations\n",
        "\n",
        "---\n",
        "## Step 12.3: End-to-End Testing\n",
        "## Step 12.3: End-to-End Testing\n",
        "### What to Test:\n",
        "### What to Test:\n",
        "Complete user workflows from start to finish.\n",
        "\n",
        "### Implementation Tasks:\n",
        "\n",
        "**Task 12.3.1: Define E2E Test Scenarios**\n",
        "\n",
        "| Scenario | Steps | Expected Result |\n",
        "|----------|-------|-----------------|\n",
        "| New User Detection | 1. Add user to DSPM<br>2. User logs in<br>3. Run sync | UBA profile created |\n",
        "| Anomaly to Alert | 1. Generate anomalous behavior<br>2. Run analysis<br>3. Check alerts | Alert created for anomaly |\n",
        "| Case Workflow | 1. Create case<br>2. Assign<br>3. Add notes<br>4. Resolve | Case closed with resolution |\n",
        "| Rule Firing | 1. Create rule<br>2. Generate matching event<br>3. Run rules | Rule matches, action executed |\n",
        "\n",
        "**Task 12.3.2: Implement E2E Tests**\n",
        "- Use test database\n",
        "- Create test data\n",
        "- Run complete workflows\n",
        "- Verify final state\n",
        "\n",
        "---\n",
        "## Step 12.4: Performance Testing\n",
        "## Step 12.4: Performance Testing\n",
        "### What to Test:\n",
        "### What to Test:\n",
        "System behavior under load.\n",
        "\n",
        "### Implementation Tasks:\n",
        "\n",
        "**Task 12.4.1: Define Performance Targets**\n",
        "\n",
        "| Metric | Target |\n",
        "|--------|--------|\n",
        "| API response time (p95) | < 500ms |\n",
        "| Full user analysis | < 5 seconds |\n",
        "| Batch analysis (100 users) | < 2 minutes |\n",
        "| Model training | < 30 minutes |\n",
        "\n",
        "**Task 12.4.2: Create Load Tests**\n",
        "\n",
        "```python\n",
        "# Using locust or similar tool\n",
        "from locust import HttpUser, task, between\n",
        "\n",
        "class UBAUser(HttpUser):\n",
        "    wait_time = between(1, 3)\n",
        "    \n",
        "    @task(3)\n",
        "    def get_dashboard(self):\n",
        "        self.client.get(\"/api/v1/uba/dashboard/summary\")\n",
        "    \n",
        "    @task(2)\n",
        "    def get_alerts(self):\n",
        "        self.client.get(\"/api/v1/uba/alerts\")\n",
        "    \n",
        "    @task(1)\n",
        "    def get_profile(self):\n",
        "        user_id = \"test_user\"\n",
        "        self.client.get(f\"/api/v1/uba/profiles/{user_id}\")\n",
        "```\n",
        "**Task 12.4.3: Run and Analyze**\n",
        "**Task 12.4.3: Run and Analyze**\n",
        "- Run load tests\n",
        "- Identify bottlenecks\n",
        "- Optimize slow queries\n",
        "\n",
        "---\n",
        "\n",
        "## Step 12.5: Deployment\n",
        "\n",
        "### Deployment Architecture\n",
        "\n",
        "```\n",
        "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
        "â”‚                         PRODUCTION DEPLOYMENT                                    â”‚\n",
        "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
        "â”‚                                                                                  â”‚\n",
        "â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚\n",
        "â”‚   â”‚                      DOCKER COMPOSE STACK                                â”‚   â”‚\n",
        "â”‚   â”‚                                                                          â”‚   â”‚\n",
        "â”‚   â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                  â”‚   â”‚\n",
        "â”‚   â”‚   â”‚    NGINX     â”‚  â”‚    DSPM      â”‚  â”‚   UBA_PRO    â”‚                  â”‚   â”‚\n",
        "â”‚   â”‚   â”‚   (Proxy)    â”‚  â”‚    API       â”‚  â”‚   (Included  â”‚                  â”‚   â”‚\n",
        "â”‚   â”‚   â”‚              â”‚  â”‚              â”‚  â”‚   in DSPM)   â”‚                  â”‚   â”‚\n",
        "â”‚   â”‚   â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                  â”‚   â”‚\n",
        "â”‚   â”‚          â”‚                 â”‚                                             â”‚   â”‚\n",
        "â”‚   â”‚          â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                             â”‚   â”‚\n",
        "â”‚   â”‚                   â”‚                                                      â”‚   â”‚\n",
        "â”‚   â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                   â”‚   â”‚\n",
        "â”‚   â”‚   â”‚        PostgreSQL             â”‚  â”‚    Redis     â”‚                   â”‚   â”‚\n",
        "â”‚   â”‚   â”‚        (Database)             â”‚  â”‚   (Cache)    â”‚                   â”‚   â”‚\n",
        "â”‚   â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                   â”‚   â”‚\n",
        "â”‚   â”‚                                                                          â”‚   â”‚\n",
        "â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚\n",
        "â”‚                                                                                  â”‚\n",
        "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
        "```\n",
        "\n",
        "### Implementation Tasks:\n",
        "**Task 12.5.1: Create Docker Configuration**\n",
        "**Task 12.5.1: Create Docker Configuration**\n",
        "- Dockerfile for UBA (if separate from DSPM)\n",
        "- docker-compose.yml updates\n",
        "- Environment variables\n",
        "**Task 12.5.2: Database Migrations**\n",
        "**Task 12.5.2: Database Migrations**\n",
        "- Create Alembic migrations for UBA tables\n",
        "- Test migration on staging\n",
        "- Plan rollback strategy\n",
        "\n",
        "**Task 12.5.3: Environment Configuration**\n",
        "\n",
        "```yaml\n",
        "# .env.production\n",
        "UBA_ENABLED=true\n",
        "UBA_ANALYSIS_INTERVAL=3600  # 1 hour\n",
        "UBA_RISK_THRESHOLD=50\n",
        "UBA_ALERT_THRESHOLD=70\n",
        "UBA_MODEL_PATH=/app/models/\n",
        "```\n",
        "**Task 12.5.4: Monitoring Setup**\n",
        "**Task 12.5.4: Monitoring Setup**\n",
        "- Health check endpoints\n",
        "- Logging configuration\n",
        "- Metrics (Prometheus/Grafana if available)\n",
        "\n",
        "**Task 12.5.5: Deployment Checklist**\n",
        "\n",
        "```\n",
        "PRE-DEPLOYMENT:\n",
        "â–¡ All tests passing\n",
        "â–¡ Code reviewed\n",
        "â–¡ Documentation updated\n",
        "â–¡ Database migrations tested\n",
        "â–¡ Rollback plan ready\n",
        "\n",
        "DEPLOYMENT:\n",
        "â–¡ Backup database\n",
        "â–¡ Run database migrations\n",
        "â–¡ Deploy new code\n",
        "â–¡ Verify health checks\n",
        "â–¡ Run smoke tests\n",
        "\n",
        "POST-DEPLOYMENT:\n",
        "â–¡ Monitor error rates\n",
        "â–¡ Check performance metrics\n",
        "â–¡ Verify scheduled jobs running\n",
        "â–¡ Test key workflows manually\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## Step 12.6: Documentation\n",
        "\n",
        "### Implementation Tasks:\n",
        "**Task 12.6.1: API Documentation**\n",
        "**Task 12.6.1: API Documentation**\n",
        "- FastAPI auto-generates OpenAPI docs\n",
        "- Add descriptions to all endpoints\n",
        "- Include request/response examples\n",
        "**Task 12.6.2: User Guide**\n",
        "**Task 12.6.2: User Guide**\n",
        "- How to use UBA dashboard\n",
        "- Understanding risk scores\n",
        "- Managing alerts and cases\n",
        "**Task 12.6.3: Admin Guide**\n",
        "**Task 12.6.3: Admin Guide**\n",
        "- Configuration options\n",
        "- Rule management\n",
        "- Model retraining\n",
        "\n",
        "**Task 12.6.4: Developer Guide**\n",
        "- Architecture overview\n",
        "- Adding new models\n",
        "- Adding new rules\n",
        "- API integration\n",
        "\n",
        "---\n",
        "\n",
        "## ðŸ“ END OF PHASE 12 - ROADMAP COMPLETE!\n",
        "\n",
        "---\n",
        "\n",
        "# ðŸŽ‰ COMPLETE UBA_PRO ROADMAP SUMMARY\n",
        "\n",
        "| Phase | Name | Key Deliverables |\n",
        "|-------|------|------------------|\n",
        "| 1 | Foundation | Project structure, DB models, config |\n",
        "| 2 | Core Utilities | Logging, exceptions, validation |\n",
        "| 3 | Data Layer | Repositories, migrations, queries |\n",
        "| 4 | User Management | User profiles, baselines |\n",
        "| 5 | Feature Extraction | 15 behavioral features (expandable) |\n",
        "| 5.5 | Training Data Preparation | Data labeling, split strategy |\n",
        "| 6 | ML Model Development | 4 anomaly detection models |\n",
        "| 7 | Analytics Engine | Model execution, risk calculator |\n",
        "| 8 | Alert & Case System | Alerts, cases, workflow |\n",
        "| 9 | Rule Engine | Rule-based detection, MITRE |\n",
        "| 10 | API Routes | REST API, schemas |\n",
        "| 11 | DSPM Integration | Data sync, event handling |\n",
        "| 11.5 | External Log Integration | SSO/Auth logs for login analysis |\n",
        "| 12 | Testing & Deployment | Tests, deployment, docs |\n",
        "\n",
        "---\n",
        "\n",
        "## âš ï¸ CRITICAL DATA DEPENDENCIES\n",
        "\n",
        "| Capability | Data Source | Status |\n",
        "|------------|-------------|--------|\n",
        "| Access Anomaly Detection | DSPM `access_controls` | âœ… AVAILABLE |\n",
        "| Policy Violation Analysis | DSPM `policy_violations` | âœ… AVAILABLE |\n",
        "| Over-privilege Detection | DSPM `identity_mappings` WHERE `issue = 'Over-privileged'` | âœ… AVAILABLE |\n",
        "| Stale Access Detection | DSPM `identity_mappings` WHERE `issue = 'Stale access'` | âœ… AVAILABLE |\n",
        "| Risk Labeling | DSPM (`identity_mappings.issue`, `policy_violations`) | âœ… AVAILABLE |\n",
        "| DSPM App Login Analysis | DSPM `user_sessions` | âœ… AVAILABLE (DSPM app only) |\n",
        "| Enterprise Login Anomaly Detection | External IdP/SSO/SIEM Logs | âš ï¸ REQUIRES Phase 11.5 |\n",
        "| Enterprise Session Analysis | External IdP/SSO/SIEM Logs | âš ï¸ REQUIRES Phase 11.5 |\n",
        "| Enterprise IP/Geo Analysis | External IdP/SSO/SIEM Logs | âš ï¸ REQUIRES Phase 11.5 |\n",
        "\n",
        "---\n",
        "## What You'll Have at the End:\n",
        "## What You'll Have at the End:\n",
        "\n",
        "```\n",
        "UBA_PRO CAPABILITIES:\n",
        "âœ… User Behavioral Profiling\n",
        "âœ… User Behavioral Profiling\n",
        "   - Automatic baseline establishment\n",
        "   - Continuous behavior tracking\n",
        "   - Historical trend analysis\n",
        "\n",
        "âœ… ML-Based Anomaly Detection\n",
        "   - Access pattern anomalies (DSPM data)\n",
        "   - Data transfer anomalies (DSPM data)\n",
        "   - Time-based anomalies (DSPM data)\n",
        "   - Login anomaly detection (âš ï¸ requires external auth logs)\n",
        "âœ… Risk Scoring System\n",
        "âœ… Risk Scoring System\n",
        "   - Composite risk scores (0-100)\n",
        "   - Risk trend tracking\n",
        "   - Risk level classification\n",
        "\n",
        "âœ… Alert Management\n",
        "   - Automated alert generation\n",
        "   - Priority-based queue\n",
        "   - Analyst assignment\n",
        "   - SLA tracking\n",
        "\n",
        "âœ… Case Management\n",
        "   - Group related alerts\n",
        "   - Investigation workflow\n",
        "   - Resolution tracking\n",
        "   - Audit trail\n",
        "âœ… Rule Engine\n",
        "âœ… Rule Engine\n",
        "   - 10+ built-in rules\n",
        "   - Custom rule creation\n",
        "   - MITRE ATT&CK mapping\n",
        "âœ… DSPM Integration\n",
        "âœ… DSPM Integration\n",
        "   - Seamless data flow\n",
        "   - Real-time event processing\n",
        "   - Unified UI experience\n",
        "\n",
        "âœ… Full API Coverage\n",
        "   - 30+ REST endpoints\n",
        "   - Complete documentation\n",
        "   - Authentication/Authorization\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## Next Steps After Roadmap:\n",
        "\n",
        "1. **Present to CTO** - Use this roadmap\n",
        "2. **Get Approval** - Confirm resources\n",
        "3. **Start Phase 1** - Follow step-by-step\n",
        "4. **Regular Updates** - Track progress against roadmap\n",
        "5. **Iterate** - Adjust based on learnings\n",
        "\n",
        "---\n",
        "\n",
        "## ðŸ’¡ SUCCESS TIPS:\n",
        "\n",
        "1. **Follow the phases in order** - Each builds on previous\n",
        "2. **Test as you go** - Don't save testing for end\n",
        "3. **Get feedback early** - Demo to stakeholders after each phase\n",
        "4. **Document decisions** - Future you will thank current you\n",
        "5. **Start simple** - Get basic working before optimizing\n",
        "\n",
        "---\n",
        "\n",
        "**ðŸŽ“ CONGRATULATIONS!**\n",
        "\n",
        "You now have a COMPLETE, DETAILED roadmap to build UBA_PRO from scratch.\n",
        "This is a professional-grade implementation plan that covers:\n",
        "- Architecture\n",
        "- Database design\n",
        "- Feature engineering\n",
        "- Machine learning\n",
        "- API development\n",
        "\n",
        "- Integration\n",
        "- TestingGood luck with your implementation! ðŸš€\n",
        "\n",
        "- Testing\n",
        "- DeploymentGood luck with your implementation! ðŸš€\n",
        "\n",
        "- Deployment\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}