{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ashish78905/OPTICONNECT_CALLL_CENTER_ANALYSIS-ASSIGNMENT/blob/main/preprocessing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#THESE ARE THE TECHNIQUES TO HANDLE MISSING VALUES"
      ],
      "metadata": {
        "id": "m8b17iPv_R0b"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xDL1IXAVDAvD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#this operation we are seeing on the sample data set\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n"
      ],
      "metadata": {
        "id": "-IPuXQy2Cr1z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6p60BuU_93OZ"
      },
      "outputs": [],
      "source": [
        "df.isnull().sum()#to see how rows are there are misssing values in each of the particular column"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.dropna()# this the first method to handle missing values by dropping the missing values,by default it drop the missing value as per row"
      ],
      "metadata": {
        "id": "sHhY_JRh96OE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.dropna(axis = 1) # now here missing cvalue will be droped as per column"
      ],
      "metadata": {
        "id": "Cnj12PBa-EHg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['B'].fillna(df['B'].mean()) # this is the second type to handle missing values by imputing a column with its mean value"
      ],
      "metadata": {
        "id": "EUe_XJ8W-FGV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['C'].fillna(0)# tghis is another method to handle missing value by putting any random number or chracter"
      ],
      "metadata": {
        "id": "HrOnSlCr-LsU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['B'] = df['B'].fillna(df['B'].median())  # here imputting with median"
      ],
      "metadata": {
        "id": "JodrFFRi-PvU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.info() # from this you can see by seeing the overall information of the dataframe and can be seen how many null values are there"
      ],
      "metadata": {
        "id": "TvykZuBd-UhN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.dropna(axis=0).shape # from this you can check after dropping the missing value what is my shape of the dataframe"
      ],
      "metadata": {
        "id": "7LdUIqAM-Wq5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.histplot(df.age, kde = True) # to see how to impute mean value use this to see weather the data distribution is normal or not ,if that is normal distribution then impute with mean else with median"
      ],
      "metadata": {
        "id": "IqTfM_NM-eHB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.distplot(df['mean_imputation'], kde = True) # after the immputing the mean or median you can see the distribution"
      ],
      "metadata": {
        "id": "8oJH2hFG-iNU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df[df['embarked'].notna()]['embarked'].mode() # if the data is categorical in nature then impute that with mode"
      ],
      "metadata": {
        "id": "LXLEwlsl-rlf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['embarked'].isna().sum() # at last check again is there any null value persent or not"
      ],
      "metadata": {
        "id": "YL3t-I0r-u3D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " # WE WILL GO TOWARDS HANDLING IMBALANCED DATASET"
      ],
      "metadata": {
        "id": "SNENX9r_Aq5W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "np.random.seed(1) #for reproducibility\n",
        "\n",
        "no_samples = 1000   #defining the total number of data we will use in our dataset\n",
        "class_0_ratio = 0.9    #  class o of the target variable will be 90%\n",
        "no_class_0 = int(no_samples * class_0_ratio) # class 0 of the target variable will be 900\n",
        "no_class_1 = 100  # class 1 of the target variable will be 100\n"
      ],
      "metadata": {
        "id": "--ezxn9YBCk1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "no_class_0, no_class_1    # here the number of class 0 and class 1 can be seen"
      ],
      "metadata": {
        "id": "Hp-oiYRlL0Rz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(np.random.normal(0, 1, no_class_0))"
      ],
      "metadata": {
        "id": "0LTtUOIvMFyQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class_0 = {'feature1': np.random.normal(0, 1, no_class_0),\n",
        "          'feature2': np.random.normal(0, 1, no_class_0),\n",
        "          'target': [0]*no_class_0}     # here we are defining our class 0 values"
      ],
      "metadata": {
        "id": "KBe9uD52MIsI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class_0 = pd.DataFrame(class_0)\n",
        "class_0   # making the class o into dataframe"
      ],
      "metadata": {
        "id": "1XRNH-YxMLja"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class_1 = pd.DataFrame({'feature1': np.random.normal(3, 1, no_class_1),\n",
        "          'feature2': np.random.normal(3, 1, no_class_1),\n",
        "          'target': [1]*no_class_1})\n",
        "class_1        # similarly for class 1"
      ],
      "metadata": {
        "id": "0KdLPr01MRm3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.concat([class_0, class_1]).reset_index(drop = True)\n",
        "df       # finsally class o and class 1 value will ba concatenated"
      ],
      "metadata": {
        "id": "cwtzpc6DMXj9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.target.value_counts() # to see the value cont of class 1 and 0 that is actually 900 and 100"
      ],
      "metadata": {
        "id": "MyZo0PQrMdYY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "above we have creted the imbalanced class manually using numpy but most of time we will use premade dataset in the industrial problem to handle imbalanced class"
      ],
      "metadata": {
        "id": "Estsf-W6PRqH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "----------------\n",
        "now lets go towards upsampling"
      ],
      "metadata": {
        "id": "YjohKhx_Pnff"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#upsampling\n",
        "\n",
        "df_minority = df[df.target == 1]\n",
        "df_majority = df[df.target == 0]   ##just demostrating the value of class 0 nd 1"
      ],
      "metadata": {
        "id": "roSKZK3pPi91"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.utils import resample   # imported this to use same number of again and again or repetation of number to use in the upsampling"
      ],
      "metadata": {
        "id": "6qUtYKPfP_aR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#replace = True with replacement\n",
        "df_minority_upsampled = resample(df_minority, replace = True, n_samples = len(df_majority), random_state = 1) # here we have upsampled the minority data equals to the majority data with replacement"
      ],
      "metadata": {
        "id": "AVGSDn7-QXTm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_minority_upsampled.shape  # to check shape of minority data after upsampled"
      ],
      "metadata": {
        "id": "2NYLmLEsQdao"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_upsampled = pd.concat([df_majority, df_minority_upsampled])\n",
        "df_upsampled   # now we have concatinated the both upsampled data and majority data"
      ],
      "metadata": {
        "id": "3HUq8v7xQhyW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_upsampled.target.value_counts()     # to see the value count of mjority and upsampled data"
      ],
      "metadata": {
        "id": "5_g07w6KQylS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "--------------\n",
        "NOW WE WILL GO TOWARDS THE DOWNSAMPLING TECHNIQUE"
      ],
      "metadata": {
        "id": "R15OjOTdTAiD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_majority_downsampled = resample(df_majority, replace = False, n_samples = len(df_minority), random_state = 1) # here we have downsampled the data without any replacement at all"
      ],
      "metadata": {
        "id": "dCRoOXQwTHpd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_downsampled # to see the dataframe of the downsampled data"
      ],
      "metadata": {
        "id": "lXR7ksIfTReO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_downsampled.target.value_counts() # to see the value count of the minority data and the downsampled data"
      ],
      "metadata": {
        "id": "-D7pn0qfTQCH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "------------------\n",
        "now lets go toward SMOTE technique"
      ],
      "metadata": {
        "id": "KK_KgE4bT3DB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import make_classification # imported nescessary classification to make the dataset before applying SMOTE TECHNIQUE BUT THIS IS not use as usual in the realword as most of time data are predefined,but just to oveserve we are using this"
      ],
      "metadata": {
        "id": "FIPnkvxMUJ18"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X, y = make_classification(n_samples = 1000, n_redundant=0, n_features = 2, n_clusters_per_class=1, weights = [0.90], random_state=1)   # now we have classified the whole dataset into the two categories X and Y"
      ],
      "metadata": {
        "id": "K8KB0-BAUSVF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x  # it will see the created x data"
      ],
      "metadata": {
        "id": "lNLrCaotUVh1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " y # it will show the created y data"
      ],
      "metadata": {
        "id": "-oNDGq12Uaab"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(y[y==0])  # returns the valuecount of the 0 in y data"
      ],
      "metadata": {
        "id": "CWkJ1965Ub0W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df1 = pd.DataFrame(X, columns = ['f1', 'f2'])\n",
        "df2 = pd.DataFrame(y, columns = ['target'])\n",
        "final_df = pd.concat([df1, df2], axis=1)\n",
        "final_df # now we have created the dataframe"
      ],
      "metadata": {
        "id": "-cQYX-slUhzy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final_df.target.value_counts()  # it will show the number of value count 0 and 1 in the target variable i.e. Y"
      ],
      "metadata": {
        "id": "kdonm0G_Ukgc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.scatter(final_df['f1'], final_df['f2'], c = final_df['target']) # it shows data looks before applying SMOTE technique"
      ],
      "metadata": {
        "id": "bdZHqfeVUllM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install imblearn  # imported imblearn to use SMOTE technique in the dataframe"
      ],
      "metadata": {
        "id": "0EJIhPZXUpHs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from imblearn.over_sampling import SMOTE"
      ],
      "metadata": {
        "id": "c8tKtSF8Ur1o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "oversample = SMOTE()   # making the smote object"
      ],
      "metadata": {
        "id": "seNhgA7fUt9L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X, y = oversample.fit_resample(final_df[['f1', 'f2']], final_df['target'])"
      ],
      "metadata": {
        "id": "ZK-AUCq-UwYJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X.shape"
      ],
      "metadata": {
        "id": "s0BVBqG4UzJZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y.shape"
      ],
      "metadata": {
        "id": "flUh9lFVU2Y8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(y[y==0])#it shows the the value count of 0 in target variable y"
      ],
      "metadata": {
        "id": "E0GbWRnkU5eo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(y[y==1])# it shows the value count of 1 in the target variable y"
      ],
      "metadata": {
        "id": "Ohvmhn_WU7RH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df1 = pd.DataFrame(X, columns = ['f1', 'f2'])\n",
        "df2 = pd.DataFrame(y, columns = ['target'])\n",
        "oversample_df = pd.concat([df1, df2], axis=1)\n",
        "oversample_df"
      ],
      "metadata": {
        "id": "ggA70V-qU9hQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.scatter(oversample_df['f1'], oversample_df['f2'], c = oversample_df['target'])"
      ],
      "metadata": {
        "id": "MoxcXgLiVAAG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "oversample_df[oversample_df.target == 1]"
      ],
      "metadata": {
        "id": "B9HbMVnZVCjl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_minority"
      ],
      "metadata": {
        "id": "OGAeMUqDVGHq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "----------------\n",
        "#NOW LETS GO TOWARDS THE INTERPOLATION,WITH THE SUPERVISION OF BUSSINESS TEAM"
      ],
      "metadata": {
        "id": "l61_JLPW2NGN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#linear interpolation\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "x = np.array([1, 2, 3, 4, 5])\n",
        "y = np.array([1, 3, 5, 7, 9])\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "plt.scatter(x,y)"
      ],
      "metadata": {
        "id": "E_8CTDC82WyI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#interpolate the data using interpolation>>linear\n",
        "\n",
        "x_new = np.linspace(1, 5, 10) #create new x values\n",
        "y_interp = np.interp(x_new, x, y)"
      ],
      "metadata": {
        "id": "S9iDpYNg3DYj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.scatter(x_new, y_interp)"
      ],
      "metadata": {
        "id": "v-wc4e6P3G-d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#cubic interpolation\n",
        "x = np.array([1, 2, 3, 4, 5])\n",
        "y = np.array([1, 8, 27, 64, 125])"
      ],
      "metadata": {
        "id": "GBeMWG-N3Jsp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.scatter(x,y)"
      ],
      "metadata": {
        "id": "rKt9iAtX3LqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.interpolate import interp1d"
      ],
      "metadata": {
        "id": "TJ72Ss9h3OfY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#cubic interpolation function\n",
        "\n",
        "f = interp1d(x, y, kind = 'cubic')"
      ],
      "metadata": {
        "id": "GDYCY3A43Qzz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_new = np.linspace(1, 5, 10)\n",
        "y_interp = f(x_new)"
      ],
      "metadata": {
        "id": "JE8cfvYp3Sqj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_interp"
      ],
      "metadata": {
        "id": "7UO2WOo63VBC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.scatter(x, y)"
      ],
      "metadata": {
        "id": "-7YZZx1f3X9o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.scatter(x_new, y_interp)"
      ],
      "metadata": {
        "id": "sRHnrpc13atl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#polynomial interpolation(you will got pre made data )\n",
        "\n",
        "x = np.array([1, 2, 3, 4, 5])\n",
        "y = np.array([1, 4, 9, 1, 25])"
      ],
      "metadata": {
        "id": "mysIWlLV3dZp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#interpolate using polynomial interpolation\n",
        "p = np.polyfit(x, y, 2)"
      ],
      "metadata": {
        "id": "mjgcC6113gey"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_new = np.linspace(1, 5, 10)\n",
        "y_interp = np.polyval(p, x_new)"
      ],
      "metadata": {
        "id": "7GIupys33ibR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.scatter(x, y)"
      ],
      "metadata": {
        "id": "fCW2J_GK3kp9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.scatter(x_new, y_interp)"
      ],
      "metadata": {
        "id": "JBuSphfm3o9c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "--------------------\n",
        "#NOW WE WILL GO TOWARDS HANDAING OUTLIERS"
      ],
      "metadata": {
        "id": "BrCDHC6i5d4z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "salary = [11, 40, 45, 68, 65, 68, 78, 90, 57, 74, 91, 92, 88, 68, 57, 48, 99, 101, 68, 77, 110, 140]"
      ],
      "metadata": {
        "id": "0g7oAIL-5sjT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.DataFrame(salary, columns = ['Salary'])"
      ],
      "metadata": {
        "id": "7N2ryrZk4Bee"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.describe()"
      ],
      "metadata": {
        "id": "FaAtPtV9-XN1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#To check outlier >> distplot, boxplot\n",
        "\n",
        "\n",
        "plt.figure(figsize = (12, 5))\n",
        "plt.subplot(1, 2, 1)\n",
        "sns.histplot(df['Salary'], kde = True)\n",
        "plt.title(\"Dist plot\")\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "sns.boxplot(data =df, x = 'Salary')\n",
        "plt.title(\"Box plot\")"
      ],
      "metadata": {
        "id": "Zy-x-Whs-Z0s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#dropping the outlier\n",
        "Q1 = df['Salary'].quantile(0.25)\n",
        "Q3 = df['Salary'].quantile(0.75)\n",
        "\n",
        "IQR = Q3-Q1\n",
        "\n",
        "lower_fence = Q1 -1.5 *IQR\n",
        "upper_fence = Q3 + 1.5 *IQR     # we have calculated quartile 1 and quartile 3 anf further we\n",
        "#will use this into upcoming codes"
      ],
      "metadata": {
        "id": "bqdGKdko-elu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lower_fence# it will the lowest side outlier side\n"
      ],
      "metadata": {
        "id": "M38DZTJP-y81"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "upper_fence# it will the highest side outlier side\n"
      ],
      "metadata": {
        "id": "AD0imPYa-9TK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_filtered = df[(df.Salary >= lower_fence) & (df.Salary <= upper_fence)]\n",
        "# we have droped the outlier here"
      ],
      "metadata": {
        "id": "uX6yfdW-_DB5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_filtered.shape# to check the shape of the data after dropping outlier that will actually be reduced from the origional one"
      ],
      "metadata": {
        "id": "GJREzsJB_Lt5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# it will confirm there is no any outlier at all\n",
        "plt.figure(figsize = (12, 5))\n",
        "plt.subplot(1, 2, 1)\n",
        "sns.histplot(df_filtered['Salary'], kde = True)\n",
        "plt.title(\"Dist plot\")\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "sns.boxplot(data =df_filtered, x = 'Salary')\n",
        "plt.title(\"Box plot\")"
      ],
      "metadata": {
        "id": "gybo-vBC_VZT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#imputation with mean and median\n",
        "\n",
        "df['Salary_imputed_mean'] = np.where((df.Salary >= upper_fence) | (df.Salary <= lower_fence), df['Salary'].mean(), df['Salary'])\n",
        "# here we have imputed with the mean"
      ],
      "metadata": {
        "id": "I7K86Bg8_waL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# check after the mean imputation,the outlier have been removed\n",
        "plt.subplot(1, 2, 1)\n",
        "sns.histplot(df['Salary_imputed_mean'], kde = True)\n",
        "plt.title(\"The dist plot of filtered salary\")\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "sns.boxplot(data =df, x = 'Salary_imputed_mean')\n",
        "plt.title(\"The box plot of salary\")"
      ],
      "metadata": {
        "id": "viWYqYZ7AGG9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# similarly we have imputed with median\n",
        "df['Salary_imputed_median'] = np.where((df.Salary >= upper_fence) | (df.Salary <= lower_fence), df['Salary'].median(), df['Salary'])"
      ],
      "metadata": {
        "id": "DXuX6XNjFEDi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "id": "gSAzseOyFXY2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#to confirm is there any outlier present after imputation or not\n",
        "plt.subplot(1, 2, 1)\n",
        "sns.histplot(df['Salary_imputed_median'], kde=True)\n",
        "plt.title(\"The dist plot of filtered salary\")\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "sns.boxplot(data=df, x='Salary_imputed_median')\n",
        "plt.title(\"The box plot of salary\")\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "7OeBUNjUFaqu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Capping >> replacing with the nearest value which is not outlier\n",
        "df"
      ],
      "metadata": {
        "id": "hmQmJl8xFrgJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lower_cap = df['Salary'].quantile(0.05) #lower cap as 5th percentile\n",
        "upper_cap = df['Salary'].quantile(0.95) #upper cap at 95th percentile\n",
        "lower_cap\n",
        "upper_cap\n",
        "# we will replace the outlier with the 5th and 95th percentile"
      ],
      "metadata": {
        "id": "3wbe9-51FwQv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#replace outliers with the cap\n",
        "\n",
        "df['Salary_capped'] = np.where(df['Salary'] < lower_cap, lower_cap,\n",
        "        np.where(df['Salary'] > upper_cap, upper_cap,\n",
        "                df['Salary']))"
      ],
      "metadata": {
        "id": "PW65u0A0F73M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#now to confirm is there any outlier present after capping or not\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "sns.histplot(df['Salary_capped'], kde=True)\n",
        "plt.title(\"The dist plot of capped salary\")\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "sns.boxplot(data=df, x='Salary_capped')\n",
        "plt.title(\"The box plot of capped salary\")\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Hndn2qiCGCOs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "-------------------------\n",
        "#Now lets go towards the Feature Scaling ######"
      ],
      "metadata": {
        "id": "IYYYIHJdGr3z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#feature scaling\n",
        "#Standardisation >> mu=0 and sigma 1\n",
        "#Normalization (minmax scaling) >> data comes between 0 and 1\n",
        "#unit vector"
      ],
      "metadata": {
        "id": "3RmKNGGgG8pw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = sns.load_dataset('tips')\n",
        "df"
      ],
      "metadata": {
        "id": "xaLXyvDKUVEf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Actually feature scaling is optional at all as it does not change the distribution of the data ,the distribution will remains same in both before or after apllying the feature scaling on the data"
      ],
      "metadata": {
        "id": "FX9BiigNUgfB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lets Apply Standardalization Technique"
      ],
      "metadata": {
        "id": "fRYW6i4oVIqU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler"
      ],
      "metadata": {
        "id": "gwIrxMjVVNlm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scaler = StandardScaler()# we first create the object of the standardScaler to use their class"
      ],
      "metadata": {
        "id": "Ai-F_nh6VdmT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scaler # it shows ,ye object have been created"
      ],
      "metadata": {
        "id": "SiAkTZODVvOp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scaler.fit_transform(df[['total_bill', 'tip']])#it apply fit and transom to the written column"
      ],
      "metadata": {
        "id": "LE0_64LXV3z6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pd.DataFrame(scaler.fit_transform(df[['total_bill', 'tip']]), columns = ['total_bill', 'tip'])#it shows the standarlized data in the form of the dataframe"
      ],
      "metadata": {
        "id": "SvsDi5UgWLAN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scaler.transform([[13, 1]])# this ia a doubt about the traning and test data ,we can not use fit on test data .....clear it properly both for standardization and normalization"
      ],
      "metadata": {
        "id": "izd-pKafYF1a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "lets apply mix max scaling/normalization"
      ],
      "metadata": {
        "id": "yzFJI9tBWlie"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import MinMaxScaler"
      ],
      "metadata": {
        "id": "WnPZnXtEWzx2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "min_max = MinMaxScaler()"
      ],
      "metadata": {
        "id": "ZH5cTgs0W5eE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "min_max.fit_transform(df[['total_bill', 'tip']])"
      ],
      "metadata": {
        "id": "xWSqDCPjW8b5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "min_max.transform([[10, 2]])"
      ],
      "metadata": {
        "id": "435RkCIHXBY1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "now lets apply unit vector"
      ],
      "metadata": {
        "id": "JdYXmoxYXDDh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#unit vector\n",
        "\n",
        "from sklearn.preprocessing import normalize# its not that normalization just importing for unit vector"
      ],
      "metadata": {
        "id": "9UqW_-JrXHN9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "uv = normalize(df[['total_bill']])"
      ],
      "metadata": {
        "id": "A7aXf0RjXNts"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "uv #its one dimension so all the values are 1"
      ],
      "metadata": {
        "id": "xGV8kmlbXP_e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "-----------\n",
        "#Now lets go towards the Encoding"
      ],
      "metadata": {
        "id": "iw94vHJQZGuU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#data encoding>> converting categorical columns into numeric\n",
        "#Nominal/OHE\n",
        "#label and ordinal encoding\n",
        "#Target guided ordinal encoding"
      ],
      "metadata": {
        "id": "aWsDnpRyZbpi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.preprocessing import OneHotEncoder#importing  the necessary library from the sklearn"
      ],
      "metadata": {
        "id": "fn9JvXM8lpuJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.DataFrame({'status': ['single', 'married', 'separated', 'single', 'single', 'married', 'married']})\n",
        "df# made dataframe to use later"
      ],
      "metadata": {
        "id": "p3WmRHwWl1vs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoder = OneHotEncoder()#first create the class of one hot encoder to use its class"
      ],
      "metadata": {
        "id": "1XAwAhbtl-aW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoder# to ckeck object have been crated or not"
      ],
      "metadata": {
        "id": "EIMST_axmBjP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoded = encoder.fit_transform(df[['status']]).toarray()#it will store the encoded into the array"
      ],
      "metadata": {
        "id": "Bt7O9bibmFfk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoder.get_feature_names_out()#it will show the name of the encoded column"
      ],
      "metadata": {
        "id": "i-u3QX0qmIJK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoder_df = pd.DataFrame(encoded, columns = encoder.get_feature_names_out())\n",
        "encoder_df # will convert the array into the dataframe ,after this we will delete the some extra column"
      ],
      "metadata": {
        "id": "IilbZU-dmfNy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#new data\n",
        "encoder.transform([['single']]).toarray()# it will show the output of this in the encoded form[recheck]"
      ],
      "metadata": {
        "id": "WSIQXFswmjtE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pd.concat([df, encoder_df], axis=1)#it will add the encoded dataframe with the real dataframe ,and further we will delete the orgional column"
      ],
      "metadata": {
        "id": "K0-g8sqqmqBP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "we actually delete the actual column from the dataset and make will use n-1 encoded column"
      ],
      "metadata": {
        "id": "OBb1fHBFt3Dh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "------------------\n",
        "label encoding>> assigns unique label to the categories"
      ],
      "metadata": {
        "id": "9REessCwmz7M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelEncoder"
      ],
      "metadata": {
        "id": "SqNmHIc8m2x-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "label_encoder = LabelEncoder()"
      ],
      "metadata": {
        "id": "8rgA0gZqnJ2N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "label_encoder.fit_transform(df[['status']])"
      ],
      "metadata": {
        "id": "AZkbryXBnMcD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "label_encoder.transform([['single']])"
      ],
      "metadata": {
        "id": "sh0IUN7rnQLC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "label_encoder.transform([['separated']])"
      ],
      "metadata": {
        "id": "7IJaNmhnnbk9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ordinal encoding\n",
        "\n",
        "High school: 1\n",
        "graduate: 2\n",
        "pg:3\n",
        "phd:4"
      ],
      "metadata": {
        "id": "dPK3LX3hngjz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import OrdinalEncoder"
      ],
      "metadata": {
        "id": "k_625bkynj0P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.DataFrame({\"qualification\": [\"HS\", \"GR\", \"PG\", \"PhD\", \"HS\", \"HS\", \"PhD\",\"PG\" ]})\n",
        "df"
      ],
      "metadata": {
        "id": "3noptBGunn-y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoder = OrdinalEncoder(categories=[[\"HS\", \"GR\", \"PG\", \"PhD\"]])#here we will write the name of the column whichever we like to do ordinal encoding"
      ],
      "metadata": {
        "id": "l90RKg8inwTT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoder.fit_transform(df[['qualification']])\n",
        "#qualification data encoded into as the ordinal encoding"
      ],
      "metadata": {
        "id": "R4DlMuPPn1YC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#new data\n",
        "encoder.transform([['PG']])# here we test on the new data"
      ],
      "metadata": {
        "id": "4eDxqUtCn5jN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Target guided ordinal encoding\n",
        "relationship with target variable\n",
        " A  lot of unique categories\n",
        "Replace the category with the mean and median of respective group target"
      ],
      "metadata": {
        "id": "tcS6kRjNn8es"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.DataFrame({'time':['lunch', 'breakfast', 'dinner', 'lunch', 'breakfast', 'dinner', 'lunch', 'breakfast', 'dinner'],\n",
        "             'total_bill': [120, 130, 90, 125, 150, 190, 160, 180, 189]})\n",
        "df"
      ],
      "metadata": {
        "id": "LWXL_FCmn_X8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mean_price = df.groupby('time')['total_bill'].mean().to_dict()\n",
        "mean_price  # here we have calculated the mean of luch,bf and dinner wrt to totl bill"
      ],
      "metadata": {
        "id": "hnTQCUU-oGC0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['time_encoded'] = df['time'].map(mean_price)\n",
        "df #we have calculated the mean before and now we will impute all the values of the luch ,dinner and bf with the mean of each"
      ],
      "metadata": {
        "id": "ccwz_XSWoMvT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "rerevise the target guided ordinal encoding"
      ],
      "metadata": {
        "id": "_1DoKyoEvZGi"
      }
    }
  ]
}