{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMEiUb6kWTWp1jXG/aIizms",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ashish78905/OPTICONNECT_CALLL_CENTER_ANALYSIS-ASSIGNMENT/blob/main/regression%20\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_deldz-Tt5pE"
      },
      "outputs": [],
      "source": [
        "# ---------------- SIMPLE LINEAR REGRESSION PIPELINE ----------------\n",
        "\n",
        "# Import required libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "from sklearn.datasets import load_diabetes  # Dataset used in the file\n",
        "\n",
        "# ---------------- LOAD DATA ----------------\n",
        "# This section corresponds to loading the diabetes dataset from sklearn.\n",
        "diabetes = load_diabetes()\n",
        "df = pd.DataFrame(diabetes.data, columns=diabetes.feature_names)\n",
        "df['target'] = diabetes.target\n",
        "print(\"Dataset Preview:\")\n",
        "print(df.head())\n",
        "print(\"\\nDataset Description Snippet:\")\n",
        "print(diabetes.DESCR[:500] + \"...\") # Display first 500 chars of description\n",
        "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
        "\n",
        "\n",
        "# ---------------- DEFINE FEATURES & TARGET ----------------\n",
        "# As per the file, we use a single feature ('bmi') for simple regression.\n",
        "X = df[[\"bmi\"]]                 # Independent variable (feature)\n",
        "y = df[\"target\"]                # Dependent variable (target)\n",
        "\n",
        "# ---------------- SPLIT DATA ----------------\n",
        "# The file splits the data into training and testing sets.\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=1\n",
        ")\n",
        "\n",
        "# ---------------- SCALE FEATURES ----------------\n",
        "# While not strictly necessary for simple regression, scaling is good practice.\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# ---------------- TRAIN MODEL ----------------\n",
        "# Create and train the Linear Regression model.\n",
        "regressor = LinearRegression()\n",
        "regressor.fit(X_train_scaled, y_train)\n",
        "\n",
        "# ---------------- MODEL PARAMETERS ----------------\n",
        "# Extract the learned coefficient (slope) and intercept.\n",
        "coef = regressor.coef_\n",
        "intercept = regressor.intercept_\n",
        "print(f\"Coefficient (Slope): {coef[0]:.4f}\")\n",
        "print(f\"Intercept: {intercept:.4f}\")\n",
        "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
        "\n",
        "# ---------------- PREDICTIONS ----------------\n",
        "y_pred_test = regressor.predict(X_test_scaled)\n",
        "\n",
        "# ---------------- VISUALIZE TESTING DATA ----------------\n",
        "# This visualization code is taken directly from the file.\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.scatter(X_test_scaled, y_test, color=\"black\", label=\"Actual data\")\n",
        "plt.plot(X_test_scaled, y_pred_test, color=\"blue\", linewidth=3, label=\"Regression line\")\n",
        "plt.title(\"Testing Data - Simple Linear Regression on Diabetes Data\")\n",
        "plt.xlabel(\"BMI (scaled)\")\n",
        "plt.ylabel(\"One-Year Disease Progression (target)\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# ---------------- PERFORMANCE METRICS ----------------\n",
        "mse = mean_squared_error(y_test, y_pred_test)\n",
        "mae = mean_absolute_error(y_test, y_pred_test)\n",
        "rmse = np.sqrt(mse)\n",
        "r2 = r2_score(y_test, y_pred_test)\n",
        "adj_r2 = 1 - (1-r2) * (len(y_test)-1) / (len(y_test) - X_test.shape[1] - 1)\n",
        "\n",
        "print(\"--- Performance Metrics on Test Data ---\")\n",
        "print(f\"Mean Squared Error (MSE): {mse:.4f}\")\n",
        "print(f\"Mean Absolute Error (MAE): {mae:.4f}\")\n",
        "print(f\"Root Mean Squared Error (RMSE): {rmse:.4f}\")\n",
        "print(f\"R-Squared (R²): {r2:.4f}\")\n",
        "print(f\"Adjusted R-Squared: {adj_r2:.4f}\")\n",
        "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
        "\n",
        "# ---------------- RESIDUAL ANALYSIS ----------------\n",
        "# Calculate residuals to check model assumptions.\n",
        "error = y_test - y_pred_test\n",
        "\n",
        "# 1. Residual Plot\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.scatter(y_pred_test, error)\n",
        "plt.axhline(y=0, color=\"red\", linestyle=\"--\")\n",
        "plt.title(\"Residual Plot\")\n",
        "plt.xlabel(\"Predicted Values\")\n",
        "plt.ylabel(\"Residuals (Actual - Predicted)\")\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# 2. Residual Distribution\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.histplot(error, kde=True)\n",
        "plt.title(\"Residual Distribution\")\n",
        "plt.xlabel(\"Residual Value\")\n",
        "plt.show()\n",
        "\n",
        "# ---------------- END OF PIPELINE ----------------\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------- COMPREHENSIVE MULTIPLE LINEAR REGRESSION PIPELINE ----------------\n",
        "\n",
        "# Import required libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.feature_selection import RFE\n",
        "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
        "import statsmodels.api as sm\n",
        "\n",
        "# ---------------- LOAD DATA ----------------\n",
        "# As per the source file, we use the California Housing dataset\n",
        "housing = fetch_california_housing()\n",
        "df = pd.DataFrame(housing.data, columns=housing.feature_names)\n",
        "df['PRICE'] = housing.target\n",
        "print(\"Data Preview:\")\n",
        "print(df.head())\n",
        "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
        "\n",
        "# ---------------- DEFINE FEATURES & TARGET ----------------\n",
        "X = df.drop('PRICE', axis=1)             # Independent variables (features)\n",
        "y = df['PRICE']                          # Dependent variable (target)\n",
        "\n",
        "# ---------------- MULTICOLLINEARITY CHECK (VIF) ----------------\n",
        "# This check is performed before splitting and scaling to analyze the original features.\n",
        "print(\"Multicollinearity Check (VIF):\")\n",
        "# Add a constant to the features matrix for the VIF calculation\n",
        "X_vif = sm.add_constant(X)\n",
        "vif_data = pd.DataFrame()\n",
        "vif_data[\"Feature\"] = X_vif.columns\n",
        "vif_data[\"VIF\"] = [variance_inflation_factor(X_vif.values, i) for i in range(len(X_vif.columns))]\n",
        "print(vif_data)\n",
        "print(\"\\nNote: A VIF > 10 is often considered a sign of high multicollinearity.\")\n",
        "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
        "\n",
        "# ---------------- SPLIT DATA ----------------\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42\n",
        ")                                        # 70-30 train-test split\n",
        "\n",
        "# ---------------- FEATURE SELECTION (RFE) ----------------\n",
        "# Using Recursive Feature Elimination to find the top features\n",
        "print(\"Feature Selection (RFE):\")\n",
        "estimator = LinearRegression()\n",
        "# As per the source file, selecting the top 5 features\n",
        "rfe = RFE(estimator, n_features_to_select=5)\n",
        "rfe.fit(X_train, y_train)\n",
        "\n",
        "print(\"\\nRFE Feature Ranking (1 is best):\")\n",
        "# Create a series to display feature names with their rankings\n",
        "rfe_ranking = pd.Series(rfe.ranking_, index=X.columns)\n",
        "print(rfe_ranking.sort_values())\n",
        "\n",
        "selected_features = X.columns[rfe.support_]\n",
        "print(f\"\\nTop {len(selected_features)} features selected by RFE: {list(selected_features)}\")\n",
        "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
        "# Note: For a real project, you might retrain the model using only these selected features.\n",
        "# The following steps will proceed with all features as in the original notebook's main flow.\n",
        "\n",
        "# ---------------- SCALE FEATURES ----------------\n",
        "scaler = StandardScaler()                # Create scaler object\n",
        "X_train_scaled = scaler.fit_transform(X_train)  # Fit & transform on training data\n",
        "X_test_scaled = scaler.transform(X_test)        # Transform test data only\n",
        "\n",
        "# ---------------- TRAIN MODEL ----------------\n",
        "regressor = LinearRegression()           # Create model object\n",
        "regressor.fit(X_train_scaled, y_train)   # Train the model on scaled data\n",
        "\n",
        "# ---------------- MODEL PARAMETERS ----------------\n",
        "print(\"Model Parameters:\")\n",
        "coef = regressor.coef_                   # Coefficients for each feature\n",
        "intercept = regressor.intercept_         # Intercept\n",
        "print(\"Coefficients:\", coef)\n",
        "print(\"Intercept:\", intercept)\n",
        "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
        "\n",
        "# ---------------- PREDICTIONS ----------------\n",
        "y_pred_train = regressor.predict(X_train_scaled)  # Predictions on training set\n",
        "y_pred_test = regressor.predict(X_test_scaled)    # Predictions on testing set\n",
        "\n",
        "# ---------------- PERFORMANCE METRICS ----------------\n",
        "print(\"Performance Metrics on Test Data:\")\n",
        "mse = mean_squared_error(y_test, y_pred_test)     # Mean Squared Error\n",
        "mae = mean_absolute_error(y_test, y_pred_test)    # Mean Absolute Error\n",
        "rmse = np.sqrt(mse)                               # Root Mean Squared Error\n",
        "r2 = r2_score(y_test, y_pred_test)                # R-Squared\n",
        "adj_r2 = 1 - (1-r2) * (len(y_test)-1) / (len(y_test) - X_test.shape[1] - 1)  # Adjusted R²\n",
        "\n",
        "print(\"MSE:\", mse)\n",
        "print(\"MAE:\", mae)\n",
        "print(\"RMSE:\", rmse)\n",
        "print(\"R²:\", r2)\n",
        "print(\"Adjusted R²:\", adj_r2)\n",
        "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
        "\n",
        "# ---------------- RESIDUAL ANALYSIS ----------------\n",
        "print(\"Visualizing Residuals:\")\n",
        "error = y_test - y_pred_test                      # Residuals\n",
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "# Plot 1: Residuals vs Predicted Values\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.scatter(y_pred_test, error)\n",
        "plt.axhline(y=0, color=\"red\", linestyle=\"--\")\n",
        "plt.title(\"Residual Plot\")\n",
        "plt.xlabel(\"Predicted Values\")\n",
        "plt.ylabel(\"Residuals (Actual - Predicted)\")\n",
        "\n",
        "# Plot 2: Distribution of Residuals\n",
        "plt.subplot(1, 2, 2)\n",
        "sns.histplot(error, kde=True)\n",
        "plt.title(\"Residual Distribution\")\n",
        "plt.xlabel(\"Residuals\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# ---------------- END OF PIPELINE ----------------\n"
      ],
      "metadata": {
        "id": "n38UEZyCuRuu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------- POLYNOMIAL REGRESSION PIPELINE ----------------\n",
        "\n",
        "# Import required libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.preprocessing import PolynomialFeatures, StandardScaler\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "\n",
        "# ---------------- GENERATE SYNTHETIC DATA ----------------\n",
        "# As per the source file, we create a non-linear dataset for this example.\n",
        "np.random.seed(0)\n",
        "X = 2 - 3 * np.random.normal(0, 1, 100)\n",
        "y = X - 2 * (X ** 2) + 0.5 * (X ** 3) + np.random.normal(-3, 3, 100)\n",
        "\n",
        "# Reshape X to be a 2D array for scikit-learn\n",
        "X = X[:, np.newaxis]\n",
        "\n",
        "# Visualize the synthetic data\n",
        "plt.scatter(X, y, s=20)\n",
        "plt.title(\"Generated Non-Linear Synthetic Data\")\n",
        "plt.xlabel(\"Feature\")\n",
        "plt.ylabel(\"Target\")\n",
        "plt.show()\n",
        "\n",
        "# ---------------- DEFINE FEATURES & TARGET ----------------\n",
        "# X and y are already defined during data generation.\n",
        "\n",
        "# ---------------- SPLIT DATA ----------------\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=1\n",
        ")                                        # 80-20 train-test split\n",
        "\n",
        "# ---------------- TRANSFORM FEATURES (POLYNOMIAL) ----------------\n",
        "# This is the key step in Polynomial Regression.\n",
        "# We create new features that are powers of the original feature.\n",
        "# The degree determines the complexity of the curve. Degree=3 was used in the file.\n",
        "poly_features = PolynomialFeatures(degree=3, include_bias=False)\n",
        "X_train_poly = poly_features.fit_transform(X_train)\n",
        "X_test_poly = poly_features.transform(X_test)\n",
        "\n",
        "# ---------------- SCALE FEATURES ----------------\n",
        "# Scaling is important as polynomial features can have very different magnitudes.\n",
        "scaler = StandardScaler()\n",
        "X_train_poly_scaled = scaler.fit_transform(X_train_poly)\n",
        "X_test_poly_scaled = scaler.transform(X_test_poly)\n",
        "\n",
        "# ---------------- TRAIN MODEL ----------------\n",
        "# A simple Linear Regression model is trained on the complex (polynomial) features.\n",
        "regressor = LinearRegression()\n",
        "regressor.fit(X_train_poly_scaled, y_train)\n",
        "\n",
        "# ---------------- MODEL PARAMETERS ----------------\n",
        "coef = regressor.coef_\n",
        "intercept = regressor.intercept_\n",
        "print(f\"Coefficients (for X, X^2, X^3): {coef}\")\n",
        "print(f\"Intercept: {intercept}\")\n",
        "\n",
        "# ---------------- PREDICTIONS ----------------\n",
        "y_pred_test = regressor.predict(X_test_poly_scaled)\n",
        "\n",
        "# ---------------- VISUALIZE RESULTS ON TESTING DATA ----------------\n",
        "# To plot the curve, we need to predict on a smooth range of X values.\n",
        "X_new = np.linspace(X.min(), X.max(), 200).reshape(-1, 1)\n",
        "X_new_poly = poly_features.transform(X_new)\n",
        "X_new_poly_scaled = scaler.transform(X_new_poly)\n",
        "y_new = regressor.predict(X_new_poly_scaled)\n",
        "\n",
        "plt.scatter(X_test, y_test, color=\"green\", label=\"Testing data\")\n",
        "plt.plot(X_new, y_new, color=\"red\", label=\"Polynomial Regression Fit (degree=3)\")\n",
        "plt.title(\"Testing Data - Polynomial Regression\")\n",
        "plt.xlabel(\"Feature\")\n",
        "plt.ylabel(\"Target\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# ---------------- PERFORMANCE METRICS ----------------\n",
        "mse = mean_squared_error(y_test, y_pred_test)\n",
        "mae = mean_absolute_error(y_test, y_pred_test)\n",
        "rmse = np.sqrt(mse)\n",
        "r2 = r2_score(y_test, y_pred_test)\n",
        "# Adjusted R-squared needs to account for the number of polynomial features\n",
        "adj_r2 = 1 - (1-r2) * (len(y_test)-1) / (len(y_test) - X_test_poly.shape[1] - 1)\n",
        "\n",
        "print(\"MSE:\", mse)\n",
        "print(\"MAE:\", mae)\n",
        "print(\"RMSE:\", rmse)\n",
        "print(\"R²:\", r2)\n",
        "print(\"Adjusted R²:\", adj_r2)\n",
        "\n",
        "# ---------------- RESIDUAL ANALYSIS ----------------\n",
        "error = y_test - y_pred_test\n",
        "plt.scatter(y_pred_test, error)\n",
        "plt.axhline(y=0, color=\"red\", linestyle=\"--\")\n",
        "plt.title(\"Residual Plot\")\n",
        "plt.xlabel(\"Predicted Values\")\n",
        "plt.ylabel(\"Residuals\")\n",
        "plt.show()\n",
        "\n",
        "sns.histplot(error, kde=True)\n",
        "plt.title(\"Residual Distribution\")\n",
        "plt.show()\n",
        "\n",
        "# ---------------- END OF PIPELINE ----------------\n"
      ],
      "metadata": {
        "id": "nXmrVwQru7I2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------- MODEL PICKLING CODE SNIPPETS (FROM FILE) ----------------\n",
        "# This script contains the exact code snippets you provided for saving and loading a model.\n",
        "# NOTE: This script assumes that a trained 'model' object and a test set 'X_test' already exist.\n",
        "\n",
        "# Import the required library\n",
        "import pickle\n",
        "\n",
        "# ---------------- 1. SAVING THE TRAINED MODEL ----------------\n",
        "# This is the first code snippet you provided.\n",
        "# It saves a pre-existing, trained model object to a file named \"model.pkl\".\n",
        "# The file is opened in 'write-binary' (\"wb\") mode.\n",
        "\n",
        "pickle.dump(model, open(\"model.pkl\", \"wb\"))\n",
        "\n",
        "\n",
        "# ---------------- 2. LOADING THE SAVED MODEL ----------------\n",
        "# This is the second code snippet you provided.\n",
        "# It loads the model back from the \"model.pkl\" file.\n",
        "# The file is opened in 'read-binary' (\"rb\") mode.\n",
        "\n",
        "model = pickle.load(open(\"model.pkl\", 'rb'))\n",
        "\n",
        "\n",
        "# ---------------- 3. USING THE LOADED MODEL FOR PREDICTIONS ----------------\n",
        "# This is the final code snippet you provided.\n",
        "# It uses the newly loaded 'model' object to make predictions on 'X_test'.\n",
        "\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# ---------------- END OF SNIPPETS ----------------\n"
      ],
      "metadata": {
        "id": "nU9DrWUzwkWd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------- COMPREHENSIVE FEATURE ANALYSIS PIPELINE (VIF & RFE) ----------------\n",
        "# This script contains ALL the code you provided for visual correlation, iterative VIF checks,\n",
        "# and Recursive Feature Elimination (RFE).\n",
        "\n",
        "# Import required libraries\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
        "from sklearn.feature_selection import RFE\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "# ---------------- 1. LOAD DATA ----------------\n",
        "# This section loads the California Housing dataset as specified.\n",
        "print(\"--- Step 1: Loading the California Housing Dataset ---\")\n",
        "data = fetch_california_housing()\n",
        "df = pd.DataFrame(data.data, columns=data.feature_names)\n",
        "df['Price'] = data.target\n",
        "print(\"Dataset loaded successfully. Preview:\")\n",
        "print(df.head())\n",
        "print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
        "\n",
        "\n",
        "# ---------------- 2. VISUAL CORRELATION ANALYSIS ----------------\n",
        "# This section creates the heatmap and clustermap from your code.\n",
        "print(\"--- Step 2: Generating Visual Correlation Plots ---\")\n",
        "\n",
        "# 2a. Correlation Heatmap\n",
        "print(\"Displaying Correlation Heatmap...\")\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(df.corr(), annot=True, vmin=-1, vmax=1, cmap='coolwarm')\n",
        "plt.title('Correlation Heatmap of Features')\n",
        "plt.show()\n",
        "\n",
        "# 2b. Correlation Clustermap\n",
        "print(\"\\nDisplaying Correlation Clustermap...\")\n",
        "# A clustermap reorganizes the rows and columns to group similar features together.\n",
        "\n",
        "sns.clustermap(df.corr(), vmin=-1, vmax=1, annot=True, cmap='coolwarm')\n",
        "plt.suptitle('Correlation Clustermap of Features', y=1.02)\n",
        "plt.show()\n",
        "print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
        "\n",
        "\n",
        "# ---------------- 3. ITERATIVE VIF CALCULATION AND FEATURE ELIMINATION ----------------\n",
        "# This section replicates the manual, step-by-step process of dropping features\n",
        "# and recalculating VIF that was present in your code.\n",
        "print(\"--- Step 3: Iterative VIF Calculation ---\")\n",
        "\n",
        "# Start with a copy of the full dataset for modification.\n",
        "df1 = df.copy()\n",
        "df1.drop('Price', axis=1, inplace=True) # VIF is calculated on features only\n",
        "\n",
        "# --- ROUND 1: Initial VIF Calculation ---\n",
        "print(\"\\n--- VIF Round 1 (After dropping 'Longitude') ---\")\n",
        "# Your code dropped 'Longitude' first.\n",
        "df1.drop(\"Longitude\", axis=1, inplace=True)\n",
        "vif = pd.DataFrame()\n",
        "vif[\"Feature\"] = df1.columns\n",
        "vif[\"VIF\"] = [variance_inflation_factor(df1.values, i) for i in range(len(df1.columns))]\n",
        "print(vif)\n",
        "\n",
        "# --- ROUND 2: Recalculate VIF after dropping 'AveRooms' ---\n",
        "print(\"\\n--- VIF Round 2 (After dropping 'AveRooms') ---\")\n",
        "# Your code then dropped 'AveRooms'.\n",
        "df1.drop(\"AveRooms\", axis=1, inplace=True)\n",
        "vif = pd.DataFrame()\n",
        "vif[\"Feature\"] = df1.columns\n",
        "vif[\"VIF\"] = [variance_inflation_factor(df1.values, i) for i in range(len(df1.columns))]\n",
        "print(vif)\n",
        "\n",
        "# --- ROUND 3: Recalculate VIF after dropping 'Latitude' ---\n",
        "print(\"\\n--- VIF Round 3 (After dropping 'Latitude') ---\")\n",
        "# Finally, your code dropped 'Latitude'.\n",
        "df1.drop(\"Latitude\", axis=1, inplace=True)\n",
        "vif = pd.DataFrame()\n",
        "vif[\"Feature\"] = df1.columns\n",
        "vif[\"VIF\"] = [variance_inflation_factor(df1.values, i) for i in range(len(df1.columns))]\n",
        "print(vif)\n",
        "print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
        "\n",
        "\n",
        "# ---------------- 4. RECURSIVE FEATURE ELIMINATION (RFE) ----------------\n",
        "# This section performs RFE on the ORIGINAL, full dataset as specified in your code.\n",
        "print(\"--- Step 4: Performing Recursive Feature Elimination (RFE) ---\")\n",
        "\n",
        "# IMPORTANT: Your code resets X and y to use the original 'df', not the modified 'df1'.\n",
        "X = df.iloc[:, :-1]   # All features from the original dataframe\n",
        "y = df.iloc[:, -1]    # The target variable\n",
        "\n",
        "# Initialize RFE with a Linear Regression model, selecting the top 5 features.\n",
        "rfe = RFE(estimator=LinearRegression(), n_features_to_select=5)\n",
        "\n",
        "# Fit RFE on the data.\n",
        "rfe.fit(X, y)\n",
        "\n",
        "# The predict step was in your code, so it is included here.\n",
        "# As your comment noted, it's not strictly necessary for the selection process itself.\n",
        "rfe.predict(X)\n",
        "\n",
        "# --- Display RFE Results ---\n",
        "print(\"\\n--- RFE Results ---\")\n",
        "# `rfe.support_` is a boolean mask of the selected features.\n",
        "print(\"RFE Support (Mask):\")\n",
        "print(rfe.support_)\n",
        "\n",
        "# `rfe.ranking_` shows the rank of each feature (1 is best).\n",
        "print(\"\\nRFE Feature Ranking:\")\n",
        "print(rfe.ranking_)\n",
        "\n",
        "# Get the actual names of the selected features using the support mask.\n",
        "selected_features = X.columns[rfe.support_]\n",
        "print(\"\\nFinal Selected Features by RFE:\")\n",
        "print(selected_features.tolist())\n",
        "print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
        "print(\"This concludes all the code provided for this section.\")\n",
        "\n",
        "# ---------------- END OF PIPELINE ----------------\n",
        "\n"
      ],
      "metadata": {
        "id": "-G93mWK2vFdr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------- COMPREHENSIVE REGULARIZED REGRESSION PIPELINE (LASSO, RIDGE, ELASTICNET) ----------------\n",
        "# This script contains ALL the code from your file for implementing Lasso, Ridge, and ElasticNet regression.\n",
        "\n",
        "# Import required libraries\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LassoCV, RidgeCV, ElasticNetCV\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "# ---------------- 1. LOAD AND PREPROCESS DATA ----------------\n",
        "# The 'mpg' dataset was used in your file for these models.\n",
        "print(\"--- Step 1: Loading and Cleaning the 'mpg' Dataset ---\")\n",
        "df = sns.load_dataset('mpg')\n",
        "\n",
        "# Perform the exact data cleaning steps from your file:\n",
        "# 1. Drop rows with any missing values.\n",
        "df.dropna(inplace=True)\n",
        "# 2. Drop non-numeric columns that cannot be used in the model.\n",
        "df.drop(['origin', 'name'], axis=1, inplace=True)\n",
        "\n",
        "print(\"Data successfully cleaned. Preview of the final dataset:\")\n",
        "print(df.head())\n",
        "print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
        "\n",
        "\n",
        "# ---------------- 2. DEFINE FEATURES, TARGET, SPLIT & SCALE ----------------\n",
        "# Define features (X) and target (y).\n",
        "X = df.drop('mpg', axis=1)\n",
        "y = df['mpg']\n",
        "\n",
        "# Split the data into training and testing sets.\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Scale the features. Regularized models are sensitive to feature scales.\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "\n",
        "# ---------------- 3. LASSO REGRESSION WITH CROSS-VALIDATION (LassoCV) ----------------\n",
        "# This section contains the exact implementation of LassoCV from your file.\n",
        "print(\"--- Step 3: Implementing Lasso Regression (LassoCV) ---\")\n",
        "\n",
        "# Initialize LassoCV. It will test multiple alphas using 5-fold cross-validation.\n",
        "lasso_cv = LassoCV(cv=5)\n",
        "# Fit the model on the scaled training data.\n",
        "lasso_cv.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Make predictions on the test set.\n",
        "y_pred_lasso = lasso_cv.predict(X_test_scaled)\n",
        "\n",
        "# Print the results, just as in your file.\n",
        "print(f\"Best Alpha found by LassoCV: {lasso_cv.alpha_:.4f}\")\n",
        "print(f\"R-squared score (Lasso): {r2_score(y_test, y_pred_lasso):.4f}\")\n",
        "print(\"Lasso Coefficients:\")\n",
        "print(pd.DataFrame(lasso_cv.coef_, index=X.columns, columns=[\"Coefficient\"]))\n",
        "print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
        "\n",
        "\n",
        "# ---------------- 4. RIDGE REGRESSION WITH CROSS-VALIDATION (RidgeCV) ----------------\n",
        "# This section contains the exact implementation of RidgeCV from your file.\n",
        "print(\"--- Step 4: Implementing Ridge Regression (RidgeCV) ---\")\n",
        "\n",
        "# Initialize RidgeCV with 5-fold cross-validation.\n",
        "ridge_cv = RidgeCV(cv=5)\n",
        "# Fit the model.\n",
        "ridge_cv.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Make predictions.\n",
        "y_pred_ridge = ridge_cv.predict(X_test_scaled)\n",
        "\n",
        "# Print the results.\n",
        "print(f\"Best Alpha found by RidgeCV: {ridge_cv.alpha_:.4f}\")\n",
        "print(f\"R-squared score (Ridge): {r2_score(y_test, y_pred_ridge):.4f}\")\n",
        "print(\"Ridge Coefficients:\")\n",
        "print(pd.DataFrame(ridge_cv.coef_, index=X.columns, columns=[\"Coefficient\"]))\n",
        "print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
        "\n",
        "\n",
        "# ---------------- 5. ELASTICNET REGRESSION WITH CROSS-VALIDATION (ElasticNetCV) ----------------\n",
        "# This section contains the exact implementation of ElasticNetCV from your file.\n",
        "print(\"--- Step 5: Implementing ElasticNet Regression (ElasticNetCV) ---\")\n",
        "\n",
        "# Initialize ElasticNetCV with 5-fold cross-validation.\n",
        "# It will find the best alpha and the best l1_ratio.\n",
        "elastic_net_cv = ElasticNetCV(cv=5)\n",
        "# Fit the model.\n",
        "elastic_net_cv.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Make predictions.\n",
        "y_pred_elastic = elastic_net_cv.predict(X_test_scaled)\n",
        "\n",
        "# Print the results.\n",
        "print(f\"Best Alpha found by ElasticNetCV: {elastic_net_cv.alpha_:.4f}\")\n",
        "print(f\"Best L1 Ratio found by ElasticNetCV: {elastic_net_cv.l1_ratio_:.4f}\")\n",
        "print(f\"R-squared score (ElasticNet): {r2_score(y_test, y_pred_elastic):.4f}\")\n",
        "print(\"ElasticNet Coefficients:\")\n",
        "print(pd.DataFrame(elastic_net_cv.coef_, index=X.columns, columns=[\"Coefficient\"]))\n",
        "print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
        "print(\"This concludes all the code for regularized regression from your file.\")\n",
        "\n",
        "# ---------------- END OF PIPELINE ----------------\n",
        "\n"
      ],
      "metadata": {
        "id": "f1tvqrPrvXbz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}