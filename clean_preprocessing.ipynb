{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNtPU2dLAUIVllIdVnjX/sr",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ashish78905/OPTICONNECT_CALLL_CENTER_ANALYSIS-ASSIGNMENT/blob/main/clean_preprocessing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# HANDALING MISSING VALUE"
      ],
      "metadata": {
        "id": "fm-cZ2PBHTQ_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CUqfH8URHPGN"
      },
      "outputs": [],
      "source": [
        "# 1. To see how many missing values are there in each column\n",
        "df.isnull().sum()\n",
        "\n",
        "# 2. Handle missing values by dropping them row-wise (default)\n",
        "df.dropna()\n",
        "\n",
        "# 3. Handle missing values by dropping them column-wise\n",
        "df.dropna(axis=1)\n",
        "\n",
        "# 4. Impute missing values of any numerical column with its mean\n",
        "df['any_numerical_column'].fillna(df['any_numerical_column'].mean())\n",
        "\n",
        "# 5. Handle missing values by filling with any constant (e.g., 0 or random character)\n",
        "df['any_column'].fillna(0)\n",
        "\n",
        "# 6. Impute missing values of any numerical column with its median\n",
        "df['any_numerical_column'] = df['any_numerical_column'].fillna(df['any_numerical_column'].median())\n",
        "\n",
        "# 7. Check overall dataframe info (datatypes, non-null counts, memory usage)\n",
        "df.info()\n",
        "\n",
        "# 8. Check the shape of dataframe after dropping missing values\n",
        "df.dropna(axis=0).shape\n",
        "\n",
        "# 9. Visualize distribution of any numerical column\n",
        "#    If normal distribution → impute with mean, else → with median\n",
        "sns.histplot(df['any_numerical_column'], kde=True)\n",
        "\n",
        "# 10. Visualize distribution after imputation (mean/median)\n",
        "sns.distplot(df['any_numerical_column_after_imputation'], kde=True)\n",
        "\n",
        "# 11. For categorical data, check mode of any categorical column\n",
        "df[df['any_categorical_column'].notna()]['any_categorical_column'].mode()\n",
        "\n",
        "# 12. Final check to see if any null values remain in categorical column\n",
        "df['any_categorical_column'].isna().sum()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# HANDALING IMBALANCED DATASET"
      ],
      "metadata": {
        "id": "Nu6O-wXuHbaY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ================================================================\n",
        "# Handling Imbalanced Dataset (Any Dataset) - Same Sequence Preserved\n",
        "# ================================================================\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "np.random.seed(1)  # for reproducibility\n",
        "\n",
        "# 1. Define dataset size and imbalance ratio\n",
        "no_samples = 1000\n",
        "class_0_ratio = 0.9\n",
        "no_class_0 = int(no_samples * class_0_ratio)  # class 0 → 90% (900 samples)\n",
        "no_class_1 = 100                              # class 1 → 10% (100 samples)\n",
        "\n",
        "no_class_0, no_class_1  # check number of samples per class\n",
        "\n",
        "len(np.random.normal(0, 1, no_class_0))  # length of generated majority class samples\n",
        "\n",
        "# 2. Create majority class (class 0)\n",
        "class_0 = {\n",
        "    'feature1': np.random.normal(0, 1, no_class_0),\n",
        "    'feature2': np.random.normal(0, 1, no_class_0),\n",
        "    'target': [0] * no_class_0\n",
        "}\n",
        "class_0 = pd.DataFrame(class_0)\n",
        "class_0  # check class 0 dataframe\n",
        "\n",
        "# 3. Create minority class (class 1)\n",
        "class_1 = pd.DataFrame({\n",
        "    'feature1': np.random.normal(3, 1, no_class_1),\n",
        "    'feature2': np.random.normal(3, 1, no_class_1),\n",
        "    'target': [1] * no_class_1\n",
        "})\n",
        "class_1  # check class 1 dataframe\n",
        "\n",
        "# 4. Combine both classes\n",
        "df = pd.concat([class_0, class_1]).reset_index(drop=True)\n",
        "df  # final dataset\n",
        "\n",
        "# 5. Check imbalance in target\n",
        "df.target.value_counts()  # shows ~900 vs 100\n",
        "\n",
        "# ------------------------------------------------\n",
        "# UPSAMPLING\n",
        "# ------------------------------------------------\n",
        "\n",
        "# 6. Separate minority and majority classes\n",
        "df_minority = df[df.target == 1]\n",
        "df_majority = df[df.target == 0]   # just demonstration\n",
        "\n",
        "from sklearn.utils import resample  # resampling utility\n",
        "\n",
        "# 7. Upsample minority class\n",
        "df_minority_upsampled = resample(\n",
        "    df_minority,\n",
        "    replace=True,                     # with replacement\n",
        "    n_samples=len(df_majority),       # match majority count\n",
        "    random_state=1\n",
        ")\n",
        "\n",
        "df_minority_upsampled.shape  # check shape of minority after upsampling\n",
        "\n",
        "# 8. Combine majority and upsampled minority\n",
        "df_upsampled = pd.concat([df_majority, df_minority_upsampled])\n",
        "df_upsampled  # final upsampled dataframe\n",
        "\n",
        "df_upsampled.target.value_counts()  # now balanced (equal counts)\n",
        "\n",
        "# ------------------------------------------------\n",
        "# DOWNSAMPLING\n",
        "# ------------------------------------------------\n",
        "\n",
        "# 9. Downsample majority class\n",
        "df_majority_downsampled = resample(\n",
        "    df_majority,\n",
        "    replace=False,                    # without replacement\n",
        "    n_samples=len(df_minority),       # match minority count\n",
        "    random_state=1\n",
        ")\n",
        "\n",
        "df_downsampled = pd.concat([df_majority_downsampled, df_minority])\n",
        "df_downsampled  # downsampled dataset\n",
        "\n",
        "df_downsampled.target.value_counts()  # balanced counts\n",
        "\n",
        "# ------------------------------------------------\n",
        "# SMOTE\n",
        "# ------------------------------------------------\n",
        "\n",
        "from sklearn.datasets import make_classification\n",
        "\n",
        "# 10. Create imbalanced dataset for SMOTE demonstration\n",
        "X, y = make_classification(\n",
        "    n_samples=1000,\n",
        "    n_redundant=0,\n",
        "    n_features=2,\n",
        "    n_clusters_per_class=1,\n",
        "    weights=[0.90],   # imbalance\n",
        "    random_state=1\n",
        ")\n",
        "\n",
        "X  # generated features\n",
        "y  # generated target\n",
        "\n",
        "len(y[y == 0])  # count of class 0\n",
        "\n",
        "# 11. Convert to DataFrame\n",
        "df1 = pd.DataFrame(X, columns=['f1', 'f2'])\n",
        "df2 = pd.DataFrame(y, columns=['target'])\n",
        "final_df = pd.concat([df1, df2], axis=1)\n",
        "final_df  # dataset before SMOTE\n",
        "\n",
        "final_df.target.value_counts()  # imbalance in target\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "plt.scatter(final_df['f1'], final_df['f2'], c=final_df['target'])\n",
        "# visualization before SMOTE\n",
        "\n",
        "# 12. Apply SMOTE\n",
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "oversample = SMOTE()  # create SMOTE object\n",
        "\n",
        "X, y = oversample.fit_resample(final_df[['f1', 'f2']], final_df['target'])\n",
        "\n",
        "X.shape  # resampled features shape\n",
        "y.shape  # resampled target shape\n",
        "\n",
        "len(y[y == 0])  # new class 0 count\n",
        "len(y[y == 1])  # new class 1 count\n",
        "\n",
        "# 13. Convert back to DataFrame\n",
        "df1 = pd.DataFrame(X, columns=['f1', 'f2'])\n",
        "df2 = pd.DataFrame(y, columns=['target'])\n",
        "oversample_df = pd.concat([df1, df2], axis=1)\n",
        "oversample_df  # dataset after SMOTE\n",
        "\n",
        "plt.scatter(oversample_df['f1'], oversample_df['f2'], c=oversample_df['target'])\n",
        "# visualization after SMOTE\n",
        "\n",
        "oversample_df[oversample_df.target == 1]  # check minority samples\n"
      ],
      "metadata": {
        "id": "mKq-UA3sHpy3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# INTERPOLATION"
      ],
      "metadata": {
        "id": "U8Kvuu-iWBpp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ================================================================\n",
        "# Interpolation Techniques (Any Dataset) - Same Sequence Preserved\n",
        "# ================================================================\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.interpolate import interp1d\n",
        "\n",
        "# ------------------------------------------------\n",
        "# 1. Linear Interpolation\n",
        "# ------------------------------------------------\n",
        "\n",
        "x = np.array([1, 2, 3, 4, 5])\n",
        "y = np.array([1, 3, 5, 7, 9])\n",
        "\n",
        "plt.scatter(x, y)  # original data points\n",
        "\n",
        "# interpolate the data using linear interpolation\n",
        "x_new = np.linspace(1, 5, 10)  # create new x values\n",
        "y_interp = np.interp(x_new, x, y)  # linear interpolation\n",
        "\n",
        "plt.scatter(x_new, y_interp)  # interpolated points\n",
        "\n",
        "# ------------------------------------------------\n",
        "# 2. Cubic Interpolation\n",
        "# ------------------------------------------------\n",
        "\n",
        "x = np.array([1, 2, 3, 4, 5])\n",
        "y = np.array([1, 8, 27, 64, 125])\n",
        "\n",
        "plt.scatter(x, y)  # original data points\n",
        "\n",
        "# cubic interpolation function\n",
        "f = interp1d(x, y, kind='cubic')\n",
        "\n",
        "x_new = np.linspace(1, 5, 10)\n",
        "y_interp = f(x_new)\n",
        "\n",
        "y_interp  # interpolated values\n",
        "\n",
        "plt.scatter(x, y)          # original points\n",
        "plt.scatter(x_new, y_interp)  # cubic interpolated points\n",
        "\n",
        "# ------------------------------------------------\n",
        "# 3. Polynomial Interpolation\n",
        "# ------------------------------------------------\n",
        "\n",
        "x = np.array([1, 2, 3, 4, 5])\n",
        "y = np.array([1, 4, 9, 1, 25])\n",
        "\n",
        "# interpolate using polynomial interpolation (degree 2 here)\n",
        "p = np.polyfit(x, y, 2)\n",
        "\n",
        "x_new = np.linspace(1, 5, 10)\n",
        "y_interp = np.polyval(p, x_new)\n",
        "\n",
        "plt.scatter(x, y)          # original points\n",
        "plt.scatter(x_new, y_interp)  # polynomial interpolated points\n"
      ],
      "metadata": {
        "id": "j07klsdMWITt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# HANDALING OUTLIERS"
      ],
      "metadata": {
        "id": "hK3MWXswX9n4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ================================================================\n",
        "# Outlier Detection and Handling Techniques (Any Dataset)\n",
        "# ================================================================\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# ------------------------------------------------\n",
        "# 1. Create Data\n",
        "# ------------------------------------------------\n",
        "salary = [11, 40, 45, 68, 65, 68, 78, 90, 57, 74,\n",
        "          91, 92, 88, 68, 57, 48, 99, 101, 68, 77,\n",
        "          110, 140]\n",
        "\n",
        "df = pd.DataFrame(salary, columns=['Salary'])\n",
        "df.describe()\n",
        "\n",
        "# ------------------------------------------------\n",
        "# 2. Check Outliers → distplot & boxplot\n",
        "# ------------------------------------------------\n",
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "sns.histplot(df['Salary'], kde=True)\n",
        "plt.title(\"Dist plot\")\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "sns.boxplot(data=df, x='Salary')\n",
        "plt.title(\"Box plot\")\n",
        "\n",
        "# ------------------------------------------------\n",
        "# 3. Dropping the Outlier using IQR Method\n",
        "# ------------------------------------------------\n",
        "Q1 = df['Salary'].quantile(0.25)\n",
        "Q3 = df['Salary'].quantile(0.75)\n",
        "IQR = Q3 - Q1\n",
        "\n",
        "lower_fence = Q1 - 1.5 * IQR\n",
        "upper_fence = Q3 + 1.5 * IQR\n",
        "\n",
        "lower_fence   # lowest side outlier threshold\n",
        "upper_fence   # highest side outlier threshold\n",
        "\n",
        "df_filtered = df[(df.Salary >= lower_fence) & (df.Salary <= upper_fence)]\n",
        "df_filtered.shape  # shape after removing outliers\n",
        "\n",
        "# confirm there is no outlier after filtering\n",
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "sns.histplot(df_filtered['Salary'], kde=True)\n",
        "plt.title(\"Dist plot after dropping outliers\")\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "sns.boxplot(data=df_filtered, x='Salary')\n",
        "plt.title(\"Box plot after dropping outliers\")\n",
        "\n",
        "# ------------------------------------------------\n",
        "# 4. Imputation with Mean\n",
        "# ------------------------------------------------\n",
        "df['Salary_imputed_mean'] = np.where(\n",
        "    (df.Salary >= upper_fence) | (df.Salary <= lower_fence),\n",
        "    df['Salary'].mean(),\n",
        "    df['Salary']\n",
        ")\n",
        "\n",
        "# check after mean imputation\n",
        "plt.subplot(1, 2, 1)\n",
        "sns.histplot(df['Salary_imputed_mean'], kde=True)\n",
        "plt.title(\"Dist plot after mean imputation\")\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "sns.boxplot(data=df, x='Salary_imputed_mean')\n",
        "plt.title(\"Box plot after mean imputation\")\n",
        "\n",
        "# ------------------------------------------------\n",
        "# 5. Imputation with Median\n",
        "# ------------------------------------------------\n",
        "df['Salary_imputed_median'] = np.where(\n",
        "    (df.Salary >= upper_fence) | (df.Salary <= lower_fence),\n",
        "    df['Salary'].median(),\n",
        "    df['Salary']\n",
        ")\n",
        "\n",
        "# check after median imputation\n",
        "plt.subplot(1, 2, 1)\n",
        "sns.histplot(df['Salary_imputed_median'], kde=True)\n",
        "plt.title(\"Dist plot after median imputation\")\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "sns.boxplot(data=df, x='Salary_imputed_median')\n",
        "plt.title(\"Box plot after median imputation\")\n",
        "\n",
        "plt.show()\n",
        "\n",
        "# ------------------------------------------------\n",
        "# 6. Capping Technique (Replace Outliers with 5th & 95th Percentiles)\n",
        "# ------------------------------------------------\n",
        "lower_cap = df['Salary'].quantile(0.05)  # lower cap\n",
        "upper_cap = df['Salary'].quantile(0.95)  # upper cap\n",
        "\n",
        "lower_cap, upper_cap\n",
        "\n",
        "df['Salary_capped'] = np.where(\n",
        "    df['Salary'] < lower_cap, lower_cap,\n",
        "    np.where(df['Salary'] > upper_cap, upper_cap, df['Salary'])\n",
        ")\n",
        "\n",
        "# confirm after capping\n",
        "plt.subplot(1, 2, 1)\n",
        "sns.histplot(df['Salary_capped'], kde=True)\n",
        "plt.title(\"Dist plot after capping\")\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "sns.boxplot(data=df, x='Salary_capped')\n",
        "plt.title(\"Box plot after capping\")\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "6xGxBfG2YCv9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# FEATURE SCALING"
      ],
      "metadata": {
        "id": "3Q-EqozkYtaE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ================================================================\n",
        "# Feature Scaling Techniques (Any Dataset)\n",
        "# ================================================================\n",
        "\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler, normalize\n",
        "\n",
        "# ------------------------------------------------\n",
        "# 1. Load Data\n",
        "# ------------------------------------------------\n",
        "df = sns.load_dataset('tips')\n",
        "df\n",
        "\n",
        "# Note:\n",
        "# Feature scaling is optional, as it does not change the distribution\n",
        "# of data. Distribution remains same before and after scaling.\n",
        "\n",
        "# ------------------------------------------------\n",
        "# 2. Standardization (mu=0, sigma=1)\n",
        "# ------------------------------------------------\n",
        "scaler = StandardScaler()   # create object of StandardScaler\n",
        "scaler   # object created\n",
        "\n",
        "# apply fit_transform on training data\n",
        "scaler.fit_transform(df[['total_bill', 'tip']])\n",
        "\n",
        "# show standardized data as DataFrame\n",
        "pd.DataFrame(\n",
        "    scaler.fit_transform(df[['total_bill', 'tip']]),\n",
        "    columns=['total_bill', 'tip']\n",
        ")\n",
        "\n",
        "# transform a new value (NOTE: we do NOT fit on test data)\n",
        "scaler.transform([[13, 1]])\n",
        "\n",
        "# ------------------------------------------------\n",
        "# 3. Min-Max Scaling (Normalization → values between 0 and 1)\n",
        "# ------------------------------------------------\n",
        "min_max = MinMaxScaler()   # create object of MinMaxScaler\n",
        "\n",
        "# apply fit_transform on training data\n",
        "min_max.fit_transform(df[['total_bill', 'tip']])\n",
        "\n",
        "# transform a new value (again: only transform on test data)\n",
        "min_max.transform([[10, 2]])\n",
        "\n",
        "# ------------------------------------------------\n",
        "# 4. Unit Vector Scaling\n",
        "# ------------------------------------------------\n",
        "# (normalize data vectors → length = 1)\n",
        "uv = normalize(df[['total_bill']])\n",
        "\n",
        "uv   # since it's 1D, all values become 1\n"
      ],
      "metadata": {
        "id": "lbXaI4tQYxqf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#  Encoding"
      ],
      "metadata": {
        "id": "bVPYpTVuZij-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ================================================================\n",
        "# Data Encoding Techniques (Any Dataset)\n",
        "# ================================================================\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import OneHotEncoder, LabelEncoder, OrdinalEncoder\n",
        "\n",
        "# ------------------------------------------------\n",
        "# 1. One Hot Encoding (Nominal Data)\n",
        "# ------------------------------------------------\n",
        "df = pd.DataFrame({'status': ['single', 'married', 'separated',\n",
        "                              'single', 'single', 'married', 'married']})\n",
        "df   # original dataframe\n",
        "\n",
        "encoder = OneHotEncoder()   # create OneHotEncoder object\n",
        "encoder   # confirm object created\n",
        "\n",
        "encoded = encoder.fit_transform(df[['status']]).toarray()   # encoded array\n",
        "encoder.get_feature_names_out()   # column names after encoding\n",
        "\n",
        "encoder_df = pd.DataFrame(encoded, columns=encoder.get_feature_names_out())\n",
        "encoder_df   # encoded DataFrame\n",
        "\n",
        "# new data transformation\n",
        "encoder.transform([['single']]).toarray()\n",
        "\n",
        "# combine encoded columns with original\n",
        "pd.concat([df, encoder_df], axis=1)\n",
        "\n",
        "# Note: In practice, we delete the original column and keep n-1 encoded columns.\n",
        "\n",
        "# ------------------------------------------------\n",
        "# 2. Label Encoding\n",
        "# ------------------------------------------------\n",
        "label_encoder = LabelEncoder()\n",
        "\n",
        "label_encoder.fit_transform(df[['status']])   # encodes status column\n",
        "label_encoder.transform([['single']])\n",
        "label_encoder.transform([['separated']])\n",
        "\n",
        "# ------------------------------------------------\n",
        "# 3. Ordinal Encoding\n",
        "# ------------------------------------------------\n",
        "# Example: High School < Graduate < Post Graduate < PhD\n",
        "\n",
        "df = pd.DataFrame({\"qualification\": [\"HS\", \"GR\", \"PG\", \"PhD\",\n",
        "                                     \"HS\", \"HS\", \"PhD\", \"PG\"]})\n",
        "df   # original dataframe\n",
        "\n",
        "encoder = OrdinalEncoder(categories=[[\"HS\", \"GR\", \"PG\", \"PhD\"]])   # define order\n",
        "encoder.fit_transform(df[['qualification']])   # apply encoding\n",
        "\n",
        "# new data transformation\n",
        "encoder.transform([['PG']])\n",
        "\n",
        "# ------------------------------------------------\n",
        "# 4. Target Guided Ordinal Encoding\n",
        "# ------------------------------------------------\n",
        "# Replace category with mean (or median) of target variable\n",
        "\n",
        "df = pd.DataFrame({'time': ['lunch', 'breakfast', 'dinner',\n",
        "                            'lunch', 'breakfast', 'dinner',\n",
        "                            'lunch', 'breakfast', 'dinner'],\n",
        "                   'total_bill': [120, 130, 90, 125, 150,\n",
        "                                  190, 160, 180, 189]})\n",
        "df   # original dataframe\n",
        "\n",
        "mean_price = df.groupby('time')['total_bill'].mean().to_dict()   # mean wrt target\n",
        "mean_price\n",
        "\n",
        "df['time_encoded'] = df['time'].map(mean_price)   # replace categories with means\n",
        "df\n"
      ],
      "metadata": {
        "id": "PbFFEf4BZmIm"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}