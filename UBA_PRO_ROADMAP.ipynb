{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ashish78905/OPTICONNECT_CALLL_CENTER_ANALYSIS-ASSIGNMENT/blob/main/UBA_PRO_ROADMAP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5a176de9",
      "metadata": {
        "id": "5a176de9"
      },
      "source": [
        "---\n",
        "\n",
        "# ðŸ—ºï¸ UBA_PRO COMPLETE DEVELOPMENT ROADMAP\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "# ðŸ“‹ ROADMAP OVERVIEW\n",
        "\n",
        "```\n",
        "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
        "â”‚                        UBA_PRO DEVELOPMENT PHASES                                â”‚\n",
        "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
        "â”‚                                                                                  â”‚\n",
        "â”‚  PHASE 1: FOUNDATION                                                             â”‚\n",
        "â”‚  â”œâ”€â”€ Environment Setup                                                           â”‚\n",
        "â”‚  â”œâ”€â”€ Project Structure Creation                                                  â”‚\n",
        "â”‚  â””â”€â”€ Database Models & Migrations                                                â”‚\n",
        "â”‚                                                                                  â”‚\n",
        "â”‚  PHASE 2: CORE UTILITIES                                                         â”‚\n",
        "â”‚  â”œâ”€â”€ Hash Utilities                                                              â”‚\n",
        "â”‚  â”œâ”€â”€ Encoding Utilities                                                          â”‚\n",
        "â”‚  â”œâ”€â”€ Timestamp Utilities                                                         â”‚\n",
        "â”‚  â””â”€â”€ Configuration System                                                        â”‚\n",
        "â”‚                                                                                  â”‚\n",
        "â”‚  PHASE 3: DATA LAYER                                                             â”‚\n",
        "â”‚  â”œâ”€â”€ DSPM Database Connector                                                     â”‚\n",
        "â”‚  â”œâ”€â”€ Data Extraction Services                                                    â”‚\n",
        "â”‚  â”œâ”€â”€ CSV/Parquet Connectors                                                      â”‚\n",
        "â”‚  â””â”€â”€ CoreDataFrame Implementation                                                â”‚\n",
        "â”‚                                                                                  â”‚\n",
        "â”‚  PHASE 4: USER & ENTITY MANAGEMENT                                               â”‚\n",
        "â”‚  â”œâ”€â”€ User Profile Service                                                        â”‚\n",
        "â”‚  â”œâ”€â”€ Entity Management                                                           â”‚\n",
        "â”‚  â”œâ”€â”€ Behavior Baseline Storage                                                   â”‚\n",
        "â”‚  â””â”€â”€ User Risk Profile Database                                                  â”‚\n",
        "â”‚                                                                                  â”‚\n",
        "â”‚  PHASE 5: FEATURE EXTRACTION                                                     â”‚\n",
        "â”‚  â”œâ”€â”€ Login Feature Extractor                                                     â”‚\n",
        "â”‚  â”œâ”€â”€ Access Feature Extractor                                                    â”‚\n",
        "â”‚  â”œâ”€â”€ Data Movement Feature Extractor                                             â”‚\n",
        "â”‚  â”œâ”€â”€ Time-based Feature Extractor                                                â”‚\n",
        "â”‚  â””â”€â”€ Feature Pipeline Integration                                                â”‚\n",
        "â”‚                                                                                  â”‚\n",
        "â”‚  PHASE 6: ML MODEL DEVELOPMENT                                                   â”‚\n",
        "â”‚  â”œâ”€â”€ Base Model Interface                                                        â”‚\n",
        "â”‚  â”œâ”€â”€ Login Anomaly Model (Isolation Forest)                                      â”‚\n",
        "â”‚  â”œâ”€â”€ Access Anomaly Model                                                        â”‚\n",
        "â”‚  â”œâ”€â”€ Data Transfer Anomaly Model                                                 â”‚\n",
        "â”‚  â”œâ”€â”€ Time Anomaly Model                                                          â”‚\n",
        "â”‚  â”œâ”€â”€ Model Training Pipeline                                                     â”‚\n",
        "â”‚  â”œâ”€â”€ Model Validation & Testing                                                  â”‚\n",
        "â”‚  â””â”€â”€ Model Persistence (Save/Load)                                               â”‚\n",
        "â”‚                                                                                  â”‚\n",
        "â”‚  PHASE 7: ANALYTICS ENGINE                                                       â”‚\n",
        "â”‚  â”œâ”€â”€ Model Execution Engine                                                      â”‚\n",
        "â”‚  â”œâ”€â”€ Risk Score Calculator                                                       â”‚\n",
        "â”‚  â”œâ”€â”€ Anomaly Aggregator                                                          â”‚\n",
        "â”‚  â””â”€â”€ Scheduler Integration                                                       â”‚\n",
        "â”‚                                                                                  â”‚\n",
        "â”‚  PHASE 8: ALERT & CASE SYSTEM                                                    â”‚\n",
        "â”‚  â”œâ”€â”€ Alert Generation Service                                                    â”‚\n",
        "â”‚  â”œâ”€â”€ Alert Priority System                                                       â”‚\n",
        "â”‚  â”œâ”€â”€ Case Management Service                                                     â”‚\n",
        "â”‚  â””â”€â”€ Workflow Integration                                                        â”‚\n",
        "â”‚                                                                                  â”‚\n",
        "â”‚  PHASE 9: RULE ENGINE                                                            â”‚\n",
        "â”‚  â”œâ”€â”€ Rule Definition System                                                      â”‚\n",
        "â”‚  â”œâ”€â”€ Rule Evaluation Engine                                                      â”‚\n",
        "â”‚  â”œâ”€â”€ Rule-based Detection                                                        â”‚\n",
        "â”‚  â””â”€â”€ MITRE ATT&CK Mapping                                                        â”‚\n",
        "â”‚                                                                                  â”‚\n",
        "â”‚  PHASE 10: API LAYER                                                             â”‚\n",
        "â”‚  â”œâ”€â”€ FastAPI Routes Implementation                                               â”‚\n",
        "â”‚  â”œâ”€â”€ Pydantic Schemas                                                            â”‚\n",
        "â”‚  â”œâ”€â”€ API Authentication                                                          â”‚\n",
        "â”‚  â””â”€â”€ DSPM Integration Endpoints                                                  â”‚\n",
        "â”‚                                                                                  â”‚\n",
        "â”‚  PHASE 11: TESTING & VALIDATION                                                  â”‚\n",
        "â”‚  â”œâ”€â”€ Unit Tests                                                                  â”‚\n",
        "â”‚  â”œâ”€â”€ Integration Tests                                                           â”‚\n",
        "â”‚  â”œâ”€â”€ ML Model Performance Tests                                                  â”‚\n",
        "â”‚  â””â”€â”€ End-to-End Testing                                                          â”‚\n",
        "â”‚                                                                                  â”‚\n",
        "â”‚  PHASE 12: DEPLOYMENT & INTEGRATION                                              â”‚\n",
        "â”‚  â”œâ”€â”€ DSPM Integration                                                            â”‚\n",
        "â”‚  â”œâ”€â”€ Docker Configuration                                                        â”‚\n",
        "â”‚  â”œâ”€â”€ Production Deployment                                                       â”‚\n",
        "â”‚  â””â”€â”€ Monitoring Setup                                                            â”‚\n",
        "â”‚                                                                                  â”‚\n",
        "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "# ðŸ“Š FEATURE MAPPING: OpenUBA â†’ UBA_PRO\n",
        "\n",
        "## What You Will Implement (ALL OpenUBA Features)\n",
        "\n",
        "| OpenUBA Feature | OpenUBA Status | UBA_PRO Location | Priority |\n",
        "|-----------------|---------------|------------------|----------|\n",
        "| **Core Engine** | âœ… Implemented | `uba/analytics/engine.py` | P0 |\n",
        "| **Process Engine** | âœ… Implemented | `uba/services/behavior_service.py` | P0 |\n",
        "| **Model Engine** | âœ… Implemented | `uba/analytics/engine.py` | P0 |\n",
        "| **Model Library** | âš ï¸ Placeholder | `uba/analytics/model_library/` | P0 |\n",
        "| **Dataset Loading** | âœ… Implemented | `uba/connectors/` + `uba/analytics/data_loader.py` | P0 |\n",
        "| **User Management** | âœ… Implemented | `uba/services/profile_service.py` | P1 |\n",
        "| **Risk Calculation** | âŒ Placeholder | `uba/analytics/risk_calculator.py` + `uba/services/risk_service.py` | P0 |\n",
        "| **Anomaly Detection** | âŒ Placeholder | `uba/services/anomaly_service.py` | P0 |\n",
        "| **Alert System** | âŒ Placeholder | `uba/services/alert_service.py` | P1 |\n",
        "| **Case Management** | âŒ Placeholder | `uba/services/case_service.py` | P2 |\n",
        "| **Rule Engine** | âŒ Placeholder | `uba/services/rule_service.py` (NEW) | P2 |\n",
        "| **Hash Utilities** | âœ… Implemented | `uba/utils/hash_utils.py` | P0 |\n",
        "| **Encode Utilities** | âœ… Implemented | `uba/utils/encode_utils.py` | P0 |\n",
        "| **Entity Management** | âš ï¸ Partial | `uba/services/entity_service.py` (NEW) | P1 |\n",
        "| **Display/API** | âš ï¸ Partial | `uba/routes/` | P1 |\n",
        "| **MITRE Mapping** | Config only | `uba/config/mitre_mapping.py` (NEW) | P2 |\n",
        "| **Model Verification** | âœ… Implemented | `uba/utils/model_verification.py` (NEW) | P1 |\n",
        "| **DSPM Integration** | NEW | `uba/services/dspm_integration.py` | P0 |\n",
        "\n",
        "---\n",
        "\n",
        "## Priority Legend\n",
        "- **P0**: Must have for MVP (Minimum Viable Product)\n",
        "- **P1**: Important for full functionality\n",
        "- **P2**: Nice to have, can be added later"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "acc0a174",
      "metadata": {
        "id": "acc0a174"
      },
      "source": [
        "---\n",
        "\n",
        "# ðŸ”§ PHASE 1: FOUNDATION\n",
        "\n",
        "## Goal: Set up the complete development environment and project structure\n",
        "\n",
        "---\n",
        "\n",
        "## Step 1.1: Environment Setup\n",
        "\n",
        "### What You Need to Install:\n",
        "1. **Python 3.10+** - The programming language\n",
        "2. **PostgreSQL** - Database (same as DSPM uses)\n",
        "3. **Git** - Version control\n",
        "4. **VS Code** - Code editor (you already have this)\n",
        "5. **Virtual Environment** - Isolated Python environment\n",
        "\n",
        "### How to Do It:\n",
        "\n",
        "**Task 1.1.1: Create Virtual Environment**\n",
        "- Open terminal in UBA_PRO folder\n",
        "- Create a new Python virtual environment\n",
        "- Activate the virtual environment\n",
        "- This keeps your project dependencies separate from other projects\n",
        "\n",
        "**Task 1.1.2: Create requirements.txt**\n",
        "- List all Python packages your project needs\n",
        "- Key packages: fastapi, uvicorn, sqlalchemy, pandas, numpy, scikit-learn, pydantic\n",
        "\n",
        "**Task 1.1.3: Install Dependencies**\n",
        "- Use pip to install all packages from requirements.txt\n",
        "- Verify installation by importing key packages\n",
        "\n",
        "### Verification Checklist:\n",
        "- [ ] Python 3.10+ installed and working\n",
        "- [ ] Virtual environment created and activated\n",
        "- [ ] All packages installed without errors\n",
        "- [ ] Can import pandas, numpy, sklearn, fastapi\n",
        "\n",
        "---\n",
        "\n",
        "## Step 1.2: Project Structure Creation\n",
        "\n",
        "### What You Will Create:\n",
        "Convert all `.info` files to actual `.py` files with proper structure\n",
        "\n",
        "### Directory Structure to Create:\n",
        "\n",
        "```\n",
        "UBA_PRO/\n",
        "â”œâ”€â”€ uba/\n",
        "â”‚   â”œâ”€â”€ __init__.py                 # Package initializer\n",
        "â”‚   â”œâ”€â”€ main.py                     # FastAPI application entry point\n",
        "â”‚   â”‚\n",
        "â”‚   â”œâ”€â”€ config/\n",
        "â”‚   â”‚   â”œâ”€â”€ __init__.py\n",
        "â”‚   â”‚   â”œâ”€â”€ settings.py             # Environment configuration\n",
        "â”‚   â”‚   â””â”€â”€ constants.py            # Static values and enums\n",
        "â”‚   â”‚\n",
        "â”‚   â”œâ”€â”€ core/\n",
        "â”‚   â”‚   â”œâ”€â”€ __init__.py\n",
        "â”‚   â”‚   â”œâ”€â”€ database.py             # Database connection\n",
        "â”‚   â”‚   â”œâ”€â”€ logging.py              # Logging configuration\n",
        "â”‚   â”‚   â””â”€â”€ exceptions.py           # Custom exceptions\n",
        "â”‚   â”‚\n",
        "â”‚   â”œâ”€â”€ models/\n",
        "â”‚   â”‚   â”œâ”€â”€ __init__.py\n",
        "â”‚   â”‚   â””â”€â”€ uba_models.py           # SQLAlchemy database models\n",
        "â”‚   â”‚\n",
        "â”‚   â”œâ”€â”€ schemas/\n",
        "â”‚   â”‚   â”œâ”€â”€ __init__.py\n",
        "â”‚   â”‚   â””â”€â”€ uba_schemas.py          # Pydantic request/response schemas\n",
        "â”‚   â”‚\n",
        "â”‚   â”œâ”€â”€ routes/\n",
        "â”‚   â”‚   â”œâ”€â”€ __init__.py\n",
        "â”‚   â”‚   â”œâ”€â”€ dashboard.py\n",
        "â”‚   â”‚   â”œâ”€â”€ profiles.py\n",
        "â”‚   â”‚   â”œâ”€â”€ risk.py\n",
        "â”‚   â”‚   â”œâ”€â”€ anomalies.py\n",
        "â”‚   â”‚   â”œâ”€â”€ alerts.py\n",
        "â”‚   â”‚   â”œâ”€â”€ cases.py\n",
        "â”‚   â”‚   â”œâ”€â”€ models.py\n",
        "â”‚   â”‚   â””â”€â”€ behaviors.py\n",
        "â”‚   â”‚\n",
        "â”‚   â”œâ”€â”€ services/\n",
        "â”‚   â”‚   â”œâ”€â”€ __init__.py\n",
        "â”‚   â”‚   â”œâ”€â”€ profile_service.py\n",
        "â”‚   â”‚   â”œâ”€â”€ risk_service.py\n",
        "â”‚   â”‚   â”œâ”€â”€ anomaly_service.py\n",
        "â”‚   â”‚   â”œâ”€â”€ alert_service.py\n",
        "â”‚   â”‚   â”œâ”€â”€ case_service.py\n",
        "â”‚   â”‚   â”œâ”€â”€ behavior_service.py\n",
        "â”‚   â”‚   â”œâ”€â”€ model_service.py\n",
        "â”‚   â”‚   â”œâ”€â”€ rule_service.py         # NEW - not in OpenUBA\n",
        "â”‚   â”‚   â”œâ”€â”€ entity_service.py       # NEW - expanded from OpenUBA\n",
        "â”‚   â”‚   â””â”€â”€ dspm_integration.py     # NEW - DSPM connector\n",
        "â”‚   â”‚\n",
        "â”‚   â”œâ”€â”€ connectors/\n",
        "â”‚   â”‚   â”œâ”€â”€ __init__.py\n",
        "â”‚   â”‚   â”œâ”€â”€ dspm_connector.py       # Connect to DSPM PostgreSQL\n",
        "â”‚   â”‚   â”œâ”€â”€ csv_connector.py\n",
        "â”‚   â”‚   â”œâ”€â”€ parquet_connector.py\n",
        "â”‚   â”‚   â””â”€â”€ elasticsearch_connector.py\n",
        "â”‚   â”‚\n",
        "â”‚   â”œâ”€â”€ analytics/\n",
        "â”‚   â”‚   â”œâ”€â”€ __init__.py\n",
        "â”‚   â”‚   â”œâ”€â”€ engine.py               # Main analytics orchestrator\n",
        "â”‚   â”‚   â”œâ”€â”€ data_loader.py          # Load data from various sources\n",
        "â”‚   â”‚   â”œâ”€â”€ feature_extractor.py    # Extract ML features\n",
        "â”‚   â”‚   â”œâ”€â”€ risk_calculator.py      # Calculate risk scores\n",
        "â”‚   â”‚   â”œâ”€â”€ core_dataframe.py       # DataFrame wrapper\n",
        "â”‚   â”‚   â””â”€â”€ model_library/\n",
        "â”‚   â”‚       â”œâ”€â”€ __init__.py\n",
        "â”‚   â”‚       â”œâ”€â”€ base_model.py       # Abstract base for all models\n",
        "â”‚   â”‚       â”œâ”€â”€ login_anomaly.py    # Login behavior model\n",
        "â”‚   â”‚       â”œâ”€â”€ access_anomaly.py   # Data access model\n",
        "â”‚   â”‚       â”œâ”€â”€ transfer_anomaly.py # Data transfer model\n",
        "â”‚   â”‚       â””â”€â”€ time_anomaly.py     # Time-based model\n",
        "â”‚   â”‚\n",
        "â”‚   â”œâ”€â”€ utils/\n",
        "â”‚   â”‚   â”œâ”€â”€ __init__.py\n",
        "â”‚   â”‚   â”œâ”€â”€ hash_utils.py           # SHA-256 hashing\n",
        "â”‚   â”‚   â”œâ”€â”€ encode_utils.py         # Base64 encoding\n",
        "â”‚   â”‚   â”œâ”€â”€ timestamp.py            # Time utilities\n",
        "â”‚   â”‚   â””â”€â”€ model_verification.py   # Model integrity checks\n",
        "â”‚   â”‚\n",
        "â”‚   â”œâ”€â”€ middleware/\n",
        "â”‚   â”‚   â”œâ”€â”€ __init__.py\n",
        "â”‚   â”‚   â””â”€â”€ auth_middleware.py      # Authentication\n",
        "â”‚   â”‚\n",
        "â”‚   â””â”€â”€ tests/\n",
        "â”‚       â”œâ”€â”€ __init__.py\n",
        "â”‚       â”œâ”€â”€ test_utils.py\n",
        "â”‚       â”œâ”€â”€ test_connectors.py\n",
        "â”‚       â”œâ”€â”€ test_services.py\n",
        "â”‚       â”œâ”€â”€ test_models.py\n",
        "â”‚       â””â”€â”€ test_analytics.py\n",
        "â”‚\n",
        "â”œâ”€â”€ alembic/                        # Database migrations\n",
        "â”‚   â”œâ”€â”€ versions/\n",
        "â”‚   â””â”€â”€ env.py\n",
        "â”‚\n",
        "â”œâ”€â”€ docs/\n",
        "â”‚   â””â”€â”€ TRAINING_DATA_STRATEGY.md\n",
        "â”‚\n",
        "â”œâ”€â”€ requirements.txt\n",
        "â”œâ”€â”€ alembic.ini\n",
        "â”œâ”€â”€ .env                            # Environment variables\n",
        "â”œâ”€â”€ .gitignore\n",
        "â””â”€â”€ README.md\n",
        "```\n",
        "\n",
        "### How to Do It:\n",
        "\n",
        "**Task 1.2.1: Create All __init__.py Files**\n",
        "- Every folder needs an `__init__.py` file\n",
        "- This makes Python recognize it as a package\n",
        "- Start with empty files, add imports later\n",
        "\n",
        "**Task 1.2.2: Create Base Python Files**\n",
        "- Convert each `.info` file to a `.py` file\n",
        "- Add proper docstrings and class/function stubs\n",
        "- Do NOT write implementation yet - just structure\n",
        "\n",
        "**Task 1.2.3: Create Configuration Files**\n",
        "- Create `.env` file for environment variables\n",
        "- Create `requirements.txt` with all dependencies\n",
        "- Create `.gitignore` to exclude unnecessary files\n",
        "\n",
        "### Verification Checklist:\n",
        "- [ ] All directories created\n",
        "- [ ] All `__init__.py` files in place\n",
        "- [ ] Base `.py` files created (empty stubs)\n",
        "- [ ] Can run `python -c \"import uba\"` without errors\n",
        "\n",
        "---\n",
        "\n",
        "## Step 1.3: Database Models & Migrations\n",
        "\n",
        "### What You Will Create:\n",
        "SQLAlchemy models for UBA-specific tables\n",
        "\n",
        "### UBA Database Tables to Create:\n",
        "\n",
        "| Table Name | Purpose | Key Fields |\n",
        "|------------|---------|------------|\n",
        "| `uba_user_profiles` | Store user behavior baselines | user_id, baseline_data, risk_score, last_updated |\n",
        "| `uba_risk_scores` | Historical risk score tracking | user_id, score, components, calculated_at |\n",
        "| `uba_anomalies` | Detected anomalies | user_id, anomaly_type, score, details, detected_at |\n",
        "| `uba_alerts` | Generated alerts | anomaly_id, severity, status, created_at |\n",
        "| `uba_cases` | Investigation cases | alert_ids, status, assignee, created_at |\n",
        "| `uba_rules` | Detection rules | rule_name, conditions, severity, enabled |\n",
        "| `uba_model_registry` | ML model metadata | model_name, version, path, status, metrics |\n",
        "| `uba_model_runs` | Model execution logs | model_id, started_at, completed_at, results |\n",
        "| `uba_feature_cache` | Cached extracted features | user_id, feature_type, features, extracted_at |\n",
        "\n",
        "### How to Do It:\n",
        "\n",
        "**Task 1.3.1: Create SQLAlchemy Models**\n",
        "- Define each table as a Python class\n",
        "- Use same patterns as DSPM models (you analyzed these)\n",
        "- Include proper indexes and relationships\n",
        "\n",
        "**Task 1.3.2: Set Up Alembic for Migrations**\n",
        "- Initialize Alembic in your project\n",
        "- Configure it to connect to PostgreSQL\n",
        "- This allows database schema versioning\n",
        "\n",
        "**Task 1.3.3: Create Initial Migration**\n",
        "- Generate migration from your models\n",
        "- Review the generated SQL\n",
        "- Apply migration to create tables\n",
        "\n",
        "**Task 1.3.4: Verify Tables Created**\n",
        "- Connect to PostgreSQL\n",
        "- Check that all tables exist\n",
        "- Verify columns and indexes are correct\n",
        "\n",
        "### Verification Checklist:\n",
        "- [ ] All SQLAlchemy models defined\n",
        "- [ ] Alembic configured correctly\n",
        "- [ ] Migration file generated\n",
        "- [ ] Tables created in PostgreSQL\n",
        "- [ ] Can query empty tables without errors\n",
        "\n",
        "---\n",
        "\n",
        "## ðŸ“ END OF PHASE 1\n",
        "\n",
        "### What You Have After Phase 1:\n",
        "1. âœ… Working development environment\n",
        "2. âœ… Complete project structure with all files\n",
        "3. âœ… Database tables created and ready\n",
        "4. âœ… Can run the application (empty, but no errors)\n",
        "\n",
        "### How to Test Phase 1 is Complete:\n",
        "1. Run `python -c \"from uba import main\"` - should not error\n",
        "2. Run `alembic current` - should show migration version\n",
        "3. Connect to PostgreSQL and see UBA tables"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "af805d11",
      "metadata": {
        "id": "af805d11"
      },
      "source": [
        "---\n",
        "\n",
        "# ðŸ”§ PHASE 2: CORE UTILITIES\n",
        "\n",
        "## Goal: Implement reusable utility functions (directly from OpenUBA)\n",
        "\n",
        "---\n",
        "\n",
        "## Step 2.1: Hash Utilities\n",
        "\n",
        "### What This Does:\n",
        "Generates SHA-256 cryptographic hashes for data integrity verification.\n",
        "Used for: Model verification, file integrity, data fingerprinting.\n",
        "\n",
        "### OpenUBA Reference:\n",
        "File: `core/hash.py` (87 lines - 100% reusable)\n",
        "\n",
        "### Implementation Tasks:\n",
        "\n",
        "**Task 2.1.1: Create Hash Base Class**\n",
        "- Location: `uba/utils/hash_utils.py`\n",
        "- Create a base `Hash` class with common functionality\n",
        "- Method: `compute()` - generates SHA-256 hash\n",
        "\n",
        "**Task 2.1.2: Create HashData Class**\n",
        "- Inherits from Hash base class\n",
        "- Takes raw bytes/string data as input\n",
        "- Returns hex-encoded hash string\n",
        "- Example: `HashData(b\"hello\").result` â†’ `\"2cf24dba5fb0a30e...\"`\n",
        "\n",
        "**Task 2.1.3: Create HashFile Class**\n",
        "- Inherits from Hash base class\n",
        "- Takes file path as input\n",
        "- Reads file and computes hash\n",
        "- Example: `HashFile(\"model.pkl\").result` â†’ `\"a5b9f...\"`\n",
        "\n",
        "**Task 2.1.4: Write Unit Tests**\n",
        "- Test with known input/output pairs\n",
        "- Test with various data types\n",
        "- Test file hashing with temp files\n",
        "\n",
        "### Verification Checklist:\n",
        "- [ ] HashData produces correct SHA-256 output\n",
        "- [ ] HashFile correctly hashes files\n",
        "- [ ] All unit tests pass\n",
        "- [ ] Edge cases handled (empty data, missing file)\n",
        "\n",
        "---\n",
        "\n",
        "## Step 2.2: Encoding Utilities\n",
        "\n",
        "### What This Does:\n",
        "Base64 encoding/decoding for data serialization.\n",
        "Used for: Model payload encoding, API data transfer, file encoding.\n",
        "\n",
        "### OpenUBA Reference:\n",
        "File: `core/encode.py` (84 lines - 100% reusable)\n",
        "\n",
        "### Implementation Tasks:\n",
        "\n",
        "**Task 2.2.1: Create Base64 Class**\n",
        "- Location: `uba/utils/encode_utils.py`\n",
        "- Method: `encode()` - convert bytes to base64 string\n",
        "- Method: `decode()` - convert base64 string to bytes\n",
        "\n",
        "**Task 2.2.2: Create B64EncodeFile Function**\n",
        "- Takes file path as input\n",
        "- Reads file and returns base64-encoded content\n",
        "- Used for model serialization\n",
        "\n",
        "**Task 2.2.3: Create B64DecodeFile Function**\n",
        "- Takes base64 string and output path\n",
        "- Decodes and writes to file\n",
        "- Used for model deserialization\n",
        "\n",
        "**Task 2.2.4: Write Unit Tests**\n",
        "- Test encode/decode round-trip\n",
        "- Test with binary files (images, pickles)\n",
        "- Test with text files\n",
        "\n",
        "### Verification Checklist:\n",
        "- [ ] Encode/decode round-trip produces original data\n",
        "- [ ] File operations work correctly\n",
        "- [ ] Binary data handled properly\n",
        "- [ ] All unit tests pass\n",
        "\n",
        "---\n",
        "\n",
        "## Step 2.3: Timestamp Utilities\n",
        "\n",
        "### What This Does:\n",
        "Standardized timestamp generation and formatting.\n",
        "Used for: Logging, audit trails, data timestamps.\n",
        "\n",
        "### OpenUBA Reference:\n",
        "File: `core/utility.py` (34 lines - Timestamp class)\n",
        "\n",
        "### Implementation Tasks:\n",
        "\n",
        "**Task 2.3.1: Create Timestamp Class**\n",
        "- Location: `uba/utils/timestamp.py`\n",
        "- Property: `readable` - human-readable format (e.g., \"2026-01-07 14:30:00\")\n",
        "- Property: `epoch` - Unix timestamp (seconds since 1970)\n",
        "- Property: `iso` - ISO 8601 format\n",
        "\n",
        "**Task 2.3.2: Add Timezone Support**\n",
        "- Default to UTC for consistency\n",
        "- Method to convert to local timezone\n",
        "- Store timezone info with timestamps\n",
        "\n",
        "**Task 2.3.3: Add Time Difference Calculation**\n",
        "- Method: `diff_seconds(other_timestamp)` - difference in seconds\n",
        "- Method: `diff_hours(other_timestamp)` - difference in hours\n",
        "- Used for session duration calculation\n",
        "\n",
        "**Task 2.3.4: Write Unit Tests**\n",
        "- Test all format outputs\n",
        "- Test timezone conversions\n",
        "- Test time difference calculations\n",
        "\n",
        "### Verification Checklist:\n",
        "- [ ] All timestamp formats correct\n",
        "- [ ] Timezone handling works\n",
        "- [ ] Time differences calculated correctly\n",
        "- [ ] All unit tests pass\n",
        "\n",
        "---\n",
        "\n",
        "## Step 2.4: Configuration System\n",
        "\n",
        "### What This Does:\n",
        "Centralized configuration management using environment variables.\n",
        "Used for: Database connections, API keys, feature flags.\n",
        "\n",
        "### Implementation Tasks:\n",
        "\n",
        "**Task 2.4.1: Create Settings Class**\n",
        "- Location: `uba/config/settings.py`\n",
        "- Use Pydantic `BaseSettings` for automatic env loading\n",
        "- Define all configuration parameters with defaults\n",
        "\n",
        "**Task 2.4.2: Define Configuration Parameters**\n",
        "\n",
        "| Parameter | Type | Default | Description |\n",
        "|-----------|------|---------|-------------|\n",
        "| `database_url` | str | - | PostgreSQL connection string |\n",
        "| `dspm_database_url` | str | - | DSPM database connection |\n",
        "| `debug` | bool | False | Enable debug mode |\n",
        "| `log_level` | str | \"INFO\" | Logging level |\n",
        "| `model_path` | str | \"./models\" | Where to store ML models |\n",
        "| `baseline_days` | int | 30 | Days for behavior baseline |\n",
        "| `anomaly_threshold` | float | 0.05 | Anomaly detection threshold |\n",
        "| `risk_weights` | dict | {...} | Risk score component weights |\n",
        "\n",
        "**Task 2.4.3: Create Constants File**\n",
        "- Location: `uba/config/constants.py`\n",
        "- Define enums for fixed values\n",
        "- Define severity levels, status types, etc.\n",
        "\n",
        "**Task 2.4.4: Create .env File Template**\n",
        "- Document all environment variables\n",
        "- Provide example values\n",
        "- Include comments explaining each\n",
        "\n",
        "### Verification Checklist:\n",
        "- [ ] Settings load from environment\n",
        "- [ ] Defaults work when env not set\n",
        "- [ ] All constants defined as enums\n",
        "- [ ] Configuration accessible throughout app\n",
        "\n",
        "---\n",
        "\n",
        "# ðŸ”§ PHASE 3: DATA LAYER\n",
        "\n",
        "## Goal: Build the data extraction and loading infrastructure\n",
        "\n",
        "---\n",
        "\n",
        "## Step 3.1: DSPM Database Connector\n",
        "\n",
        "### What This Does:\n",
        "Connects to the DSPM PostgreSQL database to extract behavioral data.\n",
        "This is your PRIMARY data source for UBA training and analysis.\n",
        "\n",
        "### DSPM Tables to Connect To:\n",
        "| Table | Data Available |\n",
        "|-------|---------------|\n",
        "| `user_sessions` | Login times, IPs, devices, locations |\n",
        "| `policy_audit_log` | User actions, policy events, violations |\n",
        "| `policy_violations` | Labeled anomalies (GOLD for ML!) |\n",
        "| `access_controls` | User-asset permission mappings |\n",
        "| `identity_mapping` | Over-privileged/stale access labels |\n",
        "| `audit_logs_trail` | All user actions with timestamps |\n",
        "| `file_metadata` | File access patterns |\n",
        "\n",
        "### Implementation Tasks:\n",
        "\n",
        "**Task 3.1.1: Create DSPMConnector Class**\n",
        "- Location: `uba/connectors/dspm_connector.py`\n",
        "- Initialize with database connection string\n",
        "- Use SQLAlchemy for database operations\n",
        "- Handle connection pooling\n",
        "\n",
        "**Task 3.1.2: Implement User Session Extraction**\n",
        "- Method: `get_user_sessions(user_id, days=30)` â†’ DataFrame\n",
        "- Extracts: login times, IPs, devices, locations\n",
        "- Returns pandas DataFrame for analysis\n",
        "\n",
        "**Task 3.1.3: Implement Policy Event Extraction**\n",
        "- Method: `get_policy_events(days=30)` â†’ DataFrame\n",
        "- Extracts: all policy audit log entries\n",
        "- Includes violation events\n",
        "\n",
        "**Task 3.1.4: Implement Access Control Extraction**\n",
        "- Method: `get_access_controls(user_id=None)` â†’ DataFrame\n",
        "- Extracts: user-asset permission mappings\n",
        "- Optional filter by user\n",
        "\n",
        "**Task 3.1.5: Implement Labeled Anomaly Extraction**\n",
        "- Method: `get_labeled_anomalies()` â†’ DataFrame\n",
        "- Sources: policy_violations + identity_mapping\n",
        "- These are your ML training labels!\n",
        "\n",
        "**Task 3.1.6: Create Bulk Extraction Methods**\n",
        "- Method: `get_all_behavioral_data(days=30)` â†’ dict of DataFrames\n",
        "- Efficiently extracts all needed data in one call\n",
        "- Used for batch processing\n",
        "\n",
        "**Task 3.1.7: Write Integration Tests**\n",
        "- Test connection to DSPM database\n",
        "- Test each extraction method\n",
        "- Verify data types and shapes\n",
        "\n",
        "### Verification Checklist:\n",
        "- [ ] Can connect to DSPM PostgreSQL\n",
        "- [ ] All extraction methods return valid DataFrames\n",
        "- [ ] Column names match expected schema\n",
        "- [ ] Date filtering works correctly\n",
        "- [ ] Integration tests pass\n",
        "\n",
        "---\n",
        "\n",
        "## Step 3.2: CSV Connector\n",
        "\n",
        "### What This Does:\n",
        "Loads data from CSV files (for testing, external log ingestion).\n",
        "Based on OpenUBA's dataset.py patterns.\n",
        "\n",
        "### OpenUBA Reference:\n",
        "File: `core/dataset.py` - CSV class\n",
        "\n",
        "### Implementation Tasks:\n",
        "\n",
        "**Task 3.2.1: Create CSVConnector Class**\n",
        "- Location: `uba/connectors/csv_connector.py`\n",
        "- Method: `load(file_path, delimiter, header)` â†’ CoreDataFrame\n",
        "- Handle various delimiters (comma, tab, space)\n",
        "\n",
        "**Task 3.2.2: Add Schema Validation**\n",
        "- Validate loaded data against expected schema\n",
        "- Report missing or extra columns\n",
        "- Handle data type conversions\n",
        "\n",
        "**Task 3.2.3: Add Batch Loading**\n",
        "- Method: `load_directory(folder_path)` â†’ list of CoreDataFrames\n",
        "- Load all CSV files from a folder\n",
        "- Optionally concatenate into single DataFrame\n",
        "\n",
        "**Task 3.2.4: Write Unit Tests**\n",
        "- Test with various CSV formats\n",
        "- Test delimiter handling\n",
        "- Test with malformed files (error handling)\n",
        "\n",
        "### Verification Checklist:\n",
        "- [ ] Loads CSV files correctly\n",
        "- [ ] Handles different delimiters\n",
        "- [ ] Schema validation works\n",
        "- [ ] Batch loading works\n",
        "- [ ] Error handling for bad files\n",
        "\n",
        "---\n",
        "\n",
        "## Step 3.3: Parquet Connector\n",
        "\n",
        "### What This Does:\n",
        "Loads data from Parquet files (efficient columnar format for big data).\n",
        "\n",
        "### Implementation Tasks:\n",
        "\n",
        "**Task 3.3.1: Create ParquetConnector Class**\n",
        "- Location: `uba/connectors/parquet_connector.py`\n",
        "- Method: `load(file_path)` â†’ CoreDataFrame\n",
        "- Use pandas or pyarrow for reading\n",
        "\n",
        "**Task 3.3.2: Add Column Selection**\n",
        "- Method: `load(file_path, columns=[...])` - load only needed columns\n",
        "- More efficient for large files\n",
        "\n",
        "**Task 3.3.3: Add Partition Support**\n",
        "- Handle partitioned Parquet datasets\n",
        "- Common in data lake scenarios\n",
        "\n",
        "**Task 3.3.4: Write Unit Tests**\n",
        "- Test basic Parquet loading\n",
        "- Test column selection\n",
        "- Test with partitioned data\n",
        "\n",
        "### Verification Checklist:\n",
        "- [ ] Loads Parquet files correctly\n",
        "- [ ] Column selection works\n",
        "- [ ] Partitioned data handled\n",
        "- [ ] Memory efficient for large files\n",
        "\n",
        "---\n",
        "\n",
        "## Step 3.4: CoreDataFrame Implementation\n",
        "\n",
        "### What This Does:\n",
        "Wrapper around pandas DataFrame providing UBA-specific operations.\n",
        "Based on OpenUBA's CoreDataFrame concept.\n",
        "\n",
        "### OpenUBA Reference:\n",
        "File: `core/dataset.py` - CoreDataFrame class\n",
        "\n",
        "### Implementation Tasks:\n",
        "\n",
        "**Task 3.4.1: Create CoreDataFrame Class**\n",
        "- Location: `uba/analytics/core_dataframe.py`\n",
        "- Wraps pandas DataFrame\n",
        "- Property: `data` - access underlying DataFrame\n",
        "- Property: `shape` - get (rows, columns)\n",
        "\n",
        "**Task 3.4.2: Add User ID Operations**\n",
        "- Method: `get_unique_users(id_column)` â†’ list\n",
        "- Method: `filter_by_user(user_id, id_column)` â†’ CoreDataFrame\n",
        "- Essential for per-user analysis\n",
        "\n",
        "**Task 3.4.3: Add Time Operations**\n",
        "- Method: `filter_by_date_range(start, end, date_column)` â†’ CoreDataFrame\n",
        "- Method: `group_by_day(date_column)` â†’ dict of CoreDataFrames\n",
        "- Essential for temporal analysis\n",
        "\n",
        "**Task 3.4.4: Add Statistics Methods**\n",
        "- Method: `describe()` â†’ summary statistics\n",
        "- Method: `null_report()` â†’ null value counts per column\n",
        "- Useful for data quality checks\n",
        "\n",
        "**Task 3.4.5: Write Unit Tests**\n",
        "- Test all methods with sample data\n",
        "- Test edge cases (empty DataFrame, missing columns)\n",
        "\n",
        "### Verification Checklist:\n",
        "- [ ] Wraps pandas DataFrame correctly\n",
        "- [ ] User operations work\n",
        "- [ ] Time operations work\n",
        "- [ ] Statistics methods work\n",
        "- [ ] All unit tests pass\n",
        "\n",
        "---\n",
        "\n",
        "## Step 3.5: Data Loader Service\n",
        "\n",
        "### What This Does:\n",
        "Orchestrates data loading from various sources.\n",
        "Main interface for analytics engine to get data.\n",
        "\n",
        "### OpenUBA Reference:\n",
        "File: `core/model.py` - ModelDataLoader enum and loading logic\n",
        "\n",
        "### Implementation Tasks:\n",
        "\n",
        "**Task 3.5.1: Create DataLoader Class**\n",
        "- Location: `uba/analytics/data_loader.py`\n",
        "- Method: `load(source_type, source_config)` â†’ CoreDataFrame\n",
        "- Delegates to appropriate connector\n",
        "\n",
        "**Task 3.5.2: Define Data Source Types**\n",
        "- Enum: DSPM_POSTGRES, LOCAL_CSV, LOCAL_PARQUET, ELASTICSEARCH\n",
        "- Each type maps to a connector\n",
        "\n",
        "**Task 3.5.3: Create Data Source Configuration**\n",
        "- Define config schema for each source type\n",
        "- Validate configuration before loading\n",
        "\n",
        "**Task 3.5.4: Add Caching Layer**\n",
        "- Cache loaded data to avoid repeated queries\n",
        "- Time-based cache expiration\n",
        "- Memory-aware caching\n",
        "\n",
        "**Task 3.5.5: Write Integration Tests**\n",
        "- Test loading from each source type\n",
        "- Test caching behavior\n",
        "- Test error handling\n",
        "\n",
        "### Verification Checklist:\n",
        "- [ ] Can load from all source types\n",
        "- [ ] Configuration validation works\n",
        "- [ ] Caching reduces repeated loads\n",
        "- [ ] Error handling is robust\n",
        "\n",
        "---\n",
        "\n",
        "# ðŸ”§ PHASE 4: USER & ENTITY MANAGEMENT\n",
        "\n",
        "## Goal: Implement user profile and behavior baseline tracking\n",
        "\n",
        "---\n",
        "\n",
        "## Step 4.1: User Profile Service\n",
        "\n",
        "### What This Does:\n",
        "Manages user profiles including behavior baselines and risk scores.\n",
        "Core of the UBA system - tracks WHO we're analyzing.\n",
        "\n",
        "### OpenUBA Reference:\n",
        "File: `core/user.py` - User, UserSet, ExtractAllUsersCSV\n",
        "\n",
        "### Implementation Tasks:\n",
        "\n",
        "**Task 4.1.1: Create UserProfile Model**\n",
        "- This should already exist in uba_models.py from Phase 1\n",
        "- Fields: user_id, first_seen, last_seen, baseline_data, risk_score\n",
        "\n",
        "**Task 4.1.2: Create ProfileService Class**\n",
        "- Location: `uba/services/profile_service.py`\n",
        "- Initialize with database session\n",
        "- Manages all user profile operations\n",
        "\n",
        "**Task 4.1.3: Implement User Discovery**\n",
        "- Method: `discover_users_from_dspm()` â†’ list of user_ids\n",
        "- Queries DSPM user_sessions table\n",
        "- Returns all unique users\n",
        "\n",
        "**Task 4.1.4: Implement Profile Creation**\n",
        "- Method: `create_profile(user_id)` â†’ UserProfile\n",
        "- Creates new profile with empty baseline\n",
        "- Sets first_seen timestamp\n",
        "\n",
        "**Task 4.1.5: Implement Profile Retrieval**\n",
        "- Method: `get_profile(user_id)` â†’ UserProfile or None\n",
        "- Method: `get_all_profiles()` â†’ list of UserProfiles\n",
        "- Method: `get_profiles_by_risk(min_score)` â†’ list of UserProfiles\n",
        "\n",
        "**Task 4.1.6: Implement Profile Update**\n",
        "- Method: `update_baseline(user_id, baseline_data)`\n",
        "- Method: `update_risk_score(user_id, score, components)`\n",
        "- Both update last_seen timestamp\n",
        "\n",
        "**Task 4.1.7: Write Unit Tests**\n",
        "- Test CRUD operations\n",
        "- Test with mock database\n",
        "- Test concurrent updates\n",
        "\n",
        "### Verification Checklist:\n",
        "- [ ] User discovery finds all DSPM users\n",
        "- [ ] Profile CRUD operations work\n",
        "- [ ] Baseline data stored correctly as JSON\n",
        "- [ ] Risk score updates tracked\n",
        "- [ ] All unit tests pass\n",
        "\n",
        "---\n",
        "\n",
        "## Step 4.2: Behavior Baseline Storage\n",
        "\n",
        "### What This Does:\n",
        "Stores and manages behavioral baselines for each user.\n",
        "Baselines are the \"normal\" patterns we compare against.\n",
        "\n",
        "### Implementation Tasks:\n",
        "\n",
        "**Task 4.2.1: Define Baseline Data Structure**\n",
        "```\n",
        "baseline_data = {\n",
        "    \"login\": {\n",
        "        \"typical_hours\": [9, 10, 11, 12, 13, 14, 15, 16, 17],\n",
        "        \"typical_ips\": [\"192.168.1.x\", \"10.0.0.x\"],\n",
        "        \"typical_devices\": [\"Windows-Chrome\", \"MacOS-Safari\"],\n",
        "        \"avg_sessions_per_day\": 2.5,\n",
        "        \"std_sessions_per_day\": 1.2\n",
        "    },\n",
        "    \"access\": {\n",
        "        \"typical_assets\": [\"asset_1\", \"asset_2\", \"asset_3\"],\n",
        "        \"avg_assets_per_day\": 5,\n",
        "        \"typical_roles\": [\"viewer\", \"editor\"]\n",
        "    },\n",
        "    \"data\": {\n",
        "        \"avg_files_accessed\": 10,\n",
        "        \"avg_data_volume_mb\": 50,\n",
        "        \"typical_file_types\": [\".pdf\", \".xlsx\", \".docx\"]\n",
        "    },\n",
        "    \"computed_at\": \"2026-01-07T12:00:00Z\",\n",
        "    \"window_days\": 30\n",
        "}\n",
        "```\n",
        "\n",
        "**Task 4.2.2: Create BaselineManager Class**\n",
        "- Location: Part of `profile_service.py` or separate file\n",
        "- Method: `compute_baseline(user_id, days=30)` â†’ baseline_data\n",
        "- Uses feature extraction (Phase 5) to compute\n",
        "\n",
        "**Task 4.2.3: Implement Baseline Comparison**\n",
        "- Method: `compare_to_baseline(user_id, current_features)` â†’ deviations\n",
        "- Returns dict of how current behavior differs from baseline\n",
        "- Used by anomaly detection\n",
        "\n",
        "**Task 4.2.4: Implement Baseline Refresh**\n",
        "- Method: `refresh_baseline(user_id)` - recompute from recent data\n",
        "- Method: `refresh_all_baselines()` - batch refresh for all users\n",
        "- Should run on schedule (daily/weekly)\n",
        "\n",
        "**Task 4.2.5: Write Unit Tests**\n",
        "- Test baseline computation\n",
        "- Test comparison logic\n",
        "- Test refresh operations\n",
        "\n",
        "### Verification Checklist:\n",
        "- [ ] Baseline structure defined and documented\n",
        "- [ ] Compute creates valid baselines\n",
        "- [ ] Comparison identifies deviations\n",
        "- [ ] Refresh updates baselines correctly\n",
        "- [ ] All unit tests pass\n",
        "\n",
        "---\n",
        "\n",
        "## Step 4.3: Entity Service\n",
        "\n",
        "### What This Does:\n",
        "Manages non-user entities (assets, systems, services).\n",
        "Tracks what resources users interact with.\n",
        "\n",
        "### OpenUBA Reference:\n",
        "File: `core/entity.py` - Entity class (partial implementation)\n",
        "\n",
        "### Implementation Tasks:\n",
        "\n",
        "**Task 4.3.1: Create Entity Model**\n",
        "- Add to uba_models.py if not present\n",
        "- Fields: entity_id, entity_type, name, metadata, first_seen\n",
        "\n",
        "**Task 4.3.2: Create EntityService Class**\n",
        "- Location: `uba/services/entity_service.py`\n",
        "- Manages entity discovery and tracking\n",
        "\n",
        "**Task 4.3.3: Implement Entity Discovery**\n",
        "- Method: `discover_entities_from_dspm()` â†’ list of entities\n",
        "- Sources: assets_details, file_metadata, table_metadata tables\n",
        "- Extracts unique assets, files, tables\n",
        "\n",
        "**Task 4.3.4: Implement Entity-User Mapping**\n",
        "- Method: `get_users_for_entity(entity_id)` â†’ list of user_ids\n",
        "- Method: `get_entities_for_user(user_id)` â†’ list of entity_ids\n",
        "- Based on access_controls table\n",
        "\n",
        "**Task 4.3.5: Implement Entity Risk**\n",
        "- Method: `compute_entity_risk(entity_id)` â†’ score\n",
        "- Based on: sensitivity level, number of users, violation count\n",
        "- High-risk entities need more monitoring\n",
        "\n",
        "**Task 4.3.6: Write Unit Tests**\n",
        "- Test discovery\n",
        "- Test mappings\n",
        "- Test risk calculation\n",
        "\n",
        "### Verification Checklist:\n",
        "- [ ] Entity discovery works\n",
        "- [ ] User-entity mappings accurate\n",
        "- [ ] Entity risk calculated\n",
        "- [ ] All unit tests pass\n",
        "\n",
        "---\n",
        "\n",
        "## ðŸ“ END OF PHASE 4\n",
        "\n",
        "### What You Have After Phase 4:\n",
        "1. âœ… All utility functions (hash, encode, timestamp)\n",
        "2. âœ… Configuration system\n",
        "3. âœ… Data connectors (DSPM, CSV, Parquet)\n",
        "4. âœ… CoreDataFrame for data manipulation\n",
        "5. âœ… User profile management\n",
        "6. âœ… Behavior baseline storage\n",
        "7. âœ… Entity management\n",
        "\n",
        "### How to Test Phases 2-4 Complete:\n",
        "\n",
        "```\n",
        "# Test utilities\n",
        "from uba.utils.hash_utils import HashData\n",
        "assert len(HashData(b\"test\").result) == 64\n",
        "\n",
        "# Test DSPM connector\n",
        "from uba.connectors.dspm_connector import DSPMConnector\n",
        "conn = DSPMConnector(settings.dspm_database_url)\n",
        "sessions = conn.get_user_sessions(days=7)\n",
        "assert sessions.shape[0] > 0\n",
        "\n",
        "# Test profile service\n",
        "from uba.services.profile_service import ProfileService\n",
        "svc = ProfileService(db_session)\n",
        "users = svc.discover_users_from_dspm()\n",
        "assert len(users) > 0\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7a4397fe",
      "metadata": {
        "id": "7a4397fe"
      },
      "source": [
        "---\n",
        "\n",
        "# ðŸ§  PHASE 5: FEATURE EXTRACTION\n",
        "\n",
        "## Goal: Extract meaningful behavioral features from raw data for ML models\n",
        "\n",
        "---\n",
        "\n",
        "## What is Feature Extraction?\n",
        "\n",
        "```\n",
        "RAW DATA                          FEATURES                        ML MODEL\n",
        "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
        "â”‚ Login: 9:00 AM  â”‚              â”‚ login_hour: 9   â”‚              â”‚                 â”‚\n",
        "â”‚ Login: 9:15 AM  â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â–º   â”‚ login_count: 5  â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â–º   â”‚  Anomaly Score  â”‚\n",
        "â”‚ Login: 9:30 AM  â”‚   EXTRACT    â”‚ unique_ips: 2   â”‚   PREDICT    â”‚     0.85        â”‚\n",
        "â”‚ Login: 2:00 AM  â”‚              â”‚ hour_std: 2.1   â”‚              â”‚                 â”‚\n",
        "â”‚ Login: 9:05 AM  â”‚              â”‚ off_hours: 1    â”‚              â”‚                 â”‚\n",
        "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
        "        â–²                                â–²                                â–²\n",
        "   Raw events               Numerical features              Anomaly detection\n",
        "   from DSPM                for ML input                    output\n",
        "```\n",
        "\n",
        "Features are NUMERICAL VALUES that ML models can understand.\n",
        "Raw logs are text - models need numbers!\n",
        "\n",
        "---\n",
        "\n",
        "## Step 5.1: Feature Extractor Base Class\n",
        "\n",
        "### What This Does:\n",
        "Provides common interface for all feature extractors.\n",
        "\n",
        "### Implementation Tasks:\n",
        "\n",
        "**Task 5.1.1: Create Base FeatureExtractor Class**\n",
        "- Location: `uba/analytics/feature_extractor.py`\n",
        "- Abstract base class that all extractors inherit from\n",
        "- Defines common interface\n",
        "\n",
        "**Task 5.1.2: Define Common Methods**\n",
        "- Method: `extract(user_id, data)` â†’ dict of features (ABSTRACT)\n",
        "- Method: `get_feature_names()` â†’ list of feature names\n",
        "- Method: `validate_features(features)` â†’ bool\n",
        "\n",
        "**Task 5.1.3: Add Feature Normalization**\n",
        "- Method: `normalize(features)` â†’ normalized features\n",
        "- Scales features to 0-1 range\n",
        "- Important for ML models to work correctly\n",
        "\n",
        "**Task 5.1.4: Add Feature Storage**\n",
        "- Method: `cache_features(user_id, features)` - save to database\n",
        "- Method: `get_cached_features(user_id)` - retrieve from database\n",
        "- Avoids recomputing same features\n",
        "\n",
        "### Verification Checklist:\n",
        "- [ ] Base class defined with abstract methods\n",
        "- [ ] Normalization produces 0-1 range values\n",
        "- [ ] Feature caching works\n",
        "- [ ] Subclasses can inherit properly\n",
        "\n",
        "---\n",
        "\n",
        "## Step 5.2: Login Feature Extractor\n",
        "\n",
        "### What This Does:\n",
        "Extracts behavioral features from user login patterns.\n",
        "Detects: unusual login times, new locations, new devices.\n",
        "\n",
        "### Data Source:\n",
        "DSPM table: `user_sessions`\n",
        "\n",
        "### Features to Extract:\n",
        "\n",
        "| Feature Name | Formula | What It Detects |\n",
        "|--------------|---------|-----------------|\n",
        "| `login_count_daily` | COUNT(logins) / days | Activity level |\n",
        "| `login_hour_mean` | AVG(hour of login) | Typical work hours |\n",
        "| `login_hour_std` | STDDEV(hour of login) | Schedule consistency |\n",
        "| `unique_ips_30d` | COUNT(DISTINCT ip_address) | Location variety |\n",
        "| `unique_devices_30d` | COUNT(DISTINCT device_info) | Device variety |\n",
        "| `unique_locations_30d` | COUNT(DISTINCT location) | Travel patterns |\n",
        "| `off_hours_ratio` | logins_outside_9to5 / total | After-hours access |\n",
        "| `weekend_ratio` | weekend_logins / total | Weekend activity |\n",
        "| `session_duration_mean` | AVG(session_length) | Work duration |\n",
        "| `session_duration_std` | STDDEV(session_length) | Duration consistency |\n",
        "| `failed_login_ratio` | failed / total_attempts | Authentication issues |\n",
        "| `new_ip_flag` | 1 if IP not in baseline | New location alert |\n",
        "| `new_device_flag` | 1 if device not in baseline | New device alert |\n",
        "| `location_entropy` | -Î£(p * log(p)) | Location randomness |\n",
        "\n",
        "### Implementation Tasks:\n",
        "\n",
        "**Task 5.2.1: Create LoginFeatureExtractor Class**\n",
        "- Location: `uba/analytics/feature_extractor.py` (or separate file)\n",
        "- Inherits from base FeatureExtractor\n",
        "- Takes user_sessions DataFrame as input\n",
        "\n",
        "**Task 5.2.2: Implement Time-based Features**\n",
        "- Extract hour from login timestamps\n",
        "- Calculate mean, std, off-hours ratio\n",
        "- Handle timezone conversions\n",
        "\n",
        "**Task 5.2.3: Implement Device/Location Features**\n",
        "- Count unique IPs, devices, locations\n",
        "- Calculate entropy for randomness\n",
        "- Compare against baseline for flags\n",
        "\n",
        "**Task 5.2.4: Implement Session Features**\n",
        "- Calculate session durations\n",
        "- Compute mean and std\n",
        "- Identify unusually long/short sessions\n",
        "\n",
        "**Task 5.2.5: Write Unit Tests**\n",
        "- Test with known data, verify calculations\n",
        "- Test edge cases (single login, no data)\n",
        "- Test normalization\n",
        "\n",
        "### Verification Checklist:\n",
        "- [ ] All 14 features extracted correctly\n",
        "- [ ] Time calculations handle timezones\n",
        "- [ ] Entropy calculation correct\n",
        "- [ ] Baseline comparison works\n",
        "- [ ] All unit tests pass\n",
        "\n",
        "---\n",
        "\n",
        "## Step 5.3: Access Feature Extractor\n",
        "\n",
        "### What This Does:\n",
        "Extracts features from data access patterns.\n",
        "Detects: unusual data access, privilege escalation, new assets.\n",
        "\n",
        "### Data Source:\n",
        "DSPM tables: `access_controls`, `policy_audit_log`, `file_metadata`\n",
        "\n",
        "### Features to Extract:\n",
        "\n",
        "| Feature Name | Formula | What It Detects |\n",
        "|--------------|---------|-----------------|\n",
        "| `assets_accessed_daily` | COUNT(DISTINCT asset) / days | Access volume |\n",
        "| `unique_assets_30d` | COUNT(DISTINCT asset_name) | Asset variety |\n",
        "| `sensitive_asset_ratio` | sensitive_assets / total | Sensitive data access |\n",
        "| `new_asset_ratio` | new_assets / total | First-time access |\n",
        "| `access_hour_mean` | AVG(hour of access) | Typical access time |\n",
        "| `access_hour_std` | STDDEV(hour of access) | Time consistency |\n",
        "| `permission_level_max` | MAX(role_weight) | Highest privilege used |\n",
        "| `permission_variety` | COUNT(DISTINCT role) | Role diversity |\n",
        "| `read_write_ratio` | writes / reads | Write activity level |\n",
        "| `cross_department_ratio` | other_dept_assets / total | Cross-boundary access |\n",
        "| `policy_violation_count` | COUNT(violations) | Policy issues |\n",
        "| `access_burst_count` | sessions > 2*mean | Unusual spikes |\n",
        "\n",
        "### Implementation Tasks:\n",
        "\n",
        "**Task 5.3.1: Create AccessFeatureExtractor Class**\n",
        "- Inherits from base FeatureExtractor\n",
        "- Combines data from multiple tables\n",
        "\n",
        "**Task 5.3.2: Implement Volume Features**\n",
        "- Count assets per day\n",
        "- Calculate variety metrics\n",
        "- Track first-time access\n",
        "\n",
        "**Task 5.3.3: Implement Permission Features**\n",
        "- Map roles to numeric weights (viewer=1, editor=2, admin=3)\n",
        "- Track maximum and variety\n",
        "- Identify privilege escalation\n",
        "\n",
        "**Task 5.3.4: Implement Pattern Features**\n",
        "- Calculate read/write ratios\n",
        "- Identify cross-department access\n",
        "- Detect access bursts\n",
        "\n",
        "**Task 5.3.5: Write Unit Tests**\n",
        "- Test all 12 features\n",
        "- Test with multi-table joins\n",
        "- Test edge cases\n",
        "\n",
        "### Verification Checklist:\n",
        "- [ ] All 12 features extracted correctly\n",
        "- [ ] Multi-table join works\n",
        "- [ ] Permission weighting applied\n",
        "- [ ] Burst detection works\n",
        "- [ ] All unit tests pass\n",
        "\n",
        "---\n",
        "\n",
        "## Step 5.4: Data Transfer Feature Extractor\n",
        "\n",
        "### What This Does:\n",
        "Extracts features from data movement patterns.\n",
        "Detects: data exfiltration, unusual downloads, bulk transfers.\n",
        "\n",
        "### Data Source:\n",
        "DSPM tables: `file_metadata`, `audit_logs_trail`, `policy_audit_log`\n",
        "\n",
        "### Features to Extract:\n",
        "\n",
        "| Feature Name | Formula | What It Detects |\n",
        "|--------------|---------|-----------------|\n",
        "| `files_accessed_daily` | COUNT(files) / days | File activity |\n",
        "| `data_volume_daily_mb` | SUM(file_size) / days / 1024^2 | Data volume |\n",
        "| `data_volume_std` | STDDEV(daily_volume) | Volume consistency |\n",
        "| `large_file_count` | COUNT(files > 100MB) | Large file access |\n",
        "| `file_type_variety` | COUNT(DISTINCT extension) | File type diversity |\n",
        "| `sensitive_file_ratio` | sensitive_files / total | Sensitive data |\n",
        "| `external_transfer_count` | COUNT(external destinations) | External sends |\n",
        "| `download_upload_ratio` | downloads / uploads | Download heavy? |\n",
        "| `bulk_operation_count` | operations > 10 files/hour | Bulk activity |\n",
        "| `after_hours_transfer` | off_hours_transfers / total | Night transfers |\n",
        "| `new_file_type_flag` | 1 if new extension | New file types |\n",
        "| `volume_spike_flag` | 1 if volume > 3*baseline | Volume spike |\n",
        "\n",
        "### Implementation Tasks:\n",
        "\n",
        "**Task 5.4.1: Create DataTransferFeatureExtractor Class**\n",
        "- Inherits from base FeatureExtractor\n",
        "- Focuses on file/data movement patterns\n",
        "\n",
        "**Task 5.4.2: Implement Volume Features**\n",
        "- Calculate daily volumes\n",
        "- Track large files\n",
        "- Compute statistics\n",
        "\n",
        "**Task 5.4.3: Implement Transfer Direction Features**\n",
        "- Identify external vs internal\n",
        "- Calculate download/upload ratios\n",
        "- Track bulk operations\n",
        "\n",
        "**Task 5.4.4: Implement Anomaly Flags**\n",
        "- Compare to baseline\n",
        "- Set flags for spikes\n",
        "- Detect new patterns\n",
        "\n",
        "**Task 5.4.5: Write Unit Tests**\n",
        "- Test all 12 features\n",
        "- Test volume calculations\n",
        "- Test flag logic\n",
        "\n",
        "### Verification Checklist:\n",
        "- [ ] All 12 features extracted correctly\n",
        "- [ ] Volume calculations accurate\n",
        "- [ ] External detection works\n",
        "- [ ] Spike detection works\n",
        "- [ ] All unit tests pass\n",
        "\n",
        "---\n",
        "\n",
        "## Step 5.5: Time Feature Extractor\n",
        "\n",
        "### What This Does:\n",
        "Extracts time-based behavioral patterns.\n",
        "Detects: schedule changes, unusual timing, pattern breaks.\n",
        "\n",
        "### Data Source:\n",
        "All DSPM behavioral tables (using timestamps)\n",
        "\n",
        "### Features to Extract:\n",
        "\n",
        "| Feature Name | Formula | What It Detects |\n",
        "|--------------|---------|-----------------|\n",
        "| `activity_hours` | list of active hours | Work schedule |\n",
        "| `peak_hour` | MODE(activity_hour) | Most active time |\n",
        "| `activity_span` | MAX(hour) - MIN(hour) | Work day length |\n",
        "| `weekend_activity_ratio` | weekend / total | Weekend work |\n",
        "| `night_activity_ratio` | 10pm-6am / total | Night activity |\n",
        "| `monday_activity` | monday_count / total | Start of week |\n",
        "| `friday_activity` | friday_count / total | End of week |\n",
        "| `activity_regularity` | 1 - (std / mean) | Schedule consistency |\n",
        "| `gap_max_hours` | MAX(time between activities) | Longest break |\n",
        "| `session_time_mean` | AVG(session_duration) | Typical session |\n",
        "| `first_activity_hour` | MODE(first daily activity) | Start time |\n",
        "| `last_activity_hour` | MODE(last daily activity) | End time |\n",
        "\n",
        "### Implementation Tasks:\n",
        "\n",
        "**Task 5.5.1: Create TimeFeatureExtractor Class**\n",
        "- Inherits from base FeatureExtractor\n",
        "- Analyzes temporal patterns\n",
        "\n",
        "**Task 5.5.2: Implement Hour Distribution**\n",
        "- Calculate activity per hour\n",
        "- Find peak hours\n",
        "- Compute span\n",
        "\n",
        "**Task 5.5.3: Implement Day-of-Week Patterns**\n",
        "- Calculate per-day ratios\n",
        "- Identify weekend patterns\n",
        "- Track week patterns\n",
        "\n",
        "**Task 5.5.4: Implement Regularity Metrics**\n",
        "- Calculate activity gaps\n",
        "- Compute regularity score\n",
        "- Track schedule changes\n",
        "\n",
        "**Task 5.5.5: Write Unit Tests**\n",
        "- Test all 12 features\n",
        "- Test with various schedules\n",
        "- Test edge cases\n",
        "\n",
        "### Verification Checklist:\n",
        "- [ ] All 12 features extracted correctly\n",
        "- [ ] Hour calculations correct\n",
        "- [ ] Day-of-week patterns work\n",
        "- [ ] Regularity metric meaningful\n",
        "- [ ] All unit tests pass\n",
        "\n",
        "---\n",
        "\n",
        "## Step 5.6: Feature Pipeline\n",
        "\n",
        "### What This Does:\n",
        "Orchestrates all feature extractors into a single pipeline.\n",
        "\n",
        "### Implementation Tasks:\n",
        "\n",
        "**Task 5.6.1: Create FeaturePipeline Class**\n",
        "- Location: `uba/analytics/feature_extractor.py`\n",
        "- Combines all extractors\n",
        "- Method: `extract_all_features(user_id)` â†’ dict\n",
        "\n",
        "**Task 5.6.2: Define Feature Groups**\n",
        "```\n",
        "FEATURE_GROUPS = {\n",
        "    \"login\": LoginFeatureExtractor,\n",
        "    \"access\": AccessFeatureExtractor,\n",
        "    \"data_transfer\": DataTransferFeatureExtractor,\n",
        "    \"time\": TimeFeatureExtractor\n",
        "}\n",
        "```\n",
        "\n",
        "**Task 5.6.3: Implement Batch Extraction**\n",
        "- Method: `extract_for_all_users()` â†’ DataFrame\n",
        "- Efficiently processes all users\n",
        "- Returns features as rows\n",
        "\n",
        "**Task 5.6.4: Add Feature Selection**\n",
        "- Method: `select_features(feature_names)` - use only specific features\n",
        "- Useful for model-specific feature sets\n",
        "\n",
        "**Task 5.6.5: Write Integration Tests**\n",
        "- Test full pipeline\n",
        "- Test batch extraction\n",
        "- Test feature selection\n",
        "\n",
        "### Verification Checklist:\n",
        "- [ ] Pipeline combines all extractors\n",
        "- [ ] Batch extraction works\n",
        "- [ ] Feature selection works\n",
        "- [ ] No missing values in output\n",
        "- [ ] All integration tests pass\n",
        "\n",
        "---\n",
        "\n",
        "# ðŸ¤– PHASE 6: ML MODEL DEVELOPMENT\n",
        "\n",
        "## Goal: Build, train, validate, and deploy ML models for anomaly detection\n",
        "\n",
        "---\n",
        "\n",
        "## ML Model Development Workflow\n",
        "\n",
        "```\n",
        "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
        "â”‚                        ML MODEL DEVELOPMENT LIFECYCLE                            â”‚\n",
        "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
        "â”‚                                                                                  â”‚\n",
        "â”‚   STEP 1: DATA PREPARATION                                                       â”‚\n",
        "â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                          â”‚\n",
        "â”‚   â”‚ Raw Data    â”‚ â†’ â”‚ Features    â”‚ â†’ â”‚ Train/Test  â”‚                          â”‚\n",
        "â”‚   â”‚ from DSPM   â”‚   â”‚ Extracted   â”‚   â”‚ Split       â”‚                          â”‚\n",
        "â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                          â”‚\n",
        "â”‚                                                                                  â”‚\n",
        "â”‚   STEP 2: MODEL TRAINING                                                         â”‚\n",
        "â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                          â”‚\n",
        "â”‚   â”‚ Choose      â”‚ â†’ â”‚ Train on    â”‚ â†’ â”‚ Validate    â”‚                          â”‚\n",
        "â”‚   â”‚ Algorithm   â”‚   â”‚ Train Set   â”‚   â”‚ on Val Set  â”‚                          â”‚\n",
        "â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                          â”‚\n",
        "â”‚                                                                                  â”‚\n",
        "â”‚   STEP 3: MODEL EVALUATION                                                       â”‚\n",
        "â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                          â”‚\n",
        "â”‚   â”‚ Test on     â”‚ â†’ â”‚ Calculate   â”‚ â†’ â”‚ Compare     â”‚                          â”‚\n",
        "â”‚   â”‚ Test Set    â”‚   â”‚ Metrics     â”‚   â”‚ to Baseline â”‚                          â”‚\n",
        "â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                          â”‚\n",
        "â”‚                                                                                  â”‚\n",
        "â”‚   STEP 4: MODEL DEPLOYMENT                                                       â”‚\n",
        "â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                          â”‚\n",
        "â”‚   â”‚ Save Model  â”‚ â†’ â”‚ Register    â”‚ â†’ â”‚ Deploy to   â”‚                          â”‚\n",
        "â”‚   â”‚ to Disk     â”‚   â”‚ in Registry â”‚   â”‚ Production  â”‚                          â”‚\n",
        "â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                          â”‚\n",
        "â”‚                                                                                  â”‚\n",
        "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## Step 6.1: Base Model Interface\n",
        "\n",
        "### What This Does:\n",
        "Defines the interface all UBA models must follow.\n",
        "Ensures consistency across all model types.\n",
        "\n",
        "### OpenUBA Reference:\n",
        "File: `core/model_library/*/MODEL.py` - all have `execute()` function\n",
        "\n",
        "### Implementation Tasks:\n",
        "\n",
        "**Task 6.1.1: Create BaseModel Abstract Class**\n",
        "- Location: `uba/analytics/model_library/base_model.py`\n",
        "- All models inherit from this\n",
        "- Defines required methods\n",
        "\n",
        "**Task 6.1.2: Define Required Methods**\n",
        "```\n",
        "class BaseModel(ABC):\n",
        "    @abstractmethod\n",
        "    def train(self, X_train, y_train=None):\n",
        "        \"\"\"Train the model on data\"\"\"\n",
        "        pass\n",
        "    \n",
        "    @abstractmethod\n",
        "    def predict(self, X):\n",
        "        \"\"\"Make predictions on new data\"\"\"\n",
        "        pass\n",
        "    \n",
        "    @abstractmethod\n",
        "    def save(self, path):\n",
        "        \"\"\"Save model to disk\"\"\"\n",
        "        pass\n",
        "    \n",
        "    @abstractmethod\n",
        "    def load(self, path):\n",
        "        \"\"\"Load model from disk\"\"\"\n",
        "        pass\n",
        "    \n",
        "    @abstractmethod\n",
        "    def get_feature_names(self):\n",
        "        \"\"\"Return list of required features\"\"\"\n",
        "        pass\n",
        "```\n",
        "\n",
        "**Task 6.1.3: Add Common Methods**\n",
        "- Method: `validate_input(X)` - check features present\n",
        "- Method: `get_model_info()` - return metadata\n",
        "- Method: `compute_metrics(y_true, y_pred)` - evaluation\n",
        "\n",
        "**Task 6.1.4: Add Model Registry Integration**\n",
        "- Method: `register(registry)` - add to registry\n",
        "- Method: `get_version()` - return version string\n",
        "\n",
        "**Task 6.1.5: Write Unit Tests**\n",
        "- Test abstract methods enforced\n",
        "- Test common methods work\n",
        "- Test inheritance\n",
        "\n",
        "### Verification Checklist:\n",
        "- [ ] Abstract class defined correctly\n",
        "- [ ] All required methods documented\n",
        "- [ ] Common methods implemented\n",
        "- [ ] Registry integration works\n",
        "- [ ] Subclasses must implement abstract methods\n",
        "\n",
        "---\n",
        "\n",
        "## Step 6.2: Login Anomaly Model\n",
        "\n",
        "### What This Does:\n",
        "Detects anomalous login behavior using Isolation Forest.\n",
        "This is typically your FIRST and MOST IMPORTANT model.\n",
        "\n",
        "### Algorithm: Isolation Forest\n",
        "- Unsupervised anomaly detection\n",
        "- Works by isolating observations\n",
        "- Anomalies are easier to isolate (shorter path)\n",
        "- No labels needed (perfect for starting out)\n",
        "\n",
        "### Features Used:\n",
        "From LoginFeatureExtractor (14 features)\n",
        "\n",
        "### Implementation Tasks:\n",
        "\n",
        "**Task 6.2.1: Create LoginAnomalyModel Class**\n",
        "- Location: `uba/analytics/model_library/login_anomaly.py`\n",
        "- Inherits from BaseModel\n",
        "- Uses sklearn IsolationForest\n",
        "\n",
        "**Task 6.2.2: Define Model Parameters**\n",
        "```\n",
        "DEFAULT_PARAMS = {\n",
        "    \"n_estimators\": 100,      # Number of trees\n",
        "    \"contamination\": 0.05,    # Expected anomaly rate (5%)\n",
        "    \"max_samples\": \"auto\",    # Samples per tree\n",
        "    \"random_state\": 42        # Reproducibility\n",
        "}\n",
        "```\n",
        "\n",
        "**Task 6.2.3: Implement train() Method**\n",
        "- Accept feature DataFrame\n",
        "- Validate features present\n",
        "- Fit IsolationForest\n",
        "- Store fitted model\n",
        "\n",
        "**Task 6.2.4: Implement predict() Method**\n",
        "- Accept new feature DataFrame\n",
        "- Return anomaly scores (-1 = anomaly, 1 = normal)\n",
        "- Also return probability scores (0-1)\n",
        "\n",
        "**Task 6.2.5: Implement save()/load() Methods**\n",
        "- Use joblib for model serialization\n",
        "- Save to specified path\n",
        "- Include metadata (version, date, features)\n",
        "\n",
        "**Task 6.2.6: Write Unit Tests**\n",
        "- Test training with sample data\n",
        "- Test prediction output format\n",
        "- Test save/load round-trip\n",
        "\n",
        "### Training Process:\n",
        "\n",
        "```\n",
        "TRAINING STEPS FOR LOGIN ANOMALY MODEL:\n",
        "\n",
        "1. COLLECT DATA\n",
        "   - Query DSPM: get_user_sessions(days=90)\n",
        "   - Need minimum 1,500 sessions, 50 users\n",
        "\n",
        "2. EXTRACT FEATURES\n",
        "   - Use LoginFeatureExtractor\n",
        "   - Get 14 features per user\n",
        "   - Store as DataFrame\n",
        "\n",
        "3. PREPARE DATA\n",
        "   - Handle missing values (fill with median)\n",
        "   - Normalize features to 0-1 range\n",
        "   - No train/test split for unsupervised!\n",
        "\n",
        "4. TRAIN MODEL\n",
        "   - Create IsolationForest\n",
        "   - Fit on all feature data\n",
        "   - Model learns \"normal\" patterns\n",
        "\n",
        "5. VALIDATE MODEL\n",
        "   - Use DSPM labeled data (policy_violations)\n",
        "   - Check if known anomalies get high scores\n",
        "   - Compute precision/recall if labels available\n",
        "\n",
        "6. SAVE MODEL\n",
        "   - Serialize with joblib\n",
        "   - Store in uba/models/login_anomaly_v1.pkl\n",
        "   - Register in model registry\n",
        "```\n",
        "\n",
        "### Verification Checklist:\n",
        "- [ ] Model trains without errors\n",
        "- [ ] Predictions return -1/1 values\n",
        "- [ ] Probability scores in 0-1 range\n",
        "- [ ] Save/load preserves model\n",
        "- [ ] Performance acceptable on labeled data\n",
        "\n",
        "---\n",
        "\n",
        "## Step 6.3: Access Anomaly Model\n",
        "\n",
        "### What This Does:\n",
        "Detects anomalous data access patterns.\n",
        "Identifies: unusual data access, first-time access to sensitive data.\n",
        "\n",
        "### Algorithm Options:\n",
        "- **Isolation Forest** (recommended to start)\n",
        "- **One-Class SVM** (alternative)\n",
        "- **Autoencoder** (for complex patterns)\n",
        "\n",
        "### Features Used:\n",
        "From AccessFeatureExtractor (12 features)\n",
        "\n",
        "### Implementation Tasks:\n",
        "\n",
        "**Task 6.3.1: Create AccessAnomalyModel Class**\n",
        "- Location: `uba/analytics/model_library/access_anomaly.py`\n",
        "- Inherits from BaseModel\n",
        "- Similar structure to LoginAnomalyModel\n",
        "\n",
        "**Task 6.3.2: Implement All Required Methods**\n",
        "- train(), predict(), save(), load()\n",
        "- Same pattern as LoginAnomalyModel\n",
        "\n",
        "**Task 6.3.3: Add Access-Specific Logic**\n",
        "- Weight sensitive asset access higher\n",
        "- Consider first-time access flags\n",
        "- Incorporate policy violation history\n",
        "\n",
        "**Task 6.3.4: Write Unit Tests**\n",
        "- Test with access pattern data\n",
        "- Verify sensitive asset weighting\n",
        "- Test prediction outputs\n",
        "\n",
        "### Verification Checklist:\n",
        "- [ ] Model trains on access features\n",
        "- [ ] Sensitive assets weighted correctly\n",
        "- [ ] Predictions make sense\n",
        "- [ ] All unit tests pass\n",
        "\n",
        "---\n",
        "\n",
        "## Step 6.4: Data Transfer Anomaly Model\n",
        "\n",
        "### What This Does:\n",
        "Detects anomalous data movement (potential exfiltration).\n",
        "Identifies: bulk downloads, unusual file access, external transfers.\n",
        "\n",
        "### Algorithm: Isolation Forest + Rule-based Hybrid\n",
        "- ML model for general anomalies\n",
        "- Rules for known bad patterns (large external transfer)\n",
        "\n",
        "### Features Used:\n",
        "From DataTransferFeatureExtractor (12 features)\n",
        "\n",
        "### Implementation Tasks:\n",
        "\n",
        "**Task 6.4.1: Create TransferAnomalyModel Class**\n",
        "- Location: `uba/analytics/model_library/transfer_anomaly.py`\n",
        "- Inherits from BaseModel\n",
        "- Hybrid ML + rules approach\n",
        "\n",
        "**Task 6.4.2: Implement ML Component**\n",
        "- Isolation Forest on transfer features\n",
        "- Detects unusual patterns\n",
        "\n",
        "**Task 6.4.3: Implement Rule Component**\n",
        "- Rule: external_transfer > threshold â†’ flag\n",
        "- Rule: volume_spike_flag == 1 â†’ flag\n",
        "- Rule: after_hours_transfer > 0.5 â†’ flag\n",
        "\n",
        "**Task 6.4.4: Combine Scores**\n",
        "- ML score: 0-1 (probability)\n",
        "- Rule score: 0-1 (how many rules triggered)\n",
        "- Final: weighted combination\n",
        "\n",
        "**Task 6.4.5: Write Unit Tests**\n",
        "- Test ML component\n",
        "- Test rule triggers\n",
        "- Test combined scoring\n",
        "\n",
        "### Verification Checklist:\n",
        "- [ ] ML component works\n",
        "- [ ] Rules trigger correctly\n",
        "- [ ] Combined scoring reasonable\n",
        "- [ ] High-risk transfers flagged\n",
        "\n",
        "---\n",
        "\n",
        "## Step 6.5: Time Anomaly Model\n",
        "\n",
        "### What This Does:\n",
        "Detects schedule/timing anomalies.\n",
        "Identifies: work hour changes, unusual activity times.\n",
        "\n",
        "### Algorithm: Statistical + Isolation Forest\n",
        "- Compare current time patterns to baseline\n",
        "- Flag significant deviations\n",
        "\n",
        "### Features Used:\n",
        "From TimeFeatureExtractor (12 features)\n",
        "\n",
        "### Implementation Tasks:\n",
        "\n",
        "**Task 6.5.1: Create TimeAnomalyModel Class**\n",
        "- Location: `uba/analytics/model_library/time_anomaly.py`\n",
        "- Inherits from BaseModel\n",
        "\n",
        "**Task 6.5.2: Implement Baseline Comparison**\n",
        "- Compare current hour distribution to baseline\n",
        "- Use statistical tests (chi-square, KS test)\n",
        "- Compute deviation score\n",
        "\n",
        "**Task 6.5.3: Implement Pattern Detection**\n",
        "- Detect new work hours\n",
        "- Detect schedule shifts\n",
        "- Flag night/weekend activity increases\n",
        "\n",
        "**Task 6.5.4: Write Unit Tests**\n",
        "- Test with various schedules\n",
        "- Test deviation detection\n",
        "- Test pattern changes\n",
        "\n",
        "### Verification Checklist:\n",
        "- [ ] Schedule comparison works\n",
        "- [ ] Deviations detected\n",
        "- [ ] Pattern changes flagged\n",
        "- [ ] All unit tests pass\n",
        "\n",
        "---\n",
        "\n",
        "## Step 6.6: Model Training Pipeline\n",
        "\n",
        "### What This Does:\n",
        "Orchestrates training of all models.\n",
        "\n",
        "### Implementation Tasks:\n",
        "\n",
        "**Task 6.6.1: Create ModelTrainer Class**\n",
        "- Location: `uba/analytics/engine.py` or separate file\n",
        "- Manages training workflow\n",
        "\n",
        "**Task 6.6.2: Implement Training Workflow**\n",
        "```\n",
        "TRAINING WORKFLOW:\n",
        "\n",
        "1. Load configuration (which models to train)\n",
        "2. For each model:\n",
        "   a. Load data from DSPM\n",
        "   b. Extract features using appropriate extractor\n",
        "   c. Prepare data (normalize, handle missing)\n",
        "   d. Train model\n",
        "   e. Validate model (if labels available)\n",
        "   f. Save model to disk\n",
        "   g. Register in model registry\n",
        "3. Log training metrics\n",
        "4. Report completion\n",
        "```\n",
        "\n",
        "**Task 6.6.3: Implement Validation Logic**\n",
        "- Use DSPM labeled data (policy_violations, identity_mapping)\n",
        "- Compute: precision, recall, F1-score (if labels available)\n",
        "- For unlabeled: use contamination estimate\n",
        "\n",
        "**Task 6.6.4: Implement Model Registry**\n",
        "- Store model metadata in database (uba_model_registry table)\n",
        "- Track: model_name, version, path, trained_at, metrics\n",
        "\n",
        "**Task 6.6.5: Add Retraining Schedule**\n",
        "- Models should be retrained periodically\n",
        "- Weekly or monthly depending on data volume\n",
        "- Detect model drift (performance degradation)\n",
        "\n",
        "### Verification Checklist:\n",
        "- [ ] Training pipeline runs end-to-end\n",
        "- [ ] All models trained and saved\n",
        "- [ ] Validation metrics computed\n",
        "- [ ] Models registered in database\n",
        "- [ ] Retraining can be triggered\n",
        "\n",
        "---\n",
        "\n",
        "## Step 6.7: Model Validation & Testing\n",
        "\n",
        "### What This Does:\n",
        "Ensures models perform correctly before deployment.\n",
        "\n",
        "### Validation Types:\n",
        "\n",
        "**1. Holdout Validation**\n",
        "- Split data: 80% train, 20% test\n",
        "- Train on 80%, evaluate on 20%\n",
        "- Use for supervised models\n",
        "\n",
        "**2. Cross-Validation**\n",
        "- Split into k folds (e.g., 5)\n",
        "- Train on k-1 folds, test on 1\n",
        "- Repeat k times, average results\n",
        "\n",
        "**3. Temporal Validation**\n",
        "- Train on older data\n",
        "- Test on newer data\n",
        "- More realistic for time-series\n",
        "\n",
        "**4. Labeled Data Validation**\n",
        "- Use policy_violations as ground truth\n",
        "- Check if model flags known anomalies\n",
        "- Most important validation!\n",
        "\n",
        "### Metrics to Compute:\n",
        "\n",
        "| Metric | Formula | What It Measures |\n",
        "|--------|---------|------------------|\n",
        "| Precision | TP / (TP + FP) | How many flagged are real anomalies |\n",
        "| Recall | TP / (TP + FN) | How many anomalies we catch |\n",
        "| F1-Score | 2 * (P * R) / (P + R) | Balance of precision/recall |\n",
        "| AUC-ROC | Area under ROC curve | Overall ranking quality |\n",
        "| False Positive Rate | FP / (FP + TN) | Alert fatigue measure |\n",
        "\n",
        "### Implementation Tasks:\n",
        "\n",
        "**Task 6.7.1: Create ModelValidator Class**\n",
        "- Implements all validation types\n",
        "- Computes all metrics\n",
        "\n",
        "**Task 6.7.2: Implement Labeled Validation**\n",
        "- Query policy_violations from DSPM\n",
        "- Match to user features\n",
        "- Evaluate if model detects\n",
        "\n",
        "**Task 6.7.3: Create Validation Report**\n",
        "- Generate report with all metrics\n",
        "- Include confusion matrix\n",
        "- Add recommendations\n",
        "\n",
        "**Task 6.7.4: Define Acceptance Criteria**\n",
        "```\n",
        "MINIMUM MODEL PERFORMANCE:\n",
        "\n",
        "- Precision: > 0.3 (at least 30% of alerts are real)\n",
        "- Recall: > 0.7 (catch at least 70% of anomalies)\n",
        "- False Positive Rate: < 0.1 (less than 10% false alarms)\n",
        "\n",
        "If model fails criteria â†’ investigate and retrain\n",
        "```\n",
        "\n",
        "### Verification Checklist:\n",
        "- [ ] All validation types implemented\n",
        "- [ ] Metrics computed correctly\n",
        "- [ ] Report generation works\n",
        "- [ ] Acceptance criteria defined\n",
        "- [ ] Failing models flagged\n",
        "\n",
        "---\n",
        "\n",
        "## ðŸ“ END OF PHASES 5-6\n",
        "\n",
        "### What You Have After Phase 6:\n",
        "1. âœ… Feature extraction pipeline (4 extractors, 50+ features)\n",
        "2. âœ… 4 trained ML models (login, access, transfer, time)\n",
        "3. âœ… Model training pipeline\n",
        "4. âœ… Model validation system\n",
        "5. âœ… Model registry in database\n",
        "6. âœ… Saved model files on disk\n",
        "\n",
        "### How to Test Phases 5-6 Complete:\n",
        "\n",
        "```\n",
        "# Test feature extraction\n",
        "from uba.analytics.feature_extractor import FeaturePipeline\n",
        "pipeline = FeaturePipeline()\n",
        "features = pipeline.extract_all_features(user_id=\"test_user\")\n",
        "assert len(features) > 40  # Should have 40+ features\n",
        "\n",
        "# Test model training\n",
        "from uba.analytics.model_library.login_anomaly import LoginAnomalyModel\n",
        "model = LoginAnomalyModel()\n",
        "model.train(training_features)\n",
        "predictions = model.predict(test_features)\n",
        "assert predictions.shape[0] == test_features.shape[0]\n",
        "\n",
        "# Test model save/load\n",
        "model.save(\"models/login_anomaly_v1.pkl\")\n",
        "loaded_model = LoginAnomalyModel()\n",
        "loaded_model.load(\"models/login_anomaly_v1.pkl\")\n",
        "assert loaded_model.predict(test_features).equals(predictions)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ac27f77f",
      "metadata": {
        "id": "ac27f77f"
      },
      "source": [
        "---\n",
        "\n",
        "# âš™ï¸ PHASE 7: ANALYTICS ENGINE\n",
        "\n",
        "## Goal: Build the orchestration engine that runs ML models and calculates risk scores\n",
        "\n",
        "---\n",
        "\n",
        "## Analytics Engine Architecture\n",
        "\n",
        "```\n",
        "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
        "â”‚                           ANALYTICS ENGINE FLOW                                  â”‚\n",
        "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
        "â”‚                                                                                  â”‚\n",
        "â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                                               â”‚\n",
        "â”‚   â”‚  SCHEDULER   â”‚  â—„â”€â”€ Triggers execution (every hour, daily, etc.)            â”‚\n",
        "â”‚   â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜                                                               â”‚\n",
        "â”‚          â”‚                                                                       â”‚\n",
        "â”‚          â–¼                                                                       â”‚\n",
        "â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                                               â”‚\n",
        "â”‚   â”‚ DATA LOADER  â”‚  â—„â”€â”€ Loads data from DSPM (user_sessions, etc.)              â”‚\n",
        "â”‚   â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜                                                               â”‚\n",
        "â”‚          â”‚                                                                       â”‚\n",
        "â”‚          â–¼                                                                       â”‚\n",
        "â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                                               â”‚\n",
        "â”‚   â”‚  FEATURE     â”‚  â—„â”€â”€ Extracts features from raw data                         â”‚\n",
        "â”‚   â”‚  EXTRACTOR   â”‚                                                               â”‚\n",
        "â”‚   â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜                                                               â”‚\n",
        "â”‚          â”‚                                                                       â”‚\n",
        "â”‚          â–¼                                                                       â”‚\n",
        "â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                          â”‚\n",
        "â”‚   â”‚              MODEL EXECUTION ENGINE               â”‚                          â”‚\n",
        "â”‚   â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚                          â”‚\n",
        "â”‚   â”‚  â”‚ Login   â”‚ â”‚ Access  â”‚ â”‚Transfer â”‚ â”‚  Time   â”‚ â”‚                          â”‚\n",
        "â”‚   â”‚  â”‚ Model   â”‚ â”‚ Model   â”‚ â”‚ Model   â”‚ â”‚ Model   â”‚ â”‚                          â”‚\n",
        "â”‚   â”‚  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜ â”‚                          â”‚\n",
        "â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”˜                          â”‚\n",
        "â”‚           â”‚           â”‚           â”‚           â”‚                                  â”‚\n",
        "â”‚           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                  â”‚\n",
        "â”‚                             â”‚                                                    â”‚\n",
        "â”‚                             â–¼                                                    â”‚\n",
        "â”‚                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                              â”‚\n",
        "â”‚                    â”‚    RISK      â”‚  â—„â”€â”€ Aggregates all model scores            â”‚\n",
        "â”‚                    â”‚  CALCULATOR  â”‚                                              â”‚\n",
        "â”‚                    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜                                              â”‚\n",
        "â”‚                           â”‚                                                      â”‚\n",
        "â”‚                           â–¼                                                      â”‚\n",
        "â”‚                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                              â”‚\n",
        "â”‚                    â”‚   ANOMALY    â”‚  â—„â”€â”€ Stores detected anomalies              â”‚\n",
        "â”‚                    â”‚   SERVICE    â”‚                                              â”‚\n",
        "â”‚                    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜                                              â”‚\n",
        "â”‚                           â”‚                                                      â”‚\n",
        "â”‚                           â–¼                                                      â”‚\n",
        "â”‚                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                              â”‚\n",
        "â”‚                    â”‚    ALERT     â”‚  â—„â”€â”€ Generates alerts for high scores       â”‚\n",
        "â”‚                    â”‚   SERVICE    â”‚                                              â”‚\n",
        "â”‚                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                              â”‚\n",
        "â”‚                                                                                  â”‚\n",
        "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## Step 7.1: Model Execution Engine\n",
        "\n",
        "### What This Does:\n",
        "Loads and executes all ML models on user data.\n",
        "This is the CORE of your UBA system - runs the actual analysis.\n",
        "\n",
        "### OpenUBA Reference:\n",
        "File: `core/model.py` - ModelEngine class\n",
        "\n",
        "### Implementation Tasks:\n",
        "\n",
        "**Task 7.1.1: Create AnalyticsEngine Class**\n",
        "- Location: `uba/analytics/engine.py`\n",
        "- Main orchestrator for all analytics\n",
        "- Manages model loading, execution, and result processing\n",
        "\n",
        "**Task 7.1.2: Implement Model Loading**\n",
        "- Method: `load_models()` â†’ dict of loaded models\n",
        "- Load from model registry (database)\n",
        "- Load model files from disk\n",
        "- Cache loaded models in memory\n",
        "\n",
        "**Task 7.1.3: Implement Single User Analysis**\n",
        "- Method: `analyze_user(user_id)` â†’ dict of scores\n",
        "- Extract features for user\n",
        "- Run all models on features\n",
        "- Return individual model scores\n",
        "\n",
        "**Task 7.1.4: Implement Batch Analysis**\n",
        "- Method: `analyze_all_users()` â†’ DataFrame of scores\n",
        "- Efficiently process all users\n",
        "- Use parallel processing if possible\n",
        "- Return DataFrame with user_id and all scores\n",
        "\n",
        "**Task 7.1.5: Implement Model Result Aggregation**\n",
        "- Collect results from all models\n",
        "- Store raw scores for each model\n",
        "- Pass to risk calculator\n",
        "\n",
        "**Task 7.1.6: Add Execution Logging**\n",
        "- Log start/end times\n",
        "- Log any errors\n",
        "- Store in uba_model_runs table\n",
        "\n",
        "### Verification Checklist:\n",
        "- [ ] Models load correctly from disk\n",
        "- [ ] Single user analysis works\n",
        "- [ ] Batch analysis completes\n",
        "- [ ] Results stored in database\n",
        "- [ ] Execution logged\n",
        "\n",
        "---\n",
        "\n",
        "## Step 7.2: Risk Score Calculator\n",
        "\n",
        "### What This Does:\n",
        "Aggregates individual model scores into a single risk score per user.\n",
        "This is what security analysts will see and act upon.\n",
        "\n",
        "### OpenUBA Reference:\n",
        "File: `core/risk.py` - RiskScore class (placeholder - you implement fully)\n",
        "\n",
        "### Risk Score Formula:\n",
        "\n",
        "```\n",
        "RISK SCORE CALCULATION:\n",
        "\n",
        "Individual Model Scores (each 0-1 range):\n",
        "â”œâ”€â”€ login_score      = Login Anomaly Model output\n",
        "â”œâ”€â”€ access_score     = Access Anomaly Model output  \n",
        "â”œâ”€â”€ transfer_score   = Data Transfer Model output\n",
        "â””â”€â”€ time_score       = Time Anomaly Model output\n",
        "\n",
        "Weights (configurable, must sum to 1.0):\n",
        "â”œâ”€â”€ login_weight     = 0.25 (25%)\n",
        "â”œâ”€â”€ access_weight    = 0.30 (30%)  â—„â”€â”€ Most important for DSPM\n",
        "â”œâ”€â”€ transfer_weight  = 0.30 (30%)  â—„â”€â”€ Data exfiltration focus\n",
        "â””â”€â”€ time_weight      = 0.15 (15%)\n",
        "\n",
        "COMPOSITE RISK SCORE =\n",
        "    (login_score Ã— login_weight) +\n",
        "    (access_score Ã— access_weight) +\n",
        "    (transfer_score Ã— transfer_weight) +\n",
        "    (time_score Ã— time_weight)\n",
        "\n",
        "Final Score: 0.0 (no risk) to 1.0 (critical risk)\n",
        "\n",
        "Mapped to 0-100 for display:\n",
        "â”œâ”€â”€ 0-25:   LOW risk (green)\n",
        "â”œâ”€â”€ 26-50:  MEDIUM risk (yellow)\n",
        "â”œâ”€â”€ 51-75:  HIGH risk (orange)\n",
        "â””â”€â”€ 76-100: CRITICAL risk (red)\n",
        "```\n",
        "\n",
        "### Implementation Tasks:\n",
        "\n",
        "**Task 7.2.1: Create RiskCalculator Class**\n",
        "- Location: `uba/analytics/risk_calculator.py`\n",
        "- Accepts individual model scores\n",
        "- Produces composite risk score\n",
        "\n",
        "**Task 7.2.2: Define Risk Weights**\n",
        "- Store weights in configuration\n",
        "- Allow customization per deployment\n",
        "- Weights must sum to 1.0\n",
        "\n",
        "**Task 7.2.3: Implement Score Calculation**\n",
        "- Method: `calculate(model_scores)` â†’ composite score\n",
        "- Apply weights to each model score\n",
        "- Return 0-1 normalized score\n",
        "\n",
        "**Task 7.2.4: Implement Score Classification**\n",
        "- Method: `classify(score)` â†’ risk level (LOW/MEDIUM/HIGH/CRITICAL)\n",
        "- Map numeric score to category\n",
        "- Thresholds configurable\n",
        "\n",
        "**Task 7.2.5: Implement Historical Tracking**\n",
        "- Store each score calculation in uba_risk_scores table\n",
        "- Track: user_id, score, components, calculated_at\n",
        "- Enable trend analysis\n",
        "\n",
        "**Task 7.2.6: Add Score Explanation**\n",
        "- Method: `explain(user_id)` â†’ dict of contributing factors\n",
        "- Show which models contributed most\n",
        "- Helpful for security analysts\n",
        "\n",
        "### Verification Checklist:\n",
        "- [ ] Weights sum to 1.0\n",
        "- [ ] Composite score in 0-1 range\n",
        "- [ ] Classification works correctly\n",
        "- [ ] Scores stored in database\n",
        "- [ ] Explanation provides insights\n",
        "\n",
        "---\n",
        "\n",
        "## Step 7.3: Anomaly Aggregator\n",
        "\n",
        "### What This Does:\n",
        "Collects and stores detected anomalies for further processing.\n",
        "\n",
        "### OpenUBA Reference:\n",
        "File: `core/anomaly.py` - AnomalyJob class (placeholder)\n",
        "\n",
        "### Implementation Tasks:\n",
        "\n",
        "**Task 7.3.1: Create AnomalyService Class**\n",
        "- Location: `uba/services/anomaly_service.py`\n",
        "- Manages anomaly lifecycle\n",
        "\n",
        "**Task 7.3.2: Define Anomaly Structure**\n",
        "```\n",
        "anomaly = {\n",
        "    \"id\": \"uuid\",\n",
        "    \"user_id\": \"user123\",\n",
        "    \"anomaly_type\": \"login_anomaly\",  # Which model detected\n",
        "    \"score\": 0.85,                     # Raw model score\n",
        "    \"features\": {...},                 # Features that triggered\n",
        "    \"details\": {\n",
        "        \"reason\": \"Login from new location\",\n",
        "        \"baseline_value\": \"192.168.1.x\",\n",
        "        \"current_value\": \"45.67.89.x\"\n",
        "    },\n",
        "    \"detected_at\": \"2026-01-07T14:30:00Z\",\n",
        "    \"status\": \"new\"  # new, reviewed, false_positive, confirmed\n",
        "}\n",
        "```\n",
        "\n",
        "**Task 7.3.3: Implement Anomaly Detection Logic**\n",
        "- Method: `detect_anomalies(user_id, model_scores)` â†’ list of anomalies\n",
        "- Threshold: score > 0.5 (configurable)\n",
        "- Create anomaly record for each threshold breach\n",
        "\n",
        "**Task 7.3.4: Implement Anomaly Storage**\n",
        "- Store in uba_anomalies table\n",
        "- Include all relevant context\n",
        "- Link to user profile\n",
        "\n",
        "**Task 7.3.5: Implement Anomaly Retrieval**\n",
        "- Method: `get_anomalies(user_id=None, status=None)` â†’ list\n",
        "- Filter by user, status, date range\n",
        "- Sort by score (highest first)\n",
        "\n",
        "**Task 7.3.6: Implement Status Updates**\n",
        "- Method: `update_status(anomaly_id, status, notes)`\n",
        "- Allow marking as reviewed/false_positive/confirmed\n",
        "- Track who updated and when\n",
        "\n",
        "### Verification Checklist:\n",
        "- [ ] Anomalies detected when score > threshold\n",
        "- [ ] Stored with full context\n",
        "- [ ] Retrieval filtering works\n",
        "- [ ] Status updates tracked\n",
        "\n",
        "---\n",
        "\n",
        "## Step 7.4: Scheduler Integration\n",
        "\n",
        "### What This Does:\n",
        "Runs the analytics engine on a schedule (not just on-demand).\n",
        "\n",
        "### OpenUBA Reference:\n",
        "File: `core/core.py` - scheduler_run function\n",
        "\n",
        "### Implementation Tasks:\n",
        "\n",
        "**Task 7.4.1: Create Scheduler Configuration**\n",
        "- Define schedule for each operation\n",
        "- Example: risk calculation every hour\n",
        "- Example: model retraining weekly\n",
        "\n",
        "**Task 7.4.2: Implement Scheduled Jobs**\n",
        "\n",
        "| Job Name | Frequency | What It Does |\n",
        "|----------|-----------|--------------|\n",
        "| `analyze_all_users` | Every hour | Run all models on all users |\n",
        "| `refresh_baselines` | Daily | Update user behavior baselines |\n",
        "| `retrain_models` | Weekly | Retrain ML models with new data |\n",
        "| `cleanup_old_data` | Daily | Archive old anomalies/scores |\n",
        "\n",
        "**Task 7.4.3: Integrate with DSPM Scheduler**\n",
        "- DSPM already has a scheduler (APScheduler)\n",
        "- Add UBA jobs to existing scheduler\n",
        "- Or create separate UBA scheduler\n",
        "\n",
        "**Task 7.4.4: Add Job Monitoring**\n",
        "- Log job start/end times\n",
        "- Track success/failure\n",
        "- Alert on failures\n",
        "\n",
        "### Verification Checklist:\n",
        "- [ ] Jobs run on schedule\n",
        "- [ ] Results stored after each run\n",
        "- [ ] Failures logged and alerted\n",
        "- [ ] Can manually trigger jobs\n",
        "\n",
        "---\n",
        "\n",
        "# ðŸš¨ PHASE 8: ALERT & CASE SYSTEM\n",
        "\n",
        "## Goal: Generate alerts from anomalies and manage investigation cases\n",
        "\n",
        "---\n",
        "\n",
        "## Alert System Architecture\n",
        "\n",
        "```\n",
        "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
        "â”‚                            ALERT SYSTEM FLOW                                     â”‚\n",
        "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
        "â”‚                                                                                  â”‚\n",
        "â”‚   ANOMALIES                    ALERTS                      CASES                 â”‚\n",
        "â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”                 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”                 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”‚\n",
        "â”‚   â”‚Anomaly 1â”‚â”€â”               â”‚ Alert 1 â”‚â”€â”               â”‚         â”‚           â”‚\n",
        "â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚               â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚               â”‚  Case   â”‚           â”‚\n",
        "â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚  AGGREGATE    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚  ESCALATE     â”‚    1    â”‚           â”‚\n",
        "â”‚   â”‚Anomaly 2â”‚â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º â”‚ Alert 2 â”‚â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º â”‚         â”‚           â”‚\n",
        "â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚               â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚               â”‚(Multipleâ”‚           â”‚\n",
        "â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚               â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚               â”‚ alerts) â”‚           â”‚\n",
        "â”‚   â”‚Anomaly 3â”‚â”€â”˜               â”‚ Alert 3 â”‚â”€â”˜               â”‚         â”‚           â”‚\n",
        "â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜           â”‚\n",
        "â”‚                                                                                  â”‚\n",
        "â”‚   Multiple anomalies          One alert per               Case groups            â”‚\n",
        "â”‚   can trigger one             significant event           related alerts         â”‚\n",
        "â”‚   alert (aggregation)                                     for investigation      â”‚\n",
        "â”‚                                                                                  â”‚\n",
        "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## Step 8.1: Alert Generation Service\n",
        "\n",
        "### What This Does:\n",
        "Creates alerts when anomalies exceed thresholds.\n",
        "Not every anomaly becomes an alert - only significant ones.\n",
        "\n",
        "### OpenUBA Reference:\n",
        "File: `core/alert.py` (placeholder - you implement fully)\n",
        "\n",
        "### Implementation Tasks:\n",
        "\n",
        "**Task 8.1.1: Create AlertService Class**\n",
        "- Location: `uba/services/alert_service.py`\n",
        "- Manages alert creation and lifecycle\n",
        "\n",
        "**Task 8.1.2: Define Alert Structure**\n",
        "```\n",
        "alert = {\n",
        "    \"id\": \"uuid\",\n",
        "    \"user_id\": \"user123\",\n",
        "    \"title\": \"Unusual Login Pattern Detected\",\n",
        "    \"description\": \"User logged in from new location at unusual time\",\n",
        "    \"severity\": \"HIGH\",  # LOW, MEDIUM, HIGH, CRITICAL\n",
        "    \"risk_score\": 78,\n",
        "    \"anomaly_ids\": [\"anomaly1\", \"anomaly2\"],  # Related anomalies\n",
        "    \"source_models\": [\"login_anomaly\", \"time_anomaly\"],\n",
        "    \"evidence\": {\n",
        "        \"new_ip\": \"45.67.89.x\",\n",
        "        \"login_time\": \"03:45 AM\",\n",
        "        \"baseline_hours\": \"9 AM - 6 PM\"\n",
        "    },\n",
        "    \"status\": \"open\",  # open, acknowledged, investigating, resolved, false_positive\n",
        "    \"created_at\": \"2026-01-07T14:30:00Z\",\n",
        "    \"assigned_to\": null,\n",
        "    \"resolution_notes\": null\n",
        "}\n",
        "```\n",
        "\n",
        "**Task 8.1.3: Implement Alert Generation Logic**\n",
        "- Method: `generate_alert(user_id, anomalies, risk_score)` â†’ Alert\n",
        "- Threshold: risk_score > 50 OR critical anomaly\n",
        "- Aggregate related anomalies into single alert\n",
        "\n",
        "**Task 8.1.4: Implement Alert Deduplication**\n",
        "- Don't create duplicate alerts for same issue\n",
        "- Check if similar alert exists in last 24 hours\n",
        "- Update existing alert instead of creating new\n",
        "\n",
        "**Task 8.1.5: Implement Severity Classification**\n",
        "```\n",
        "SEVERITY MAPPING:\n",
        "\n",
        "Risk Score 0-25:   No alert (too low)\n",
        "Risk Score 26-50:  LOW severity alert\n",
        "Risk Score 51-75:  MEDIUM severity alert\n",
        "Risk Score 76-90:  HIGH severity alert\n",
        "Risk Score 91-100: CRITICAL severity alert\n",
        "\n",
        "Override to CRITICAL if:\n",
        "- External data transfer detected\n",
        "- Privilege escalation detected\n",
        "- Policy violation with HIGH severity\n",
        "```\n",
        "\n",
        "**Task 8.1.6: Implement Alert Notification**\n",
        "- Method: `notify(alert)` - send notification\n",
        "- Integration with DSPM notification system\n",
        "- Email, Slack, or in-app notification\n",
        "\n",
        "### Verification Checklist:\n",
        "- [ ] Alerts created for high-risk users\n",
        "- [ ] Deduplication prevents spam\n",
        "- [ ] Severity matches risk level\n",
        "- [ ] Notifications sent\n",
        "\n",
        "---\n",
        "\n",
        "## Step 8.2: Alert Priority System\n",
        "\n",
        "### What This Does:\n",
        "Ranks alerts so analysts focus on most important first.\n",
        "\n",
        "### Implementation Tasks:\n",
        "\n",
        "**Task 8.2.1: Define Priority Score**\n",
        "```\n",
        "PRIORITY SCORE CALCULATION:\n",
        "\n",
        "Base Score = Risk Score (0-100)\n",
        "\n",
        "Modifiers:\n",
        "+ 20 if user has admin privileges\n",
        "+ 15 if sensitive data accessed\n",
        "+ 10 if external transfer involved\n",
        "+ 10 if repeat offender (previous alerts)\n",
        "+ 5 if business hours violation\n",
        "- 10 if user is on travel (known exception)\n",
        "\n",
        "Final Priority = Base + Modifiers (capped at 100)\n",
        "```\n",
        "\n",
        "**Task 8.2.2: Implement Priority Calculator**\n",
        "- Method: `calculate_priority(alert)` â†’ priority score\n",
        "- Consider user context\n",
        "- Consider asset sensitivity\n",
        "\n",
        "**Task 8.2.3: Implement Alert Queue**\n",
        "- Method: `get_alert_queue()` â†’ ordered list\n",
        "- Ordered by priority (highest first)\n",
        "- Filter by status, assignee, date\n",
        "\n",
        "**Task 8.2.4: Add SLA Tracking**\n",
        "- Define response time by severity\n",
        "- CRITICAL: 1 hour, HIGH: 4 hours, MEDIUM: 24 hours\n",
        "- Track if SLA breached\n",
        "\n",
        "### Verification Checklist:\n",
        "- [ ] Priority score calculated correctly\n",
        "- [ ] Modifiers applied\n",
        "- [ ] Queue ordered by priority\n",
        "- [ ] SLA tracking works\n",
        "\n",
        "---\n",
        "\n",
        "## Step 8.3: Case Management Service\n",
        "\n",
        "### What This Does:\n",
        "Creates and manages investigation cases for groups of related alerts.\n",
        "\n",
        "### OpenUBA Reference:\n",
        "File: `core/case.py` (placeholder - you implement fully)\n",
        "\n",
        "### Implementation Tasks:\n",
        "\n",
        "**Task 8.3.1: Create CaseService Class**\n",
        "- Location: `uba/services/case_service.py`\n",
        "- Manages case lifecycle\n",
        "\n",
        "**Task 8.3.2: Define Case Structure**\n",
        "```\n",
        "case = {\n",
        "    \"id\": \"uuid\",\n",
        "    \"title\": \"Suspicious Activity - John Doe\",\n",
        "    \"description\": \"Multiple anomalies detected over 3 days\",\n",
        "    \"user_id\": \"user123\",\n",
        "    \"alert_ids\": [\"alert1\", \"alert2\", \"alert3\"],\n",
        "    \"severity\": \"HIGH\",\n",
        "    \"status\": \"open\",  # open, investigating, pending_review, closed\n",
        "    \"assignee\": \"analyst@company.com\",\n",
        "    \"created_at\": \"2026-01-07T14:30:00Z\",\n",
        "    \"updated_at\": \"2026-01-07T15:00:00Z\",\n",
        "    \"timeline\": [\n",
        "        {\"action\": \"created\", \"by\": \"system\", \"at\": \"...\"},\n",
        "        {\"action\": \"assigned\", \"by\": \"manager\", \"to\": \"analyst\", \"at\": \"...\"},\n",
        "        {\"action\": \"note_added\", \"by\": \"analyst\", \"note\": \"Investigating...\", \"at\": \"...\"}\n",
        "    ],\n",
        "    \"resolution\": {\n",
        "        \"outcome\": null,  # confirmed_threat, false_positive, inconclusive\n",
        "        \"notes\": null,\n",
        "        \"resolved_by\": null,\n",
        "        \"resolved_at\": null\n",
        "    }\n",
        "}\n",
        "```\n",
        "\n",
        "**Task 8.3.3: Implement Case Creation**\n",
        "- Method: `create_case(alerts)` â†’ Case\n",
        "- Automatically create for CRITICAL alerts\n",
        "- Or manually create by analyst\n",
        "\n",
        "**Task 8.3.4: Implement Alert-to-Case Grouping**\n",
        "- Method: `auto_group_alerts()` - group related alerts\n",
        "- Same user within time window â†’ same case\n",
        "- Similar anomaly types â†’ same case\n",
        "\n",
        "**Task 8.3.5: Implement Case Workflow**\n",
        "- Status transitions: open â†’ investigating â†’ pending_review â†’ closed\n",
        "- Track all actions in timeline\n",
        "- Require resolution notes when closing\n",
        "\n",
        "**Task 8.3.6: Implement Case Assignment**\n",
        "- Method: `assign_case(case_id, assignee)`\n",
        "- Round-robin or manual assignment\n",
        "- Notify assignee\n",
        "\n",
        "**Task 8.3.7: Write Unit Tests**\n",
        "- Test case creation\n",
        "- Test alert grouping\n",
        "- Test workflow transitions\n",
        "\n",
        "### Verification Checklist:\n",
        "- [ ] Cases created from alerts\n",
        "- [ ] Alert grouping works\n",
        "- [ ] Workflow transitions enforced\n",
        "- [ ] Timeline tracks all actions\n",
        "- [ ] Resolution required to close\n",
        "\n",
        "---\n",
        "\n",
        "# ðŸ“ PHASE 9: RULE ENGINE\n",
        "\n",
        "## Goal: Implement rule-based detection for known threat patterns\n",
        "\n",
        "---\n",
        "\n",
        "## Rules vs ML Models\n",
        "\n",
        "```\n",
        "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
        "â”‚                        RULES vs ML MODELS                                        â”‚\n",
        "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
        "â”‚                                                                                  â”‚\n",
        "â”‚   RULES (Deterministic)              ML MODELS (Probabilistic)                  â”‚\n",
        "â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                    â”‚\n",
        "â”‚   â”‚ IF condition THEN   â”‚            â”‚ Based on patterns   â”‚                    â”‚\n",
        "â”‚   â”‚    action           â”‚            â”‚ learned from data   â”‚                    â”‚\n",
        "â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                    â”‚\n",
        "â”‚                                                                                  â”‚\n",
        "â”‚   EXAMPLES:                          EXAMPLES:                                   â”‚\n",
        "â”‚   â€¢ Login from blocked IP            â€¢ Unusual login pattern                    â”‚\n",
        "â”‚   â€¢ 5+ failed logins                 â€¢ Anomalous data access                    â”‚\n",
        "â”‚   â€¢ Access after hours               â€¢ Behavior drift over time                 â”‚\n",
        "â”‚   â€¢ Known bad file hash              â€¢ Subtle insider threat                    â”‚\n",
        "â”‚                                                                                  â”‚\n",
        "â”‚   PROS:                              PROS:                                       â”‚\n",
        "â”‚   â€¢ Predictable                      â€¢ Catches unknown patterns                 â”‚\n",
        "â”‚   â€¢ Easy to explain                  â€¢ Adapts to new threats                    â”‚\n",
        "â”‚   â€¢ No training needed               â€¢ Handles complex patterns                 â”‚\n",
        "â”‚                                                                                  â”‚\n",
        "â”‚   CONS:                              CONS:                                       â”‚\n",
        "â”‚   â€¢ Only catches known patterns      â€¢ Harder to explain                        â”‚\n",
        "â”‚   â€¢ Needs manual updates             â€¢ Needs training data                      â”‚\n",
        "â”‚   â€¢ Can be bypassed                  â€¢ Can have false positives                 â”‚\n",
        "â”‚                                                                                  â”‚\n",
        "â”‚   BEST APPROACH: USE BOTH TOGETHER!                                             â”‚\n",
        "â”‚                                                                                  â”‚\n",
        "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## Step 9.1: Rule Definition System\n",
        "\n",
        "### What This Does:\n",
        "Allows defining detection rules that trigger alerts.\n",
        "\n",
        "### OpenUBA Reference:\n",
        "File: `core/rule.py` (placeholder - you implement fully)\n",
        "File: `core/storage/models.json` - has rule examples in model_group_context\n",
        "\n",
        "### Implementation Tasks:\n",
        "\n",
        "**Task 9.1.1: Create RuleService Class**\n",
        "- Location: `uba/services/rule_service.py`\n",
        "- Manages rule CRUD and execution\n",
        "\n",
        "**Task 9.1.2: Define Rule Structure**\n",
        "```\n",
        "rule = {\n",
        "    \"id\": \"uuid\",\n",
        "    \"name\": \"After Hours Login\",\n",
        "    \"description\": \"Detect logins outside business hours\",\n",
        "    \"enabled\": true,\n",
        "    \"severity\": \"MEDIUM\",\n",
        "    \"conditions\": [\n",
        "        {\n",
        "            \"field\": \"login_hour\",\n",
        "            \"operator\": \"not_between\",\n",
        "            \"value\": [9, 17]  # Outside 9 AM - 5 PM\n",
        "        }\n",
        "    ],\n",
        "    \"logic\": \"AND\",  # AND = all conditions, OR = any condition\n",
        "    \"exceptions\": [\n",
        "        {\n",
        "            \"field\": \"user_role\",\n",
        "            \"operator\": \"equals\",\n",
        "            \"value\": \"on_call\"  # On-call users excluded\n",
        "        }\n",
        "    ],\n",
        "    \"actions\": [\"create_anomaly\", \"create_alert\"],\n",
        "    \"score\": 30,  # Risk points to add\n",
        "    \"mitre_technique\": \"T1078\",  # Valid Accounts\n",
        "    \"created_by\": \"admin\",\n",
        "    \"created_at\": \"2026-01-01T00:00:00Z\"\n",
        "}\n",
        "```\n",
        "\n",
        "**Task 9.1.3: Define Operators**\n",
        "```\n",
        "SUPPORTED OPERATORS:\n",
        "\n",
        "Comparison:\n",
        "â€¢ equals, not_equals\n",
        "â€¢ greater_than, less_than\n",
        "â€¢ greater_or_equal, less_or_equal\n",
        "â€¢ between, not_between\n",
        "\n",
        "String:\n",
        "â€¢ contains, not_contains\n",
        "â€¢ starts_with, ends_with\n",
        "â€¢ matches (regex)\n",
        "\n",
        "List:\n",
        "â€¢ in, not_in\n",
        "\n",
        "Special:\n",
        "â€¢ is_null, is_not_null\n",
        "â€¢ is_new (not in baseline)\n",
        "â€¢ deviates_from_baseline (> N std)\n",
        "```\n",
        "\n",
        "**Task 9.1.4: Implement Rule CRUD**\n",
        "- Method: `create_rule(rule_data)` â†’ Rule\n",
        "- Method: `update_rule(rule_id, updates)` â†’ Rule\n",
        "- Method: `delete_rule(rule_id)` â†’ bool\n",
        "- Method: `get_rules(enabled=True)` â†’ list\n",
        "\n",
        "**Task 9.1.5: Store Rules in Database**\n",
        "- Table: uba_rules\n",
        "- Include version tracking\n",
        "- Audit log for changes\n",
        "\n",
        "### Verification Checklist:\n",
        "- [ ] Rule structure validated\n",
        "- [ ] All operators defined\n",
        "- [ ] CRUD operations work\n",
        "- [ ] Rules stored in database\n",
        "\n",
        "---\n",
        "\n",
        "## Step 9.2: Rule Evaluation Engine\n",
        "\n",
        "### What This Does:\n",
        "Evaluates rules against user data to detect matches.\n",
        "\n",
        "### Implementation Tasks:\n",
        "\n",
        "**Task 9.2.1: Create RuleEvaluator Class**\n",
        "- Location: Part of rule_service.py or separate file\n",
        "- Evaluates rules against data\n",
        "\n",
        "**Task 9.2.2: Implement Condition Evaluation**\n",
        "- Method: `evaluate_condition(condition, data)` â†’ bool\n",
        "- Handle all operator types\n",
        "- Return True if condition matches\n",
        "\n",
        "**Task 9.2.3: Implement Rule Evaluation**\n",
        "- Method: `evaluate_rule(rule, user_data)` â†’ bool\n",
        "- Evaluate all conditions\n",
        "- Apply AND/OR logic\n",
        "- Check exceptions\n",
        "\n",
        "**Task 9.2.4: Implement Batch Evaluation**\n",
        "- Method: `evaluate_all_rules(user_id)` â†’ list of matched rules\n",
        "- Run all enabled rules against user\n",
        "- Return list of matches with details\n",
        "\n",
        "**Task 9.2.5: Implement Rule Actions**\n",
        "- When rule matches, execute actions:\n",
        "  - `create_anomaly`: Create anomaly record\n",
        "  - `create_alert`: Create alert\n",
        "  - `add_risk_score`: Add points to risk\n",
        "  - `notify`: Send notification\n",
        "\n",
        "**Task 9.2.6: Add Rule Performance Tracking**\n",
        "- Track how often each rule fires\n",
        "- Track false positive rate\n",
        "- Identify noisy rules\n",
        "\n",
        "### Verification Checklist:\n",
        "- [ ] Conditions evaluate correctly\n",
        "- [ ] AND/OR logic works\n",
        "- [ ] Exceptions prevent matches\n",
        "- [ ] Actions execute on match\n",
        "- [ ] Performance tracked\n",
        "\n",
        "---\n",
        "\n",
        "## Step 9.3: Built-in Rule Library\n",
        "\n",
        "### What This Does:\n",
        "Provides pre-built rules for common threat patterns.\n",
        "\n",
        "### Default Rules to Create:\n",
        "\n",
        "| Rule Name | Condition | Severity | MITRE |\n",
        "|-----------|-----------|----------|-------|\n",
        "| After Hours Login | login_hour NOT IN [9-17] | MEDIUM | T1078 |\n",
        "| Failed Login Spike | failed_logins > 5 in 10 min | HIGH | T1110 |\n",
        "| New Location Login | location NOT IN baseline | MEDIUM | T1078 |\n",
        "| New Device Login | device NOT IN baseline | LOW | T1078 |\n",
        "| Sensitive Data Access | sensitive_asset_ratio > 0.5 | HIGH | T1530 |\n",
        "| Bulk File Download | files_accessed > 100 in 1 hour | HIGH | T1567 |\n",
        "| External Data Transfer | external_transfer = true | CRITICAL | T1041 |\n",
        "| Privilege Escalation | role_change to admin | CRITICAL | T1078.001 |\n",
        "| Weekend Activity | day_of_week IN [Sat, Sun] | LOW | T1078 |\n",
        "| Dormant Account Login | days_since_last_login > 90 | MEDIUM | T1078.001 |\n",
        "\n",
        "**Task 9.3.1: Create Default Rules**\n",
        "- Implement each rule from table above\n",
        "- Store in database on first startup\n",
        "- Mark as system rules (not deletable)\n",
        "\n",
        "**Task 9.3.2: Allow Custom Rules**\n",
        "- Users can create additional rules\n",
        "- Custom rules can override system rules\n",
        "- Mark as user-created\n",
        "\n",
        "### Verification Checklist:\n",
        "- [ ] All 10 default rules created\n",
        "- [ ] Rules fire correctly\n",
        "- [ ] Custom rules work\n",
        "- [ ] Can enable/disable rules\n",
        "\n",
        "---\n",
        "\n",
        "## Step 9.4: MITRE ATT&CK Mapping\n",
        "\n",
        "### What This Does:\n",
        "Maps detections to MITRE ATT&CK framework for standardized reporting.\n",
        "\n",
        "### What is MITRE ATT&CK?\n",
        "- Industry standard framework for cyber attacks\n",
        "- Organizes attacks into Tactics (goals) and Techniques (methods)\n",
        "- Helps communicate threats consistently\n",
        "\n",
        "### Relevant MITRE Techniques for UBA:\n",
        "\n",
        "| Technique ID | Name | UBA Detection |\n",
        "|--------------|------|---------------|\n",
        "| T1078 | Valid Accounts | Unusual login patterns |\n",
        "| T1078.001 | Valid Accounts: Default Accounts | Dormant account usage |\n",
        "| T1110 | Brute Force | Failed login attempts |\n",
        "| T1530 | Data from Cloud Storage | Unusual data access |\n",
        "| T1567 | Exfiltration Over Web Service | Bulk data transfer |\n",
        "| T1041 | Exfiltration Over C2 Channel | External transfers |\n",
        "| T1485 | Data Destruction | Unusual delete patterns |\n",
        "| T1565 | Data Manipulation | Unusual modification |\n",
        "\n",
        "### Implementation Tasks:\n",
        "\n",
        "**Task 9.4.1: Create MITRE Mapping Configuration**\n",
        "- Location: `uba/config/mitre_mapping.py`\n",
        "- Map each detection to technique ID\n",
        "- Include tactic and description\n",
        "\n",
        "**Task 9.4.2: Include in Alerts/Cases**\n",
        "- Add mitre_technique field to alerts\n",
        "- Show in case details\n",
        "- Enable filtering by technique\n",
        "\n",
        "**Task 9.4.3: Generate MITRE Report**\n",
        "- Method: `get_mitre_coverage()` â†’ coverage report\n",
        "- Show which techniques you can detect\n",
        "- Identify gaps in coverage\n",
        "\n",
        "### Verification Checklist:\n",
        "- [ ] Mapping configuration created\n",
        "- [ ] Alerts include MITRE reference\n",
        "- [ ] Coverage report generated\n",
        "- [ ] Filtering by technique works\n",
        "\n",
        "---\n",
        "\n",
        "## ðŸ“ END OF PHASES 7-9\n",
        "\n",
        "### What You Have After Phase 9:\n",
        "1. âœ… Analytics engine running models on schedule\n",
        "2. âœ… Risk score calculator with weighted aggregation\n",
        "3. âœ… Anomaly detection and storage\n",
        "4. âœ… Alert generation with severity classification\n",
        "5. âœ… Case management for investigations\n",
        "6. âœ… Rule engine with 10+ built-in rules\n",
        "7. âœ… MITRE ATT&CK mapping\n",
        "\n",
        "### How to Test Phases 7-9 Complete:\n",
        "\n",
        "```\n",
        "# Test analytics engine\n",
        "from uba.analytics.engine import AnalyticsEngine\n",
        "engine = AnalyticsEngine()\n",
        "scores = engine.analyze_user(\"test_user\")\n",
        "assert \"risk_score\" in scores\n",
        "\n",
        "# Test alert service\n",
        "from uba.services.alert_service import AlertService\n",
        "svc = AlertService(db_session)\n",
        "alert = svc.generate_alert(user_id, anomalies, risk_score=80)\n",
        "assert alert.severity == \"HIGH\"\n",
        "\n",
        "# Test rule engine\n",
        "from uba.services.rule_service import RuleService\n",
        "svc = RuleService(db_session)\n",
        "matches = svc.evaluate_all_rules(\"test_user\")\n",
        "print(f\"Matched {len(matches)} rules\")\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6a0f289b",
      "metadata": {
        "id": "6a0f289b"
      },
      "source": [
        "---\n",
        "\n",
        "# ðŸŒ PHASE 10: API ROUTES & ENDPOINTS\n",
        "\n",
        "## Goal: Build all REST API endpoints that expose UBA functionality\n",
        "\n",
        "---\n",
        "\n",
        "## API Architecture Overview\n",
        "\n",
        "```\n",
        "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
        "â”‚                           UBA_PRO API STRUCTURE                                  â”‚\n",
        "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
        "â”‚                                                                                  â”‚\n",
        "â”‚   BASE URL: /api/v1/uba                                                          â”‚\n",
        "â”‚                                                                                  â”‚\n",
        "â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚\n",
        "â”‚   â”‚ DASHBOARD                                                                â”‚   â”‚\n",
        "â”‚   â”‚ GET  /dashboard/summary          â†’ Overall UBA statistics               â”‚   â”‚\n",
        "â”‚   â”‚ GET  /dashboard/risk-distribution â†’ Risk level breakdown                â”‚   â”‚\n",
        "â”‚   â”‚ GET  /dashboard/trends           â†’ Risk trends over time                â”‚   â”‚\n",
        "â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚\n",
        "â”‚                                                                                  â”‚\n",
        "â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚\n",
        "â”‚   â”‚ PROFILES (User Behavioral Profiles)                                      â”‚   â”‚\n",
        "â”‚   â”‚ GET  /profiles                   â†’ List all user profiles               â”‚   â”‚\n",
        "â”‚   â”‚ GET  /profiles/{user_id}         â†’ Get specific user profile            â”‚   â”‚\n",
        "â”‚   â”‚ GET  /profiles/{user_id}/timeline â†’ User activity timeline              â”‚   â”‚\n",
        "â”‚   â”‚ GET  /profiles/{user_id}/risk-history â†’ Historical risk scores          â”‚   â”‚\n",
        "â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚\n",
        "â”‚                                                                                  â”‚\n",
        "â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚\n",
        "â”‚   â”‚ RISK                                                                     â”‚   â”‚\n",
        "â”‚   â”‚ GET  /risk/scores                â†’ All current risk scores              â”‚   â”‚\n",
        "â”‚   â”‚ GET  /risk/high-risk-users       â†’ Users with score > 75                â”‚   â”‚\n",
        "â”‚   â”‚ POST /risk/calculate/{user_id}   â†’ Trigger risk recalculation           â”‚   â”‚\n",
        "â”‚   â”‚ GET  /risk/trends/{user_id}      â†’ Risk score history                   â”‚   â”‚\n",
        "â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚\n",
        "â”‚                                                                                  â”‚\n",
        "â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚\n",
        "â”‚   â”‚ ANOMALIES                                                                â”‚   â”‚\n",
        "â”‚   â”‚ GET  /anomalies                  â†’ List all anomalies                   â”‚   â”‚\n",
        "â”‚   â”‚ GET  /anomalies/{id}             â†’ Get anomaly details                  â”‚   â”‚\n",
        "â”‚   â”‚ PUT  /anomalies/{id}/status      â†’ Update anomaly status                â”‚   â”‚\n",
        "â”‚   â”‚ GET  /anomalies/by-user/{user_id} â†’ Anomalies for specific user         â”‚   â”‚\n",
        "â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚\n",
        "â”‚                                                                                  â”‚\n",
        "â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚\n",
        "â”‚   â”‚ ALERTS                                                                   â”‚   â”‚\n",
        "â”‚   â”‚ GET  /alerts                     â†’ List all alerts                      â”‚   â”‚\n",
        "â”‚   â”‚ GET  /alerts/{id}                â†’ Get alert details                    â”‚   â”‚\n",
        "â”‚   â”‚ PUT  /alerts/{id}/status         â†’ Update alert status                  â”‚   â”‚\n",
        "â”‚   â”‚ PUT  /alerts/{id}/assign         â†’ Assign to analyst                    â”‚   â”‚\n",
        "â”‚   â”‚ GET  /alerts/queue               â†’ Priority-ordered alert queue         â”‚   â”‚\n",
        "â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚\n",
        "â”‚                                                                                  â”‚\n",
        "â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚\n",
        "â”‚   â”‚ CASES                                                                    â”‚   â”‚\n",
        "â”‚   â”‚ GET  /cases                      â†’ List all cases                       â”‚   â”‚\n",
        "â”‚   â”‚ POST /cases                      â†’ Create new case                      â”‚   â”‚\n",
        "â”‚   â”‚ GET  /cases/{id}                 â†’ Get case details                     â”‚   â”‚\n",
        "â”‚   â”‚ PUT  /cases/{id}                 â†’ Update case                          â”‚   â”‚\n",
        "â”‚   â”‚ PUT  /cases/{id}/status          â†’ Change case status                   â”‚   â”‚\n",
        "â”‚   â”‚ POST /cases/{id}/notes           â†’ Add investigation note               â”‚   â”‚\n",
        "â”‚   â”‚ PUT  /cases/{id}/resolve         â†’ Close case with resolution           â”‚   â”‚\n",
        "â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚\n",
        "â”‚                                                                                  â”‚\n",
        "â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚\n",
        "â”‚   â”‚ MODELS                                                                   â”‚   â”‚\n",
        "â”‚   â”‚ GET  /models                     â†’ List all ML models                   â”‚   â”‚\n",
        "â”‚   â”‚ GET  /models/{id}                â†’ Get model details                    â”‚   â”‚\n",
        "â”‚   â”‚ POST /models/{id}/train          â†’ Trigger model training               â”‚   â”‚\n",
        "â”‚   â”‚ GET  /models/{id}/performance    â†’ Model metrics                        â”‚   â”‚\n",
        "â”‚   â”‚ PUT  /models/{id}/enable         â†’ Enable/disable model                 â”‚   â”‚\n",
        "â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚\n",
        "â”‚                                                                                  â”‚\n",
        "â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚\n",
        "â”‚   â”‚ RULES                                                                    â”‚   â”‚\n",
        "â”‚   â”‚ GET  /rules                      â†’ List all rules                       â”‚   â”‚\n",
        "â”‚   â”‚ POST /rules                      â†’ Create new rule                      â”‚   â”‚\n",
        "â”‚   â”‚ GET  /rules/{id}                 â†’ Get rule details                     â”‚   â”‚\n",
        "â”‚   â”‚ PUT  /rules/{id}                 â†’ Update rule                          â”‚   â”‚\n",
        "â”‚   â”‚ DELETE /rules/{id}               â†’ Delete rule                          â”‚   â”‚\n",
        "â”‚   â”‚ PUT  /rules/{id}/enable          â†’ Enable/disable rule                  â”‚   â”‚\n",
        "â”‚   â”‚ GET  /rules/{id}/stats           â†’ Rule firing statistics               â”‚   â”‚\n",
        "â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚\n",
        "â”‚                                                                                  â”‚\n",
        "â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚\n",
        "â”‚   â”‚ BEHAVIORS (Real-time behavior tracking)                                  â”‚   â”‚\n",
        "â”‚   â”‚ GET  /behaviors/{user_id}        â†’ Current user behavior snapshot       â”‚   â”‚\n",
        "â”‚   â”‚ GET  /behaviors/{user_id}/baseline â†’ User's baseline behavior           â”‚   â”‚\n",
        "â”‚   â”‚ GET  /behaviors/compare          â†’ Compare users/periods                â”‚   â”‚\n",
        "â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚\n",
        "â”‚                                                                                  â”‚\n",
        "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## Step 10.1: Create Route Files\n",
        "\n",
        "### Implementation Tasks:\n",
        "\n",
        "**Task 10.1.1: Create Dashboard Routes**\n",
        "- Location: `uba/routes/dashboard.py`\n",
        "- Endpoints: summary, risk-distribution, trends\n",
        "\n",
        "```python\n",
        "# Example structure (implement each endpoint):\n",
        "\n",
        "from fastapi import APIRouter, Depends\n",
        "from sqlalchemy.orm import Session\n",
        "from uba.database import get_db\n",
        "from uba.services.dashboard_service import DashboardService\n",
        "\n",
        "router = APIRouter(prefix=\"/dashboard\", tags=[\"Dashboard\"])\n",
        "\n",
        "@router.get(\"/summary\")\n",
        "def get_summary(db: Session = Depends(get_db)):\n",
        "    \"\"\"\n",
        "    Returns:\n",
        "    {\n",
        "        \"total_users\": 150,\n",
        "        \"high_risk_users\": 12,\n",
        "        \"open_alerts\": 25,\n",
        "        \"open_cases\": 5,\n",
        "        \"anomalies_today\": 47,\n",
        "        \"average_risk_score\": 32.5\n",
        "    }\n",
        "    \"\"\"\n",
        "    service = DashboardService(db)\n",
        "    return service.get_summary()\n",
        "\n",
        "@router.get(\"/risk-distribution\")\n",
        "def get_risk_distribution(db: Session = Depends(get_db)):\n",
        "    \"\"\"\n",
        "    Returns:\n",
        "    {\n",
        "        \"low\": 98,      # 0-25 score\n",
        "        \"medium\": 35,   # 26-50 score\n",
        "        \"high\": 12,     # 51-75 score\n",
        "        \"critical\": 5   # 76-100 score\n",
        "    }\n",
        "    \"\"\"\n",
        "    service = DashboardService(db)\n",
        "    return service.get_risk_distribution()\n",
        "\n",
        "@router.get(\"/trends\")\n",
        "def get_trends(days: int = 30, db: Session = Depends(get_db)):\n",
        "    \"\"\"\n",
        "    Returns daily risk trends for charting\n",
        "    \"\"\"\n",
        "    service = DashboardService(db)\n",
        "    return service.get_trends(days)\n",
        "```\n",
        "\n",
        "**Task 10.1.2: Create Profile Routes**\n",
        "- Location: `uba/routes/profiles.py`\n",
        "- User behavioral profiles and history\n",
        "\n",
        "**Task 10.1.3: Create Risk Routes**\n",
        "- Location: `uba/routes/risk.py`\n",
        "- Risk scores and calculations\n",
        "\n",
        "**Task 10.1.4: Create Anomaly Routes**\n",
        "- Location: `uba/routes/anomalies.py`\n",
        "- Anomaly listing and management\n",
        "\n",
        "**Task 10.1.5: Create Alert Routes**\n",
        "- Location: `uba/routes/alerts.py`\n",
        "- Alert queue and management\n",
        "\n",
        "**Task 10.1.6: Create Case Routes**\n",
        "- Location: `uba/routes/cases.py`\n",
        "- Case management endpoints\n",
        "\n",
        "**Task 10.1.7: Create Model Routes**\n",
        "- Location: `uba/routes/models.py`\n",
        "- ML model management\n",
        "\n",
        "**Task 10.1.8: Create Rule Routes**\n",
        "- Location: `uba/routes/rules.py`\n",
        "- Rule CRUD operations\n",
        "\n",
        "**Task 10.1.9: Create Behavior Routes**\n",
        "- Location: `uba/routes/behaviors.py`\n",
        "- Real-time behavior data\n",
        "\n",
        "---\n",
        "\n",
        "## Step 10.2: Create Pydantic Schemas\n",
        "\n",
        "### What Are Schemas?\n",
        "Pydantic schemas define the structure of API requests and responses.\n",
        "They provide validation and documentation.\n",
        "\n",
        "### Implementation Tasks:\n",
        "\n",
        "**Task 10.2.1: Create Base Schemas**\n",
        "- Location: `uba/schemas/base.py`\n",
        "\n",
        "```python\n",
        "from pydantic import BaseModel\n",
        "from datetime import datetime\n",
        "from typing import Optional, List\n",
        "\n",
        "class PaginatedResponse(BaseModel):\n",
        "    items: List\n",
        "    total: int\n",
        "    page: int\n",
        "    page_size: int\n",
        "    \n",
        "class StatusUpdate(BaseModel):\n",
        "    status: str\n",
        "    notes: Optional[str] = None\n",
        "```\n",
        "\n",
        "**Task 10.2.2: Create Profile Schemas**\n",
        "- Location: `uba/schemas/profile.py`\n",
        "\n",
        "```python\n",
        "class UserProfileResponse(BaseModel):\n",
        "    user_id: str\n",
        "    email: Optional[str]\n",
        "    department: Optional[str]\n",
        "    role: Optional[str]\n",
        "    risk_score: float\n",
        "    risk_level: str  # LOW, MEDIUM, HIGH, CRITICAL\n",
        "    last_activity: Optional[datetime]\n",
        "    anomaly_count: int\n",
        "    alert_count: int\n",
        "    baseline_established: bool\n",
        "    \n",
        "    class Config:\n",
        "        from_attributes = True\n",
        "```\n",
        "\n",
        "**Task 10.2.3: Create Risk Schemas**\n",
        "- Location: `uba/schemas/risk.py`\n",
        "\n",
        "```python\n",
        "class RiskScoreResponse(BaseModel):\n",
        "    user_id: str\n",
        "    composite_score: float\n",
        "    risk_level: str\n",
        "    components: dict  # Individual model scores\n",
        "    calculated_at: datetime\n",
        "    \n",
        "class RiskTrendPoint(BaseModel):\n",
        "    date: datetime\n",
        "    score: float\n",
        "```\n",
        "\n",
        "**Task 10.2.4: Create Anomaly Schemas**\n",
        "- Location: `uba/schemas/anomaly.py`\n",
        "\n",
        "```python\n",
        "class AnomalyResponse(BaseModel):\n",
        "    id: str\n",
        "    user_id: str\n",
        "    anomaly_type: str\n",
        "    score: float\n",
        "    details: dict\n",
        "    status: str\n",
        "    detected_at: datetime\n",
        "```\n",
        "\n",
        "**Task 10.2.5: Create Alert Schemas**\n",
        "- Location: `uba/schemas/alert.py`\n",
        "\n",
        "```python\n",
        "class AlertResponse(BaseModel):\n",
        "    id: str\n",
        "    user_id: str\n",
        "    title: str\n",
        "    description: str\n",
        "    severity: str\n",
        "    priority_score: int\n",
        "    status: str\n",
        "    assigned_to: Optional[str]\n",
        "    created_at: datetime\n",
        "    mitre_technique: Optional[str]\n",
        "\n",
        "class AlertAssignment(BaseModel):\n",
        "    assignee: str\n",
        "```\n",
        "\n",
        "**Task 10.2.6: Create Case Schemas**\n",
        "- Location: `uba/schemas/case.py`\n",
        "\n",
        "```python\n",
        "class CaseCreate(BaseModel):\n",
        "    title: str\n",
        "    description: str\n",
        "    user_id: str\n",
        "    alert_ids: List[str]\n",
        "    \n",
        "class CaseResponse(BaseModel):\n",
        "    id: str\n",
        "    title: str\n",
        "    user_id: str\n",
        "    alert_count: int\n",
        "    severity: str\n",
        "    status: str\n",
        "    assignee: Optional[str]\n",
        "    created_at: datetime\n",
        "    \n",
        "class CaseNote(BaseModel):\n",
        "    content: str\n",
        "    \n",
        "class CaseResolution(BaseModel):\n",
        "    outcome: str  # confirmed_threat, false_positive, inconclusive\n",
        "    notes: str\n",
        "```\n",
        "\n",
        "**Task 10.2.7: Create Rule Schemas**\n",
        "- Location: `uba/schemas/rule.py`\n",
        "\n",
        "```python\n",
        "class RuleCondition(BaseModel):\n",
        "    field: str\n",
        "    operator: str\n",
        "    value: any\n",
        "\n",
        "class RuleCreate(BaseModel):\n",
        "    name: str\n",
        "    description: str\n",
        "    conditions: List[RuleCondition]\n",
        "    logic: str = \"AND\"  # AND or OR\n",
        "    severity: str\n",
        "    score: int\n",
        "    enabled: bool = True\n",
        "    mitre_technique: Optional[str]\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## Step 10.3: Register Routes in Main App\n",
        "\n",
        "### Implementation Tasks:\n",
        "\n",
        "**Task 10.3.1: Create Router Registry**\n",
        "- Location: `uba/routes/__init__.py`\n",
        "\n",
        "```python\n",
        "from fastapi import APIRouter\n",
        "from uba.routes import (\n",
        "    dashboard,\n",
        "    profiles,\n",
        "    risk,\n",
        "    anomalies,\n",
        "    alerts,\n",
        "    cases,\n",
        "    models,\n",
        "    rules,\n",
        "    behaviors\n",
        ")\n",
        "\n",
        "uba_router = APIRouter(prefix=\"/api/v1/uba\")\n",
        "\n",
        "uba_router.include_router(dashboard.router)\n",
        "uba_router.include_router(profiles.router)\n",
        "uba_router.include_router(risk.router)\n",
        "uba_router.include_router(anomalies.router)\n",
        "uba_router.include_router(alerts.router)\n",
        "uba_router.include_router(cases.router)\n",
        "uba_router.include_router(models.router)\n",
        "uba_router.include_router(rules.router)\n",
        "uba_router.include_router(behaviors.router)\n",
        "```\n",
        "\n",
        "**Task 10.3.2: Register in DSPM Main App**\n",
        "- Modify DSPM's main.py to include UBA routes\n",
        "\n",
        "```python\n",
        "# In DSPM's main.py\n",
        "from uba.routes import uba_router\n",
        "\n",
        "app.include_router(uba_router)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## Step 10.4: Add Authentication & Authorization\n",
        "\n",
        "### Implementation Tasks:\n",
        "\n",
        "**Task 10.4.1: Integrate with DSPM's Keycloak Auth**\n",
        "- Use DSPM's existing `get_current_user` dependency\n",
        "- Require authentication on all UBA endpoints\n",
        "\n",
        "**Task 10.4.2: Define UBA Permissions**\n",
        "\n",
        "| Permission | Description | Roles |\n",
        "|------------|-------------|-------|\n",
        "| uba:view | View dashboards, profiles, scores | All users |\n",
        "| uba:analyze | Trigger analysis, view details | Analysts |\n",
        "| uba:manage_alerts | Update alerts, assign | Analysts, Managers |\n",
        "| uba:manage_cases | Create, update, close cases | Analysts, Managers |\n",
        "| uba:manage_rules | Create, update, delete rules | Admins |\n",
        "| uba:manage_models | Train, enable/disable models | Admins |\n",
        "\n",
        "**Task 10.4.3: Create Permission Decorator**\n",
        "\n",
        "```python\n",
        "from functools import wraps\n",
        "from fastapi import HTTPException\n",
        "\n",
        "def require_permission(permission: str):\n",
        "    def decorator(func):\n",
        "        @wraps(func)\n",
        "        async def wrapper(*args, current_user=None, **kwargs):\n",
        "            if not current_user.has_permission(permission):\n",
        "                raise HTTPException(403, \"Permission denied\")\n",
        "            return await func(*args, current_user=current_user, **kwargs)\n",
        "        return wrapper\n",
        "    return decorator\n",
        "```\n",
        "\n",
        "### Verification Checklist:\n",
        "- [ ] All routes created\n",
        "- [ ] Schemas validate input/output\n",
        "- [ ] Routes registered in app\n",
        "- [ ] Authentication required\n",
        "- [ ] Permissions enforced\n",
        "\n",
        "---\n",
        "\n",
        "# ðŸ”— PHASE 11: DSPM INTEGRATION\n",
        "\n",
        "## Goal: Connect UBA_PRO to existing DSPM data and workflows\n",
        "\n",
        "---\n",
        "\n",
        "## Integration Architecture\n",
        "\n",
        "```\n",
        "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
        "â”‚                         DSPM + UBA INTEGRATION                                   â”‚\n",
        "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
        "â”‚                                                                                  â”‚\n",
        "â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚\n",
        "â”‚   â”‚                          DSPM CORE                                       â”‚   â”‚\n",
        "â”‚   â”‚                                                                          â”‚   â”‚\n",
        "â”‚   â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚   â”‚\n",
        "â”‚   â”‚  â”‚   Users    â”‚  â”‚  Sessions  â”‚  â”‚   Policy   â”‚  â”‚   Assets   â”‚        â”‚   â”‚\n",
        "â”‚   â”‚  â”‚            â”‚  â”‚            â”‚  â”‚   Audit    â”‚  â”‚            â”‚        â”‚   â”‚\n",
        "â”‚   â”‚  â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜        â”‚   â”‚\n",
        "â”‚   â”‚        â”‚               â”‚               â”‚               â”‚               â”‚   â”‚\n",
        "â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚\n",
        "â”‚            â”‚               â”‚               â”‚               â”‚                    â”‚\n",
        "â”‚            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                    â”‚\n",
        "â”‚                                    â”‚                                             â”‚\n",
        "â”‚                                    â–¼                                             â”‚\n",
        "â”‚            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                    â”‚\n",
        "â”‚            â”‚           DSPM INTEGRATION SERVICE            â”‚                    â”‚\n",
        "â”‚            â”‚                                               â”‚                    â”‚\n",
        "â”‚            â”‚  â€¢ Reads DSPM tables for behavioral data      â”‚                    â”‚\n",
        "â”‚            â”‚  â€¢ Transforms to UBA format                   â”‚                    â”‚\n",
        "â”‚            â”‚  â€¢ Handles real-time event streaming          â”‚                    â”‚\n",
        "â”‚            â”‚  â€¢ Syncs user information                     â”‚                    â”‚\n",
        "â”‚            â”‚                                               â”‚                    â”‚\n",
        "â”‚            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                    â”‚\n",
        "â”‚                                    â”‚                                             â”‚\n",
        "â”‚                                    â–¼                                             â”‚\n",
        "â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚\n",
        "â”‚   â”‚                           UBA_PRO                                        â”‚   â”‚\n",
        "â”‚   â”‚                                                                          â”‚   â”‚\n",
        "â”‚   â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚   â”‚\n",
        "â”‚   â”‚  â”‚  Profiles  â”‚  â”‚   Risk     â”‚  â”‚  Anomaly   â”‚  â”‚   Alert    â”‚        â”‚   â”‚\n",
        "â”‚   â”‚  â”‚            â”‚  â”‚   Engine   â”‚  â”‚   Engine   â”‚  â”‚   Engine   â”‚        â”‚   â”‚\n",
        "â”‚   â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚   â”‚\n",
        "â”‚   â”‚                                                                          â”‚   â”‚\n",
        "â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚\n",
        "â”‚                                                                                  â”‚\n",
        "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## Step 11.1: DSPM Data Integration Service\n",
        "\n",
        "### What This Does:\n",
        "Reads behavioral data from DSPM tables and transforms for UBA analysis.\n",
        "\n",
        "### Implementation Tasks:\n",
        "\n",
        "**Task 11.1.1: Create DSPMIntegrationService**\n",
        "- Location: `uba/services/dspm_integration.py`\n",
        "- Main interface between DSPM and UBA\n",
        "\n",
        "**Task 11.1.2: Implement User Sync**\n",
        "- Method: `sync_users()` â†’ sync DSPM users to UBA profiles\n",
        "- Map DSPM user fields to UBA profile fields\n",
        "- Run daily to catch new users\n",
        "\n",
        "```python\n",
        "class DSPMIntegrationService:\n",
        "    def __init__(self, db: Session):\n",
        "        self.db = db\n",
        "    \n",
        "    def sync_users(self):\n",
        "        \"\"\"\n",
        "        Sync users from DSPM identity tables to UBA profiles\n",
        "        \"\"\"\n",
        "        # Get users from DSPM\n",
        "        dspm_users = self.db.query(DSPMUser).all()\n",
        "        \n",
        "        for user in dspm_users:\n",
        "            # Check if profile exists\n",
        "            profile = self.db.query(UBAProfile).filter_by(\n",
        "                dspm_user_id=user.id\n",
        "            ).first()\n",
        "            \n",
        "            if not profile:\n",
        "                # Create new profile\n",
        "                profile = UBAProfile(\n",
        "                    dspm_user_id=user.id,\n",
        "                    email=user.email,\n",
        "                    department=user.department,\n",
        "                    # ... other fields\n",
        "                )\n",
        "                self.db.add(profile)\n",
        "        \n",
        "        self.db.commit()\n",
        "```\n",
        "\n",
        "**Task 11.1.3: Implement Session Data Extraction**\n",
        "- Method: `get_user_sessions(user_id, days=30)` â†’ session data\n",
        "- Query DSPM's user_sessions table\n",
        "- Transform to UBA feature format\n",
        "\n",
        "**Task 11.1.4: Implement Policy Audit Extraction**\n",
        "- Method: `get_policy_events(user_id, days=30)` â†’ policy events\n",
        "- Query DSPM's policy_audit_log table\n",
        "- Extract behavioral signals\n",
        "\n",
        "**Task 11.1.5: Implement Access Pattern Extraction**\n",
        "- Method: `get_access_patterns(user_id, days=30)` â†’ access data\n",
        "- Query DSPM's access_controls and related tables\n",
        "- Build access behavior profile\n",
        "\n",
        "---\n",
        "\n",
        "## Step 11.2: Data Transformation Layer\n",
        "\n",
        "### What This Does:\n",
        "Converts DSPM data format to UBA feature format.\n",
        "\n",
        "### Implementation Tasks:\n",
        "\n",
        "**Task 11.2.1: Create Data Transformers**\n",
        "- Location: `uba/services/transformers.py`\n",
        "- Convert each data type\n",
        "\n",
        "**Task 11.2.2: Session Transformer**\n",
        "\n",
        "```python\n",
        "def transform_sessions(sessions: List[DSPMSession]) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Transform DSPM sessions to UBA format\n",
        "    \n",
        "    Input (DSPM format):\n",
        "    {\n",
        "        \"id\": \"...\",\n",
        "        \"user_id\": \"...\",\n",
        "        \"ip_address\": \"192.168.1.100\",\n",
        "        \"user_agent\": \"Mozilla/5.0...\",\n",
        "        \"location\": \"New York\",\n",
        "        \"created_at\": \"2026-01-07T10:00:00Z\"\n",
        "    }\n",
        "    \n",
        "    Output (UBA format):\n",
        "    {\n",
        "        \"user_id\": \"...\",\n",
        "        \"login_hour\": 10,\n",
        "        \"login_day\": 2,  # Tuesday\n",
        "        \"ip_address\": \"192.168.1.100\",\n",
        "        \"ip_is_new\": False,\n",
        "        \"location\": \"New York\",\n",
        "        \"location_is_new\": False,\n",
        "        \"device_fingerprint\": \"abc123\",\n",
        "        \"device_is_new\": False\n",
        "    }\n",
        "    \"\"\"\n",
        "    # Implementation\n",
        "    pass\n",
        "```\n",
        "\n",
        "**Task 11.2.3: Policy Event Transformer**\n",
        "\n",
        "```python\n",
        "def transform_policy_events(events: List[PolicyAuditLog]) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Transform policy events to UBA format\n",
        "    \n",
        "    Input (DSPM format):\n",
        "    {\n",
        "        \"event_type\": \"data_access\",\n",
        "        \"severity\": \"medium\",\n",
        "        \"resource_accessed\": \"sensitive_file.csv\"\n",
        "    }\n",
        "    \n",
        "    Output (UBA format):\n",
        "    {\n",
        "        \"user_id\": \"...\",\n",
        "        \"event_count\": 45,\n",
        "        \"high_severity_count\": 3,\n",
        "        \"sensitive_access_count\": 12,\n",
        "        \"unique_resources\": 8\n",
        "    }\n",
        "    \"\"\"\n",
        "    pass\n",
        "```\n",
        "\n",
        "**Task 11.2.4: Access Pattern Transformer**\n",
        "\n",
        "```python\n",
        "def transform_access_patterns(accesses: List[AccessControl]) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Transform access patterns to UBA format\n",
        "    \"\"\"\n",
        "    pass\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## Step 11.3: Real-time Event Integration\n",
        "\n",
        "### What This Does:\n",
        "Processes DSPM events in real-time for immediate analysis.\n",
        "\n",
        "### Implementation Tasks:\n",
        "\n",
        "**Task 11.3.1: Create Event Handler**\n",
        "- Location: `uba/services/event_handler.py`\n",
        "- Process incoming DSPM events\n",
        "\n",
        "**Task 11.3.2: Define Event Types**\n",
        "\n",
        "| DSPM Event | UBA Action |\n",
        "|------------|------------|\n",
        "| user_login | Update session data, check login rules |\n",
        "| data_access | Update access patterns, check access rules |\n",
        "| policy_violation | Immediate anomaly check, possible alert |\n",
        "| privilege_change | Check privilege escalation rules |\n",
        "| file_transfer | Check data exfiltration rules |\n",
        "\n",
        "**Task 11.3.3: Implement Event Processing**\n",
        "\n",
        "```python\n",
        "class UBAEventHandler:\n",
        "    def handle_event(self, event: dict):\n",
        "        \"\"\"\n",
        "        Process a DSPM event in real-time\n",
        "        \"\"\"\n",
        "        event_type = event.get(\"type\")\n",
        "        \n",
        "        if event_type == \"user_login\":\n",
        "            self._handle_login(event)\n",
        "        elif event_type == \"data_access\":\n",
        "            self._handle_access(event)\n",
        "        elif event_type == \"policy_violation\":\n",
        "            self._handle_violation(event)\n",
        "        # ... other event types\n",
        "    \n",
        "    def _handle_login(self, event):\n",
        "        # 1. Get user profile\n",
        "        # 2. Check login rules\n",
        "        # 3. Update session data\n",
        "        # 4. Trigger quick risk check if rules match\n",
        "        pass\n",
        "```\n",
        "\n",
        "**Task 11.3.4: Integrate with DSPM Event System**\n",
        "- Hook into DSPM's event publishing\n",
        "- Subscribe to relevant event types\n",
        "- Process asynchronously\n",
        "\n",
        "---\n",
        "\n",
        "## Step 11.4: DSPM UI Integration\n",
        "\n",
        "### What This Does:\n",
        "Adds UBA components to existing DSPM dashboard.\n",
        "\n",
        "### Implementation Tasks:\n",
        "\n",
        "**Task 11.4.1: Add UBA Widget to DSPM Dashboard**\n",
        "- Show high-risk users summary\n",
        "- Show recent alerts count\n",
        "- Link to full UBA dashboard\n",
        "\n",
        "**Task 11.4.2: Add Risk Indicator to User Pages**\n",
        "- Show risk score on user profile pages\n",
        "- Add link to detailed UBA profile\n",
        "\n",
        "**Task 11.4.3: Integrate Alerts with DSPM Notifications**\n",
        "- UBA alerts appear in DSPM notification center\n",
        "- Use same notification UI components\n",
        "\n",
        "**Task 11.4.4: Add UBA Section to Navigation**\n",
        "- Add \"User Analytics\" or \"Behavior Analysis\" menu item\n",
        "- Link to UBA dashboard and sub-pages\n",
        "\n",
        "---\n",
        "\n",
        "## Step 11.5: Policy Violation Integration\n",
        "\n",
        "### What This Does:\n",
        "Uses DSPM's existing policy violations as LABELED DATA for training.\n",
        "\n",
        "### This is CRITICAL for Training:\n",
        "\n",
        "```\n",
        "DSPM Policy Violations = LABELED TRAINING DATA\n",
        "\n",
        "policy_violations table:\n",
        "â”œâ”€â”€ status = \"open\"          â†’ Potential anomaly (unlabeled)\n",
        "â”œâ”€â”€ status = \"resolved\"      â†’ Confirmed issue (positive label)\n",
        "â”œâ”€â”€ status = \"false_positive\" â†’ Not an issue (negative label)\n",
        "\n",
        "USE THIS FOR:\n",
        "1. Initial model training (historical violations)\n",
        "2. Continuous learning (new labeled violations)\n",
        "3. Model evaluation (how many we catch)\n",
        "```\n",
        "\n",
        "### Implementation Tasks:\n",
        "\n",
        "**Task 11.5.1: Create Training Data Extractor**\n",
        "- Extract labeled violations from DSPM\n",
        "- Transform to ML training format\n",
        "- Create training/test split\n",
        "\n",
        "**Task 11.5.2: Implement Feedback Loop**\n",
        "- When analyst marks alert as false positive â†’ feed back to model\n",
        "- When analyst confirms threat â†’ feed back to model\n",
        "- Continuous model improvement\n",
        "\n",
        "### Verification Checklist:\n",
        "- [ ] User sync works\n",
        "- [ ] Session data extracted correctly\n",
        "- [ ] Policy events extracted correctly\n",
        "- [ ] Real-time events processed\n",
        "- [ ] UI integration complete\n",
        "- [ ] Training data extracted from violations\n",
        "\n",
        "---\n",
        "\n",
        "# ðŸ§ª PHASE 12: TESTING & DEPLOYMENT\n",
        "\n",
        "## Goal: Comprehensive testing and production deployment\n",
        "\n",
        "---\n",
        "\n",
        "## Testing Strategy\n",
        "\n",
        "```\n",
        "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
        "â”‚                           TESTING PYRAMID                                        â”‚\n",
        "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
        "â”‚                                                                                  â”‚\n",
        "â”‚                              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                       â”‚\n",
        "â”‚                              â”‚   E2E     â”‚  â—„â”€â”€ Slow, expensive, few            â”‚\n",
        "â”‚                              â”‚   Tests   â”‚      (10-20 tests)                   â”‚\n",
        "â”‚                              â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜                                       â”‚\n",
        "â”‚                           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”                                    â”‚\n",
        "â”‚                           â”‚  Integration    â”‚  â—„â”€â”€ Medium speed                 â”‚\n",
        "â”‚                           â”‚     Tests       â”‚      (50-100 tests)               â”‚\n",
        "â”‚                           â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                    â”‚\n",
        "â”‚                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                            â”‚\n",
        "â”‚                    â”‚         Unit Tests            â”‚  â—„â”€â”€ Fast, many            â”‚\n",
        "â”‚                    â”‚                               â”‚      (200+ tests)          â”‚\n",
        "â”‚                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                            â”‚\n",
        "â”‚                                                                                  â”‚\n",
        "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## Step 12.1: Unit Testing\n",
        "\n",
        "### What to Test:\n",
        "Every individual function and class method.\n",
        "\n",
        "### Implementation Tasks:\n",
        "\n",
        "**Task 12.1.1: Test Feature Extraction**\n",
        "- Location: `uba/tests/test_feature_extractor.py`\n",
        "\n",
        "```python\n",
        "import pytest\n",
        "from uba.analytics.feature_extractor import FeatureExtractor\n",
        "\n",
        "class TestFeatureExtractor:\n",
        "    \n",
        "    def test_extract_login_features(self):\n",
        "        \"\"\"Test login feature extraction\"\"\"\n",
        "        extractor = FeatureExtractor()\n",
        "        sessions = [...]  # Mock session data\n",
        "        \n",
        "        features = extractor.extract_login_features(sessions)\n",
        "        \n",
        "        assert \"login_count\" in features\n",
        "        assert \"unique_ips\" in features\n",
        "        assert features[\"login_count\"] >= 0\n",
        "    \n",
        "    def test_empty_input(self):\n",
        "        \"\"\"Test handling of empty data\"\"\"\n",
        "        extractor = FeatureExtractor()\n",
        "        features = extractor.extract_login_features([])\n",
        "        \n",
        "        assert features[\"login_count\"] == 0\n",
        "```\n",
        "\n",
        "**Task 12.1.2: Test Risk Calculator**\n",
        "- Location: `uba/tests/test_risk_calculator.py`\n",
        "\n",
        "```python\n",
        "class TestRiskCalculator:\n",
        "    \n",
        "    def test_calculate_composite_score(self):\n",
        "        calculator = RiskCalculator()\n",
        "        scores = {\n",
        "            \"login\": 0.8,\n",
        "            \"access\": 0.6,\n",
        "            \"transfer\": 0.4,\n",
        "            \"time\": 0.2\n",
        "        }\n",
        "        \n",
        "        result = calculator.calculate(scores)\n",
        "        \n",
        "        assert 0 <= result <= 1\n",
        "    \n",
        "    def test_classify_risk_level(self):\n",
        "        calculator = RiskCalculator()\n",
        "        \n",
        "        assert calculator.classify(0.1) == \"LOW\"\n",
        "        assert calculator.classify(0.4) == \"MEDIUM\"\n",
        "        assert calculator.classify(0.7) == \"HIGH\"\n",
        "        assert calculator.classify(0.9) == \"CRITICAL\"\n",
        "```\n",
        "\n",
        "**Task 12.1.3: Test ML Models**\n",
        "- Test model training\n",
        "- Test prediction\n",
        "- Test serialization/deserialization\n",
        "\n",
        "**Task 12.1.4: Test Services**\n",
        "- Test each service method\n",
        "- Mock database sessions\n",
        "- Test error handling\n",
        "\n",
        "**Task 12.1.5: Test Rule Engine**\n",
        "- Test each operator\n",
        "- Test condition evaluation\n",
        "- Test rule matching\n",
        "\n",
        "---\n",
        "\n",
        "## Step 12.2: Integration Testing\n",
        "\n",
        "### What to Test:\n",
        "Multiple components working together.\n",
        "\n",
        "### Implementation Tasks:\n",
        "\n",
        "**Task 12.2.1: Test API Endpoints**\n",
        "- Location: `uba/tests/test_api.py`\n",
        "\n",
        "```python\n",
        "from fastapi.testclient import TestClient\n",
        "from uba.main import app\n",
        "\n",
        "client = TestClient(app)\n",
        "\n",
        "class TestDashboardAPI:\n",
        "    \n",
        "    def test_get_summary(self, auth_headers):\n",
        "        response = client.get(\n",
        "            \"/api/v1/uba/dashboard/summary\",\n",
        "            headers=auth_headers\n",
        "        )\n",
        "        \n",
        "        assert response.status_code == 200\n",
        "        data = response.json()\n",
        "        assert \"total_users\" in data\n",
        "        assert \"high_risk_users\" in data\n",
        "\n",
        "class TestAlertAPI:\n",
        "    \n",
        "    def test_create_and_update_alert(self, auth_headers):\n",
        "        # Create scenario that generates alert\n",
        "        # Verify alert was created\n",
        "        # Update alert status\n",
        "        # Verify status changed\n",
        "        pass\n",
        "```\n",
        "\n",
        "**Task 12.2.2: Test Database Operations**\n",
        "- Test CRUD for all models\n",
        "- Test relationships\n",
        "- Test constraints\n",
        "\n",
        "**Task 12.2.3: Test DSPM Integration**\n",
        "- Test data extraction\n",
        "- Test transformation\n",
        "- Test sync operations\n",
        "\n",
        "---\n",
        "\n",
        "## Step 12.3: End-to-End Testing\n",
        "\n",
        "### What to Test:\n",
        "Complete user workflows from start to finish.\n",
        "\n",
        "### Implementation Tasks:\n",
        "\n",
        "**Task 12.3.1: Define E2E Test Scenarios**\n",
        "\n",
        "| Scenario | Steps | Expected Result |\n",
        "|----------|-------|-----------------|\n",
        "| New User Detection | 1. Add user to DSPM<br>2. User logs in<br>3. Run sync | UBA profile created |\n",
        "| Anomaly to Alert | 1. Generate anomalous behavior<br>2. Run analysis<br>3. Check alerts | Alert created for anomaly |\n",
        "| Case Workflow | 1. Create case<br>2. Assign<br>3. Add notes<br>4. Resolve | Case closed with resolution |\n",
        "| Rule Firing | 1. Create rule<br>2. Generate matching event<br>3. Run rules | Rule matches, action executed |\n",
        "\n",
        "**Task 12.3.2: Implement E2E Tests**\n",
        "- Use test database\n",
        "- Create test data\n",
        "- Run complete workflows\n",
        "- Verify final state\n",
        "\n",
        "---\n",
        "\n",
        "## Step 12.4: Performance Testing\n",
        "\n",
        "### What to Test:\n",
        "System behavior under load.\n",
        "\n",
        "### Implementation Tasks:\n",
        "\n",
        "**Task 12.4.1: Define Performance Targets**\n",
        "\n",
        "| Metric | Target |\n",
        "|--------|--------|\n",
        "| API response time (p95) | < 500ms |\n",
        "| Full user analysis | < 5 seconds |\n",
        "| Batch analysis (100 users) | < 2 minutes |\n",
        "| Model training | < 30 minutes |\n",
        "\n",
        "**Task 12.4.2: Create Load Tests**\n",
        "\n",
        "```python\n",
        "# Using locust or similar tool\n",
        "from locust import HttpUser, task, between\n",
        "\n",
        "class UBAUser(HttpUser):\n",
        "    wait_time = between(1, 3)\n",
        "    \n",
        "    @task(3)\n",
        "    def get_dashboard(self):\n",
        "        self.client.get(\"/api/v1/uba/dashboard/summary\")\n",
        "    \n",
        "    @task(2)\n",
        "    def get_alerts(self):\n",
        "        self.client.get(\"/api/v1/uba/alerts\")\n",
        "    \n",
        "    @task(1)\n",
        "    def get_profile(self):\n",
        "        user_id = \"test_user\"\n",
        "        self.client.get(f\"/api/v1/uba/profiles/{user_id}\")\n",
        "```\n",
        "\n",
        "**Task 12.4.3: Run and Analyze**\n",
        "- Run load tests\n",
        "- Identify bottlenecks\n",
        "- Optimize slow queries\n",
        "\n",
        "---\n",
        "\n",
        "## Step 12.5: Deployment\n",
        "\n",
        "### Deployment Architecture\n",
        "\n",
        "```\n",
        "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
        "â”‚                         PRODUCTION DEPLOYMENT                                    â”‚\n",
        "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
        "â”‚                                                                                  â”‚\n",
        "â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚\n",
        "â”‚   â”‚                      DOCKER COMPOSE STACK                                â”‚   â”‚\n",
        "â”‚   â”‚                                                                          â”‚   â”‚\n",
        "â”‚   â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                  â”‚   â”‚\n",
        "â”‚   â”‚   â”‚    NGINX     â”‚  â”‚    DSPM      â”‚  â”‚   UBA_PRO    â”‚                  â”‚   â”‚\n",
        "â”‚   â”‚   â”‚   (Proxy)    â”‚  â”‚    API       â”‚  â”‚   (Included  â”‚                  â”‚   â”‚\n",
        "â”‚   â”‚   â”‚              â”‚  â”‚              â”‚  â”‚   in DSPM)   â”‚                  â”‚   â”‚\n",
        "â”‚   â”‚   â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                  â”‚   â”‚\n",
        "â”‚   â”‚          â”‚                 â”‚                                             â”‚   â”‚\n",
        "â”‚   â”‚          â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                             â”‚   â”‚\n",
        "â”‚   â”‚                   â”‚                                                      â”‚   â”‚\n",
        "â”‚   â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                   â”‚   â”‚\n",
        "â”‚   â”‚   â”‚        PostgreSQL             â”‚  â”‚    Redis     â”‚                   â”‚   â”‚\n",
        "â”‚   â”‚   â”‚        (Database)             â”‚  â”‚   (Cache)    â”‚                   â”‚   â”‚\n",
        "â”‚   â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                   â”‚   â”‚\n",
        "â”‚   â”‚                                                                          â”‚   â”‚\n",
        "â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚\n",
        "â”‚                                                                                  â”‚\n",
        "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
        "```\n",
        "\n",
        "### Implementation Tasks:\n",
        "\n",
        "**Task 12.5.1: Create Docker Configuration**\n",
        "- Dockerfile for UBA (if separate from DSPM)\n",
        "- docker-compose.yml updates\n",
        "- Environment variables\n",
        "\n",
        "**Task 12.5.2: Database Migrations**\n",
        "- Create Alembic migrations for UBA tables\n",
        "- Test migration on staging\n",
        "- Plan rollback strategy\n",
        "\n",
        "**Task 12.5.3: Environment Configuration**\n",
        "\n",
        "```yaml\n",
        "# .env.production\n",
        "UBA_ENABLED=true\n",
        "UBA_ANALYSIS_INTERVAL=3600  # 1 hour\n",
        "UBA_RISK_THRESHOLD=50\n",
        "UBA_ALERT_THRESHOLD=70\n",
        "UBA_MODEL_PATH=/app/models/\n",
        "```\n",
        "\n",
        "**Task 12.5.4: Monitoring Setup**\n",
        "- Health check endpoints\n",
        "- Logging configuration\n",
        "- Metrics (Prometheus/Grafana if available)\n",
        "\n",
        "**Task 12.5.5: Deployment Checklist**\n",
        "\n",
        "```\n",
        "PRE-DEPLOYMENT:\n",
        "â–¡ All tests passing\n",
        "â–¡ Code reviewed\n",
        "â–¡ Documentation updated\n",
        "â–¡ Database migrations tested\n",
        "â–¡ Rollback plan ready\n",
        "\n",
        "DEPLOYMENT:\n",
        "â–¡ Backup database\n",
        "â–¡ Run database migrations\n",
        "â–¡ Deploy new code\n",
        "â–¡ Verify health checks\n",
        "â–¡ Run smoke tests\n",
        "\n",
        "POST-DEPLOYMENT:\n",
        "â–¡ Monitor error rates\n",
        "â–¡ Check performance metrics\n",
        "â–¡ Verify scheduled jobs running\n",
        "â–¡ Test key workflows manually\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## Step 12.6: Documentation\n",
        "\n",
        "### Implementation Tasks:\n",
        "\n",
        "**Task 12.6.1: API Documentation**\n",
        "- FastAPI auto-generates OpenAPI docs\n",
        "- Add descriptions to all endpoints\n",
        "- Include request/response examples\n",
        "\n",
        "**Task 12.6.2: User Guide**\n",
        "- How to use UBA dashboard\n",
        "- Understanding risk scores\n",
        "- Managing alerts and cases\n",
        "\n",
        "**Task 12.6.3: Admin Guide**\n",
        "- Configuration options\n",
        "- Rule management\n",
        "- Model retraining\n",
        "\n",
        "**Task 12.6.4: Developer Guide**\n",
        "- Architecture overview\n",
        "- Adding new models\n",
        "- Adding new rules\n",
        "- API integration\n",
        "\n",
        "---\n",
        "\n",
        "## ðŸ“ END OF PHASE 12 - ROADMAP COMPLETE!\n",
        "\n",
        "---\n",
        "\n",
        "# ðŸŽ‰ COMPLETE UBA_PRO ROADMAP SUMMARY\n",
        "\n",
        "| Phase | Name | Key Deliverables |\n",
        "|-------|------|------------------|\n",
        "| 1 | Foundation | Project structure, DB models, config |\n",
        "| 2 | Core Utilities | Logging, exceptions, validation |\n",
        "| 3 | Data Layer | Repositories, migrations, queries |\n",
        "| 4 | User Management | User profiles, baselines |\n",
        "| 5 | Feature Extraction | 50+ behavioral features |\n",
        "| 6 | ML Model Development | 4 anomaly detection models |\n",
        "| 7 | Analytics Engine | Model execution, risk calculator |\n",
        "| 8 | Alert & Case System | Alerts, cases, workflow |\n",
        "| 9 | Rule Engine | Rule-based detection, MITRE |\n",
        "| 10 | API Routes | REST API, schemas |\n",
        "| 11 | DSPM Integration | Data sync, event handling |\n",
        "| 12 | Testing & Deployment | Tests, deployment, docs |\n",
        "\n",
        "---\n",
        "\n",
        "## What You'll Have at the End:\n",
        "\n",
        "```\n",
        "UBA_PRO CAPABILITIES:\n",
        "\n",
        "âœ… User Behavioral Profiling\n",
        "   - Automatic baseline establishment\n",
        "   - Continuous behavior tracking\n",
        "   - Historical trend analysis\n",
        "\n",
        "âœ… ML-Based Anomaly Detection\n",
        "   - Login anomaly detection\n",
        "   - Access pattern anomalies\n",
        "   - Data transfer anomalies\n",
        "   - Time-based anomalies\n",
        "\n",
        "âœ… Risk Scoring System\n",
        "   - Composite risk scores (0-100)\n",
        "   - Risk trend tracking\n",
        "   - Risk level classification\n",
        "\n",
        "âœ… Alert Management\n",
        "   - Automated alert generation\n",
        "   - Priority-based queue\n",
        "   - Analyst assignment\n",
        "   - SLA tracking\n",
        "\n",
        "âœ… Case Management\n",
        "   - Group related alerts\n",
        "   - Investigation workflow\n",
        "   - Resolution tracking\n",
        "   - Audit trail\n",
        "\n",
        "âœ… Rule Engine\n",
        "   - 10+ built-in rules\n",
        "   - Custom rule creation\n",
        "   - MITRE ATT&CK mapping\n",
        "\n",
        "âœ… DSPM Integration\n",
        "   - Seamless data flow\n",
        "   - Real-time event processing\n",
        "   - Unified UI experience\n",
        "\n",
        "âœ… Full API Coverage\n",
        "   - 30+ REST endpoints\n",
        "   - Complete documentation\n",
        "   - Authentication/Authorization\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## Next Steps After Roadmap:\n",
        "\n",
        "1. **Present to CTO** - Use this roadmap\n",
        "2. **Get Approval** - Confirm resources\n",
        "3. **Start Phase 1** - Follow step-by-step\n",
        "4. **Regular Updates** - Track progress against roadmap\n",
        "5. **Iterate** - Adjust based on learnings\n",
        "\n",
        "---\n",
        "\n",
        "## ðŸ’¡ SUCCESS TIPS:\n",
        "\n",
        "1. **Follow the phases in order** - Each builds on previous\n",
        "2. **Test as you go** - Don't save testing for end\n",
        "3. **Get feedback early** - Demo to stakeholders after each phase\n",
        "4. **Document decisions** - Future you will thank current you\n",
        "5. **Start simple** - Get basic working before optimizing\n",
        "\n",
        "---\n",
        "\n",
        "**ðŸŽ“ CONGRATULATIONS!**\n",
        "\n",
        "You now have a COMPLETE, DETAILED roadmap to build UBA_PRO from scratch.\n",
        "This is a professional-grade implementation plan that covers:\n",
        "- Architecture\n",
        "- Database design\n",
        "- Feature engineering\n",
        "- Machine learning\n",
        "- API development\n",
        "- Integration\n",
        "- Testing\n",
        "- Deployment\n",
        "\n",
        "Good luck with your implementation! ðŸš€"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}