{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ashish78905/OPTICONNECT_CALLL_CENTER_ANALYSIS-ASSIGNMENT/blob/main/TILL_SUPERVISED_ML.IPYNB\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kJx7xvazcESp"
      },
      "outputs": [],
      "source": [
        "# ========== 1. DATA LOADING ==========\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import load_diabetes"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load diabetes dataset\n",
        "diabetes = load_diabetes()\n",
        "print(diabetes.DESCR)"
      ],
      "metadata": {
        "id": "pXIBr839cHK7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create DataFrame from diabetes dataset\n",
        "data = pd.DataFrame(diabetes.data, columns=diabetes.feature_names)\n",
        "data['target'] = diabetes.target"
      ],
      "metadata": {
        "id": "L2vG1padcNOm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.scatter(data.bmi, data.target)\n",
        "plt.xlabel(\"bmi\")\n",
        "plt.ylabel(\"target\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "7t9yA9ICcSSp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b6c29273"
      },
      "source": [
        "# Define features (X) and target (y)\n",
        "X = data.drop('target', axis=1)\n",
        "y = data['target']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ========== 3. TRAIN-TEST SPLIT ==========\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=1\n",
        ")\n",
        "\n",
        "print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)"
      ],
      "metadata": {
        "id": "SFQYtdAWdFS0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ========== 4. SCALING (STANDARDIZATION) ==========\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "plt.scatter(X_train[:, 0], y_train)\n",
        "plt.xlabel(\"Scaled Age\")\n",
        "plt.ylabel(\"Target\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "_QdOs5tZdY6W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ========== 5. MODEL TRAINING ==========\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "model = LinearRegression()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "print(\"Coefficient:\", model.coef_)\n",
        "print(\"Intercept:\", model.intercept_)"
      ],
      "metadata": {
        "id": "xs5lu0ordqzy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ========== 6. MODEL PREDICTION ==========\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# On training data\n",
        "plt.scatter(X_train[:, 0], y_train)\n",
        "plt.plot(X_train[:, 0], model.predict(X_train), 'r')\n",
        "plt.xlabel(\"Scaled Age\")\n",
        "plt.ylabel(\"Target\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Z2mgpGCNdxlB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# On testing data\n",
        "y_pred_test = model.predict(X_test)\n",
        "plt.scatter(X_test[:, 0], y_test)\n",
        "plt.plot(X_test[:, 0], y_pred_test, 'r')\n",
        "plt.xlabel(\"Scaled Age\")\n",
        "plt.ylabel(\"Target\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "4vr-XmRweLRH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ========== 7. PERFORMANCE METRICS ==========\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "import numpy as np\n",
        "\n",
        "mse = mean_squared_error(y_test, y_pred_test)\n",
        "mae = mean_absolute_error(y_test, y_pred_test)\n",
        "rmse = np.sqrt(mse)\n",
        "\n",
        "print(\"MSE:\", mse)\n",
        "print(\"MAE:\", mae)\n",
        "print(\"RMSE:\", rmse)\n",
        "\n",
        "score = r2_score(y_test, y_pred_test)\n",
        "print(\"R2 Score:\", score)\n",
        "\n",
        "adj_r2 = 1 - (1 - score) * (len(y_test) - 1) / (len(y_test) - X_test.shape[1] - 1)\n",
        "print(\"Adjusted R2:\", adj_r2)"
      ],
      "metadata": {
        "id": "-VDBbcaGela0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ========== 8. VISUALIZATION ==========\n",
        "plt.scatter(X_test[:, 0], y_test, color='black', label='Actual data')\n",
        "plt.plot(X_test[:, 0], y_pred_test, color='blue', linewidth=3, label=\"Linear regression line\")\n",
        "plt.xlabel(\"Scaled Age\")\n",
        "plt.ylabel(\"One year progression-target\")\n",
        "plt.title(\"Linear regression on diabetes data\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "aWz1vSYnesKh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ========== 9. MODEL ASSUMPTIONS & RESIDUALS ==========\n",
        "# Residuals\n",
        "plt.scatter(y_test, y_pred_test)\n",
        "plt.xlabel(\"Actual\")\n",
        "plt.ylabel(\"Predicted\")\n",
        "plt.title(\"Residuals plot\")\n",
        "plt.show()\n",
        "\n",
        "error = y_test - y_pred_test\n",
        "print(\"Residual errors:\\n\", error)"
      ],
      "metadata": {
        "id": "_DprmQsQe6hL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MULTIPLE LINEAR REGRESSSION"
      ],
      "metadata": {
        "id": "9C5eIPizoDMM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ========== 1. DATA LOADING ==========\n",
        "from sklearn.datasets import load_diabetes\n",
        "diabetes = load_diabetes()\n",
        "print(diabetes.DESCR)\n",
        "diabetes.data\n",
        "diabetes.target\n",
        "diabetes.feature_names\n",
        "import pandas as pd\n",
        "data = pd.DataFrame(diabetes.data, columns = diabetes.feature_names)\n",
        "data\n",
        "data['target'] = diabetes.target"
      ],
      "metadata": {
        "id": "_stqK8dWoHLt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ========== 2. EDA & DATA PREPARATION ==========\n",
        "import seaborn as sns\n",
        "\n",
        "# Divide into X (features) and y (target)\n",
        "# Define the independent features (X) by dropping the 'target' column\n",
        "X = data.drop('target', axis = 1)\n",
        "# Define the dependent feature (y) as the 'target' column\n",
        "y = data['target']\n",
        "\n",
        "# EDA on the data DataFrame\n",
        "print(\"Data Info:\")\n",
        "data.info()\n",
        "print(\"\\nData Types:\")\n",
        "print(data.dtypes)\n",
        "print(\"\\nHead of Data:\")\n",
        "display(data.head())\n",
        "print(\"\\nTail of Data:\")\n",
        "display(data.tail())\n",
        "print(\"\\nSample of Data:\")\n",
        "display(data.sample(3))\n",
        "print(\"\\nMissing Values:\")\n",
        "print(data.isnull().sum())\n",
        "print(\"\\nData Description:\")\n",
        "display(data.describe())\n",
        "print(\"\\nCorrelation Matrix:\")\n",
        "display(data.corr())\n",
        "\n",
        "# Heatmap of the correlation matrix\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(data.corr(), annot=True, cmap='coolwarm')\n",
        "plt.title('Correlation Heatmap')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "UB_xt43npeUF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ========== 3. TRAIN-TEST SPLIT ==========\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.30, random_state=1)\n",
        "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
      ],
      "metadata": {
        "id": "YHEW3g8Ipjkt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ========== 4. SCALING ==========\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "X_train\n",
        "X_test"
      ],
      "metadata": {
        "id": "r6ZsrHZypmlX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ========== 5. MODEL TRAINING ==========\n",
        "from sklearn.linear_model import LinearRegression\n",
        "model = LinearRegression()\n",
        "model.fit(X_train, y_train)\n",
        "X_train.shape\n",
        "len(model.coef_)\n",
        "model.coef_\n",
        "model.intercept_"
      ],
      "metadata": {
        "id": "OENOytbBpwy7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ========== 6. MODEL PREDICTION ==========\n",
        "y_pred = model.predict(X_test)"
      ],
      "metadata": {
        "id": "X8-OtosDp9rM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ========== 7. PERFORMANCE METRICS ==========\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "import numpy as np\n",
        "\n",
        "print(mean_squared_error(y_test, y_pred))\n",
        "print(mean_absolute_error(y_test, y_pred))\n",
        "print(np.sqrt(mean_squared_error(y_test, y_pred)))\n",
        "\n",
        "score = r2_score(y_test, y_pred)\n",
        "1 - (1 - score) * (len(y_test) - 1) / (len(y_test) - X_test.shape[1] - 1)"
      ],
      "metadata": {
        "id": "8y0VNl3_qE1l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ========== 8. MODEL ASSUMPTIONS & RESIDUALS ==========\n",
        "plt.scatter(y_test, y_pred)\n",
        "\n",
        "error = y_test - y_pred\n",
        "sns.distplot(error)\n",
        "plt.scatter(y_pred, error)"
      ],
      "metadata": {
        "id": "FtsZepegrXQ6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PICKLING MODEL"
      ],
      "metadata": {
        "id": "nM6JVvgpr336"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle   # Importing pickle module for saving/loading Python objects\n",
        "\n",
        "# Saving the trained model into a file named \"model.pkl\"\n",
        "# pickle.dump(object, file, mode)\n",
        "# object = model → the trained LinearRegression model\n",
        "# open(\"model.pkl\", \"wb\") → open file in write-binary mode to store bytes\n",
        "pickle.dump(model, open(\"model.pkl\", \"wb\"))  # write binary mode\n",
        "\n",
        "# Explanation:\n",
        "# Python object (here: model), along with its attributes (coefficients, intercept, etc.)\n",
        "# and methods, is converted into a byte stream and saved into a file.\n",
        "# Later, we can load this model back using pickle.load() without retraining.\n",
        "\n",
        "\n",
        "\n",
        "# Loading the saved model from the file \"model.pkl\"\n",
        "\n",
        "# pickle.load(file) → reads the byte stream and reconstructs the original Python object\n",
        "# open(\"model.pkl\", \"rb\") → open file in read-binary mode\n",
        "model = pickle.load(open(\"model.pkl\", 'rb'))\n",
        "\n",
        "# Now 'model' is the same trained LinearRegression model we saved earlier.\n",
        "# We can directly use it for predictions without retraining.\n",
        "\n",
        "\n",
        "\n",
        "# Using the loaded model to make predictions on the test set\n",
        "\n",
        "# model.predict(X_test) → applies the learned coefficients & intercept\n",
        "# to the features in X_test and returns predicted values for target variable\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Now y_pred contains the predictions made by the trained (or loaded) model\n"
      ],
      "metadata": {
        "id": "fEnXTsyGsDm_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# polynomial Regression"
      ],
      "metadata": {
        "id": "770sKZ4AsKF2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ========== 1. DATA GENERATION ==========\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "np.random.seed(1)\n",
        "X = 2 * np.random.rand(100, 1)\n",
        "y = 4 + 3*X + 1.5*X**2 + np.random.randn(100, 1)\n",
        "X\n",
        "y"
      ],
      "metadata": {
        "id": "FEI7lyo4sOS4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ========== 2. TRAIN-TEST SPLIT ==========\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "X_train.shape, X_test.shape\n"
      ],
      "metadata": {
        "id": "7JotGP2QtGbD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ========== 3. POLYNOMIAL TRANSFORMATION ==========\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "degree = 2\n",
        "poly_features = PolynomialFeatures(degree=2, include_bias=False)\n",
        "X_poly_train = poly_features.fit_transform(X_train)"
      ],
      "metadata": {
        "id": "mCZXRwJctJWg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ========== 4. MODEL TRAINING ==========\n",
        "poly_reg = LinearRegression()\n",
        "poly_reg.fit(X_poly_train, y_train)\n",
        "poly_reg.coef_\n",
        "poly_reg.intercept_"
      ],
      "metadata": {
        "id": "6XObJOtktMQx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ========== 5. MODEL PREDICTION ==========\n",
        "y_poly_predict = poly_reg.predict(X_poly_train)"
      ],
      "metadata": {
        "id": "tW8tZh3NtOjS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ========== 6. PERFORMANCE EVALUATION ==========\n",
        "from sklearn.metrics import mean_squared_error\n",
        "mse_train = mean_squared_error(y_train, y_poly_predict)\n",
        "print(f'Mean Squared Error on Training Data: {mse_train}')"
      ],
      "metadata": {
        "id": "z_0wAypWtQ0Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ========== 7. VISUALIZATION ==========\n",
        "plt.scatter(X_train, y_train, label=\"training data\")\n",
        "X_range = np.linspace(0, 2, 100).reshape(-1, 1)\n",
        "X_range_poly = poly_features.transform(X_range)\n",
        "plt.plot(X_range, poly_reg.predict(X_range_poly), color='red',\n",
        "         label=f'Polynomial Regression (Degree {degree})')\n",
        "plt.xlabel('X')\n",
        "plt.ylabel('y')\n",
        "plt.title('Polynomial Regression')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "y0PcLecqtXOU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Multicollenearity"
      ],
      "metadata": {
        "id": "4MZ5zCUqte0D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ========== 1. LOADING DATASET ==========\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "import pandas as pd\n",
        "\n",
        "data = fetch_california_housing()\n",
        "df = pd.DataFrame(data.data, columns=data.feature_names)\n",
        "df['Price'] = data.target"
      ],
      "metadata": {
        "id": "uM80slqxtlTb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ========== 2. MULTICOLLINEARITY ANALYSIS ==========\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "sns.heatmap(df.corr(), annot=True, vmin=-1, vmax=1)\n",
        "plt.figure(figsize=(5, 5))\n",
        "sns.clustermap(df.corr(), vmin=-1, vmax=1, annot=True)"
      ],
      "metadata": {
        "id": "xdoIWc5Tt4dL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ========== 3. VARIANCE INFLATION FACTOR (VIF) ==========\n",
        "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
        "\n",
        "df1 = df.copy()\n",
        "df1.drop(\"Longitude\", axis=1, inplace=True)\n",
        "\n",
        "vif = pd.DataFrame()\n",
        "vif[\"Feature\"] = df1.columns\n",
        "vif[\"VIF\"] = [variance_inflation_factor(df1.values, i) for i in range(len(df1.columns))]\n",
        "vif\n"
      ],
      "metadata": {
        "id": "mau65pk4t-mn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df1.drop(\"AveRooms\", axis=1, inplace=True)\n",
        "vif = pd.DataFrame()\n",
        "vif[\"Feature\"] = df1.columns\n",
        "vif[\"VIF\"] = [variance_inflation_factor(df1.values, i) for i in range(len(df1.columns))]\n",
        "vif"
      ],
      "metadata": {
        "id": "eybkFbXiuAwW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df1.drop(\"Latitude\", axis=1, inplace=True)\n",
        "vif = pd.DataFrame()\n",
        "vif[\"Feature\"] = df1.columns\n",
        "vif[\"VIF\"] = [variance_inflation_factor(df1.values, i) for i in range(len(df1.columns))]\n",
        "vif"
      ],
      "metadata": {
        "id": "vyBH8EibuC3u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ========== 4. FEATURE SELECTION (X & y) ==========\n",
        "X = df1.iloc[:, :-1]\n",
        "y = df1.iloc[:, -1]"
      ],
      "metadata": {
        "id": "snXMHWOGuFwd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ========== 5. RFE (RECURSIVE FEATURE ELIMINATION) ==========\n",
        "X = df.iloc[:, :-1]\n",
        "y = df.iloc[:, -1]\n",
        "X.columns\n",
        "\n",
        "from sklearn.feature_selection import RFE\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "X.columns\n",
        "rfe = RFE(estimator=LinearRegression(), n_features_to_select=5)\n",
        "rfe.fit(X, y)\n",
        "rfe.predict(X)\n",
        "\n",
        "print(rfe.support_)\n",
        "print(rfe.ranking_)\n",
        "\n",
        "selected_features = X.columns[rfe.support_]\n",
        "print(\"Selected Features:\", selected_features.tolist())"
      ],
      "metadata": {
        "id": "MH6dPjOquLI6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LASSO RIDGE AND ELASTIC NET EMPLIMENATION"
      ],
      "metadata": {
        "id": "jiz7qzTLwjMb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------- LOAD DATASET --------------------\n",
        "import seaborn as sns\n",
        "df = sns.load_dataset('mpg')\n",
        "df.head()\n",
        "df.drop(\"name\", axis = 1, inplace = True)\n",
        "df.isna().sum()\n",
        "df['horsepower'] = df['horsepower'].fillna(df['horsepower'].median())\n",
        "df.isna().sum()\n",
        "df.info()\n",
        "df.dtypes\n",
        "df['origin'].value_counts()\n",
        "df['origin'] = df['origin'].map({'usa': 1, \"japan\": 2, \"europe\": 3})\n",
        "df['origin'] = df['origin'].astype(int)"
      ],
      "metadata": {
        "id": "tYuYr_jBwoSA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------- FEATURE & TARGET SPLIT --------------------\n",
        "X = df.drop('mpg', axis=1)\n",
        "y = df['mpg']\n",
        "X\n",
        "y"
      ],
      "metadata": {
        "id": "Ck87aJ_6wx_S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------- TRAIN TEST SPLIT --------------------\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state=1)\n",
        "X_train.shape, X_test.shape\n"
      ],
      "metadata": {
        "id": "sdpBySBPw0UK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------- LINEAR REGRESSION --------------------\n",
        "from sklearn.linear_model import LinearRegression\n",
        "regression_model = LinearRegression()\n",
        "regression_model.fit(X_train, y_train)\n",
        "for i, col_name in enumerate(X_train.columns):\n",
        "    print(f\"The coefficient for {col_name} is {regression_model.coef_[i]}\")\n",
        "\n",
        "from sklearn.metrics import r2_score\n",
        "y_pred_linear = regression_model.predict(X_test)\n",
        "r2_linear = r2_score(y_test, y_pred_linear)\n",
        "print(f\"R square of linear regression {r2_linear}\")"
      ],
      "metadata": {
        "id": "-iIL_R_sw5Lr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------- RIDGE REGRESSION --------------------\n",
        "from sklearn.linear_model import Ridge\n",
        "ridge_regression_model = Ridge(alpha = 0.1)\n",
        "ridge_regression_model.fit(X_train, y_train)\n",
        "for i, col_name in enumerate(X_train.columns):\n",
        "    print(f\"The coefficient for {col_name} is {ridge_regression_model.coef_[i]}\")\n",
        "y_pred_ridge = ridge_regression_model.predict(X_test)\n",
        "r2_ridge = r2_score(y_test, y_pred_ridge)\n",
        "print(f\"R-squared score for Ridge Regression: {r2_ridge}\")"
      ],
      "metadata": {
        "id": "ZTbgvKdAw6Wl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------- LASSO REGRESSION --------------------\n",
        "from sklearn.linear_model import Lasso\n",
        "lasso_regression_model = Lasso(alpha = 0.5)\n",
        "lasso_regression_model.fit(X_train, y_train)\n",
        "for i, col_name in enumerate(X_train.columns):\n",
        "    print(f\"The coefficient for {col_name} is {lasso_regression_model.coef_[i]}\")\n",
        "y_pred_lasso = lasso_regression_model.predict(X_test)\n",
        "r2_lasso = r2_score(y_test, y_pred_lasso)\n",
        "print(f\"R-squared score for Lasso Regression: {r2_lasso}\")"
      ],
      "metadata": {
        "id": "iTYzAZoWw9Z6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------- ELASTIC NET REGRESSION --------------------\n",
        "from sklearn.linear_model import ElasticNet\n",
        "elastic_net_model = ElasticNet(alpha = 1, l1_ratio = 0.5)\n",
        "elastic_net_model.fit(X_train, y_train)\n",
        "for i, col_name in enumerate(X_train.columns):\n",
        "    print(f\"The coefficient for {col_name} is {elastic_net_model.coef_[i]}\")\n",
        "y_pred_elastic_net = elastic_net_model.predict(X_test)\n",
        "r2_elastic_net = r2_score(y_test, y_pred_elastic_net)\n",
        "print(f\"R-squared score for Elastic Net Regression: {r2_elastic_net}\")"
      ],
      "metadata": {
        "id": "iTANFWq3w_tV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------- LASSO WITH CROSS VALIDATION --------------------\n",
        "from sklearn.linear_model import LassoCV\n",
        "lassocv = LassoCV(cv=5)\n",
        "lassocv.fit(X_train, y_train)\n",
        "y_pred_lassocv = lassocv.predict(X_test)\n",
        "score_lassocv = r2_score(y_test, y_pred_lassocv)\n",
        "print(\"Best alpha chosen by LassoCV:\", lassocv.alpha_)\n",
        "print(\"R2 Score (LassoCV):\", score_lassocv)"
      ],
      "metadata": {
        "id": "Fhmrc6kTxGke"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------- RIDGE WITH CROSS VALIDATION --------------------\n",
        "from sklearn.linear_model import RidgeCV\n",
        "ridgecv = RidgeCV(cv=5)\n",
        "ridgecv.fit(X_train, y_train)\n",
        "y_pred_ridgecv = ridgecv.predict(X_test)\n",
        "score_ridgecv = r2_score(y_test, y_pred_ridgecv)\n",
        "print(\"Best alpha chosen by RidgeCV:\", ridgecv.alpha_)\n",
        "print(\"R2 Score (RidgeCV):\", score_ridgecv)\n",
        "print(\"RidgeCV Parameters:\", ridgecv.get_params())"
      ],
      "metadata": {
        "id": "6Fj-2SSKxKhh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LOGISTIC REGRESSION"
      ],
      "metadata": {
        "id": "TLZdivjPZjXe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------- MASTER LOGISTIC REGRESSION PIPELINE ----------------\n",
        "# This single, unified script combines all the code and concepts from the three\n",
        "# Logistic Regression examples in your file into one complete workflow.\n",
        "\n",
        "# Import required libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import warnings\n",
        "from sklearn.datasets import load_iris, make_classification\n",
        "from sklearn.model_selection import train_test_split, KFold, cross_val_score, GridSearchCV, RandomizedSearchCV\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report, roc_curve, auc, precision_score, recall_score\n",
        "\n",
        "warnings.filterwarnings('ignore')  # Ignore warnings for cleaner output\n",
        "\n",
        "# ====================================================================================\n",
        "# --- STEP 1: LOAD, PREPARE, AND EXPLORE THE DATA (FROM IRIS EXAMPLE) ---\n",
        "# ====================================================================================\n",
        "print(\"--- Step 1: Loading and Preparing the Iris Dataset for Binary Classification ---\")\n",
        "\n",
        "# ---------------- 1.1 LOAD & EXPLORE IRIS DATA ----------------\n",
        "data_iris = load_iris()\n",
        "df_iris = pd.DataFrame(data_iris.data, columns=data_iris.feature_names)\n",
        "df_iris['target'] = data_iris.target\n",
        "print(\"Iris Data Head:\")\n",
        "print(df_iris.head())\n",
        "print(\"\\nUnique Target Classes:\", df_iris['target'].unique())\n",
        "\n",
        "# ---------------- 1.2 PREPARE FOR BINARY CLASSIFICATION ----------------\n",
        "# We will convert this to a binary problem by removing class '2' (Virginica)\n",
        "df_final = df_iris[df_iris['target'] != 2]\n",
        "X = df_final.iloc[:, :-1]\n",
        "y = df_final.iloc[:, -1]\n",
        "print(\"\\nData prepared for binary classification. Unique classes remaining:\", y.unique())\n",
        "\n",
        "# ---------------- 1.3 SPLIT DATA INTO TRAINING AND TESTING SETS ----------------\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=1)\n",
        "print(f\"\\nData split into {X_train.shape[0]} training samples and {X_test.shape[0]} testing samples.\")\n",
        "print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
        "\n",
        "\n",
        "# ====================================================================================\n",
        "# --- STEP 2: INITIAL MODEL TRAINING AND EVALUATION ---\n",
        "# ====================================================================================\n",
        "print(\"--- Step 2: Training an Initial Logistic Regression Model ---\")\n",
        "\n",
        "# ---------------- 2.1 TRAIN THE MODEL ----------------\n",
        "classifier = LogisticRegression()\n",
        "classifier.fit(X_train, y_train)\n",
        "y_pred = classifier.predict(X_test)\n",
        "y_pred_proba = classifier.predict_proba(X_test)[:, 1] # Get probabilities for the positive class\n",
        "print(\"Model trained and initial predictions are made.\")\n",
        "\n",
        "# ---------------- 2.2 EVALUATION METRICS ----------------\n",
        "print(\"\\nStep 2.2: Evaluating the model with standard metrics...\")\n",
        "print(\"Confusion Matrix:\")\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "print(\"\\nAccuracy Score:\", accuracy_score(y_test, y_pred))\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "# ---------------- 2.3 ROC CURVE & AUC ----------------\n",
        "print(\"\\nStep 2.3: Plotting the ROC Curve and calculating AUC...\")\n",
        "fpr, tpr, thresholds_roc = roc_curve(y_test, y_pred_proba)\n",
        "roc_auc = auc(fpr, tpr)\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(fpr, tpr, color='darkorange', linewidth=2, label='ROC curve (area = %0.2f)' % roc_auc)\n",
        "plt.plot([0, 1], [0, 1], color='navy', linewidth=2, linestyle='--')\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# ---------------- 2.4 K-FOLD CROSS-VALIDATION ----------------\n",
        "print(\"\\nStep 2.4: Performing K-Fold Cross-Validation (k=5) for robustness check...\")\n",
        "cv = KFold(n_splits=5)\n",
        "scores = cross_val_score(classifier, X_train, y_train, cv=cv, scoring=\"accuracy\")\n",
        "print(f\"Accuracy scores for each fold: {scores}\")\n",
        "print(f\"Mean Accuracy from Cross-Validation: {np.mean(scores):.4f}\")\n",
        "print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
        "\n",
        "\n",
        "# ======================================================================================\n",
        "# --- STEP 3: THRESHOLD ANALYSIS (FROM SYNTHETIC DATA EXAMPLE) ---\n",
        "# ======================================================================================\n",
        "print(\"--- Step 3: Analyzing the Precision-Recall-Accuracy Tradeoff ---\")\n",
        "\n",
        "# ---------------- 3.1 PRECISION-RECALL TRADEOFF ANALYSIS ----------------\n",
        "print(\"\\nStep 3.1: Plotting metrics vs. different classification thresholds...\")\n",
        "thresholds_tradeoff = np.linspace(0, 1, 100)\n",
        "precisions, recalls, accuracies = [], [], []\n",
        "for threshold in thresholds_tradeoff:\n",
        "    y_pred_threshold = (y_pred_proba >= threshold).astype(int)\n",
        "    precisions.append(precision_score(y_test, y_pred_threshold, zero_division=0))\n",
        "    recalls.append(recall_score(y_test, y_pred_threshold))\n",
        "    accuracies.append(accuracy_score(y_test, y_pred_threshold))\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(thresholds_tradeoff, precisions, label='Precision')\n",
        "plt.plot(thresholds_tradeoff, recalls, label='Recall')\n",
        "plt.plot(thresholds_tradeoff, accuracies, label='Accuracy')\n",
        "plt.xlabel('Threshold Probability')\n",
        "plt.ylabel('Score')\n",
        "plt.title('Precision, Recall, and Accuracy vs. Threshold Probability')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# ---------------- 3.2 EVALUATE WITH A CUSTOM THRESHOLD ----------------\n",
        "print(\"\\nStep 3.2: Re-evaluating model with an optimal custom threshold (e.g., 0.4)...\")\n",
        "# Based on the plot, we might choose a threshold that balances precision and recall.\n",
        "custom_threshold = 0.4\n",
        "new_pred_levels = np.where(y_pred_proba > custom_threshold, 1, 0)\n",
        "print(f\"Classification Report (custom threshold = {custom_threshold}):\")\n",
        "print(classification_report(y_test, new_pred_levels))\n",
        "print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
        "\n",
        "\n",
        "# ======================================================================================\n",
        "# --- STEP 4: HYPERPARAMETER TUNING (FROM GRIDSEARCHCV & RANDOMIZEDSEARCHCV EXAMPLE) ---\n",
        "# ======================================================================================\n",
        "print(\"--- Step 4: Finding the Best Model Parameters with Hyperparameter Tuning ---\")\n",
        "\n",
        "# ---------------- 4.1 HYPERPARAMETER TUNING WITH GRIDSEARCHCV ----------------\n",
        "print(\"\\nStep 4.1: Performing an exhaustive search with GridSearchCV...\")\n",
        "params = {'penalty': ['l1', 'l2', 'elasticnet'], 'C': [1, 10, 20, 30, 40]}\n",
        "base_model = LogisticRegression(solver='liblinear') # 'l1' requires 'liblinear' solver\n",
        "clf_grid = GridSearchCV(base_model, param_grid=params, cv=5)\n",
        "clf_grid.fit(X_train, y_train)\n",
        "print(f\"Best Parameters from GridSearchCV: {clf_grid.best_params_}\")\n",
        "print(f\"Best Score from GridSearchCV: {clf_grid.best_score_:.4f}\")\n",
        "\n",
        "# ---------------- 4.2 HYPERPARAMETER TUNING WITH RANDOMIZEDSEARCHCV ----------------\n",
        "print(\"\\nStep 4.2: Performing a randomized search with RandomizedSearchCV...\")\n",
        "randomized_clf = RandomizedSearchCV(base_model, param_distributions=params, cv=5, n_iter=10)\n",
        "randomized_clf.fit(X_train, y_train)\n",
        "print(f\"Best Parameters from RandomizedSearchCV: {randomized_clf.best_params_}\")\n",
        "print(f\"Best Score from RandomizedSearchCV: {randomized_clf.best_score_:.4f}\")\n",
        "\n",
        "# ---------------- 4.3 EVALUATE THE FINAL, TUNED MODEL ----------------\n",
        "print(\"\\nStep 4.3: Evaluating the final model with the best found parameters...\")\n",
        "# Using the best parameters found by GridSearchCV to build the final model.\n",
        "final_model = LogisticRegression(**clf_grid.best_params_, solver='liblinear')\n",
        "final_model.fit(X_train, y_train)\n",
        "y_pred_final = final_model.predict(X_test)\n",
        "print(\"Final Classification Report (after tuning):\")\n",
        "print(classification_report(y_test, y_pred_final))\n",
        "print(\"\\n--- MASTER PIPELINE COMPLETE ---\")\n",
        "\n",
        "# ---------------- END OF SCRIPT ----------------\n",
        "\n"
      ],
      "metadata": {
        "id": "1bautpUTaTM7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------- MASTER MULTICLASS LOGISTIC REGRESSION PIPELINE ----------------\n",
        "# This single, unified script combines all the code and concepts from your\n",
        "# Multiclass Logistic Regression examples into one complete workflow.\n",
        "\n",
        "# Import required libraries\n",
        "import pandas as pd\n",
        "import warnings\n",
        "from sklearn.datasets import load_iris, make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report\n",
        "\n",
        "warnings.filterwarnings('ignore')  # Ignore warnings for cleaner output\n",
        "\n",
        "# ====================================================================================\n",
        "# --- STEP 1: LOAD, PREPARE, AND EXPLORE THE IRIS DATASET ---\n",
        "# ====================================================================================\n",
        "print(\"--- Step 1: Loading and Preparing the Iris Dataset for Multiclass Classification ---\")\n",
        "\n",
        "# ---------------- 1.1 LOAD & EXPLORE IRIS DATA ----------------\n",
        "data = load_iris()\n",
        "print(\"Available keys in the dataset object:\", data.keys())\n",
        "# print(\"\\nDataset Description:\")\n",
        "# print(data.DESCR) # This can be very long, so it's commented out for cleaner output\n",
        "\n",
        "# ---------------- 1.2 CREATE AND EXPLORE DATAFRAME ----------------\n",
        "df = pd.DataFrame(data.data, columns=data.feature_names)\n",
        "df['target'] = data.target\n",
        "print(\"\\nIris Data Head:\")\n",
        "print(df.head())\n",
        "print(\"\\nUnique Target Classes:\", df.target.unique())\n",
        "\n",
        "# ---------------- 1.3 SEPARATE FEATURES AND TARGET ----------------\n",
        "X = df.iloc[:, :-1]  # All columns except the last are features\n",
        "y = df.iloc[:, -1]   # The last column is the target\n",
        "print(\"\\nFeatures (X) and target (y) have been separated.\")\n",
        "\n",
        "# ---------------- 1.4 SPLIT DATA INTO TRAINING AND TESTING SETS ----------------\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=1)\n",
        "print(f\"\\nData split into {X_train.shape[0]} training samples and {X_test.shape[0]} testing samples.\")\n",
        "print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
        "\n",
        "\n",
        "# ====================================================================================\n",
        "# --- STEP 2: MODELING WITH 'ONE-VS-REST' (OVR) STRATEGY ---\n",
        "# ====================================================================================\n",
        "print(\"--- Step 2: Training a Model with the 'One-vs-Rest' (OVR) Strategy ---\")\n",
        "# The OVR strategy fits one classifier per class against all other classes.\n",
        "\n",
        "# ---------------- 2.1 TRAIN THE OVR MODEL ----------------\n",
        "# Note: By default, LogisticRegression uses 'auto' which often selects 'ovr' for multiclass.\n",
        "# We will be explicit here for clarity.\n",
        "ovr_model = LogisticRegression(multi_class='ovr', solver='lbfgs', max_iter=300)\n",
        "ovr_model.fit(X_train, y_train)\n",
        "print(\"OVR model trained successfully.\")\n",
        "\n",
        "# ---------------- 2.2 MAKE PREDICTIONS ----------------\n",
        "y_pred_ovr = ovr_model.predict(X_test)\n",
        "print(\"\\nFirst 10 predictions from OVR model:\", y_pred_ovr[:10])\n",
        "# print(\"\\nPrediction probabilities from OVR model:\\n\", ovr_model.predict_proba(X_test)[:5])\n",
        "\n",
        "# ---------------- 2.3 EVALUATE THE OVR MODEL ----------------\n",
        "print(\"\\n--- OVR Model Evaluation ---\")\n",
        "print(\"Confusion Matrix:\")\n",
        "print(confusion_matrix(y_test, y_pred_ovr))\n",
        "print(\"\\nAccuracy Score:\", accuracy_score(y_test, y_pred_ovr))\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred_ovr))\n",
        "print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
        "\n",
        "\n",
        "# ====================================================================================\n",
        "# --- STEP 3: MODELING WITH 'MULTINOMIAL' STRATEGY ---\n",
        "# ====================================================================================\n",
        "print(\"--- Step 3: Training a Model with the 'Multinomial' Strategy ---\")\n",
        "# The Multinomial strategy considers all classes at once in a single model.\n",
        "\n",
        "# ---------------- 3.1 TRAIN THE MULTINOMIAL MODEL ----------------\n",
        "multinomial_model = LogisticRegression(multi_class='multinomial', solver='lbfgs', max_iter=300)\n",
        "multinomial_model.fit(X_train, y_train)\n",
        "print(\"Multinomial model trained successfully.\")\n",
        "\n",
        "# ---------------- 3.2 MAKE PREDICTIONS ----------------\n",
        "y_pred_multinomial = multinomial_model.predict(X_test)\n",
        "print(\"\\nFirst 10 predictions from Multinomial model:\", y_pred_multinomial[:10])\n",
        "# print(\"\\nPrediction probabilities from Multinomial model:\\n\", multinomial_model.predict_proba(X_test)[:5])\n",
        "\n",
        "# ---------------- 3.3 EVALUATE THE MULTINOMIAL MODEL ----------------\n",
        "print(\"\\n--- Multinomial Model Evaluation ---\")\n",
        "print(\"Confusion Matrix:\")\n",
        "print(confusion_matrix(y_test, y_pred_multinomial))\n",
        "print(\"\\nAccuracy Score:\", accuracy_score(y_test, y_pred_multinomial))\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred_multinomial))\n",
        "print(\"\\nNOTE: The ROC AUC curve is typically used for binary classification problems.\")\n",
        "print(\"\\n--- MASTER PIPELINE COMPLETE ---\")\n",
        "\n",
        "# ---------------- END OF SCRIPT ----------------\n"
      ],
      "metadata": {
        "id": "hDYXF95BlfFg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DECISION TREE CLASSIFIER"
      ],
      "metadata": {
        "id": "8SXfRIQQmuMT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------- MASTER DECISION TREE CLASSIFIER PIPELINE ----------------\n",
        "# This single, unified script combines all the code and concepts from your\n",
        "# Decision Tree Classification examples into one complete workflow.\n",
        "\n",
        "# Import required libraries\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# ====================================================================================\n",
        "# --- STEP 1: LOAD, PREPARE, AND EXPLORE THE IRIS DATASET ---\n",
        "# ====================================================================================\n",
        "print(\"--- Step 1: Loading and Preparing the Iris Dataset ---\")\n",
        "\n",
        "# ---------------- 1.1 LOAD & EXPLORE IRIS DATA ----------------\n",
        "data = load_iris()\n",
        "# print(data.DESCR) # Uncomment to see the full description\n",
        "\n",
        "# ---------------- 1.2 CREATE DATAFRAME AND SEPARATE FEATURES/TARGET ----------------\n",
        "df = pd.DataFrame(data.data, columns=data.feature_names)\n",
        "X = df  # Feature matrix\n",
        "y = data.target  # Target vector\n",
        "print(\"Dataset loaded. Preview of features (X):\")\n",
        "print(X.head())\n",
        "print(\"\\nTarget (y) classes:\", y[:10], \"...\")\n",
        "\n",
        "# ---------------- 1.3 SPLIT DATA INTO TRAINING AND TESTING SETS ----------------\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)\n",
        "print(f\"\\nData split into {X_train.shape[0]} training samples and {X_test.shape[0]} testing samples.\")\n",
        "print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
        "\n",
        "\n",
        "# ====================================================================================\n",
        "# --- STEP 2: INITIAL MODEL AND POST-PRUNING ---\n",
        "# ====================================================================================\n",
        "print(\"--- Step 2: Training an Initial (Unpruned) Decision Tree ---\")\n",
        "\n",
        "# ---------------- 2.1 TRAIN AND VISUALIZE THE FULL TREE ----------------\n",
        "# This model is not constrained and can grow to its full depth, risking overfitting.\n",
        "unpruned_classifier = DecisionTreeClassifier(criterion='entropy')\n",
        "unpruned_classifier.fit(X_train, y_train)\n",
        "\n",
        "print(\"Visualizing the full, unpruned Decision Tree...\")\n",
        "plt.figure(figsize=(20, 15))\n",
        "plot_tree(unpruned_classifier, filled=True, feature_names=data.feature_names, class_names=data.target_names)\n",
        "plt.title(\"Full, Unpruned Decision Tree\")\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# ---------------- 2.2 TRAIN AND EVALUATE A POST-PRUNED TREE ----------------\n",
        "print(\"\\n--- Step 2.2: Training and Evaluating a Post-Pruned Tree (max_depth=2) ---\")\n",
        "# Post-pruning simplifies the tree by limiting its growth, e.g., by setting max_depth.\n",
        "# This helps prevent overfitting and improves generalization.\n",
        "pruned_classifier = DecisionTreeClassifier(criterion='entropy', max_depth=2)\n",
        "pruned_classifier.fit(X_train, y_train)\n",
        "\n",
        "print(\"Visualizing the post-pruned Decision Tree...\")\n",
        "plt.figure(figsize=(12, 8))\n",
        "plot_tree(pruned_classifier, filled=True, feature_names=data.feature_names, class_names=data.target_names)\n",
        "plt.title(\"Post-Pruned Decision Tree (max_depth=2)\")\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# --- Make predictions and evaluate the PRUNED model ---\n",
        "y_pred_pruned = pruned_classifier.predict(X_test)\n",
        "print(\"\\n--- Evaluation of the Post-Pruned Model ---\")\n",
        "score = accuracy_score(y_pred_pruned, y_test)\n",
        "print(f\"Accuracy Score: {score:.4f}\")\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_pred_pruned, y_test))\n",
        "print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
        "\n",
        "\n",
        "# ====================================================================================\n",
        "# --- STEP 3: PRE-PRUNING WITH HYPERPARAMETER TUNING (GRIDSEARCHCV) ---\n",
        "# ====================================================================================\n",
        "print(\"--- Step 3: Finding the Best Model with Pre-Pruning (GridSearchCV) ---\")\n",
        "# Pre-pruning finds the best hyperparameters BEFORE training the final model.\n",
        "\n",
        "# ---------------- 3.1 DEFINE HYPERPARAMETER GRID ----------------\n",
        "parameter = {\n",
        "    'criterion': ['gini', 'entropy', 'log_loss'],\n",
        "    'splitter': ['best', 'random'],\n",
        "    'max_depth': [1, 2, 3, 4, 5, 6],\n",
        "    'max_features': ['sqrt', 'log2', 'auto']\n",
        "}\n",
        "print(\"Defined parameter grid for hyperparameter search.\")\n",
        "\n",
        "# ---------------- 3.2 PERFORM GRID SEARCH WITH CROSS-VALIDATION ----------------\n",
        "# GridSearchCV exhaustively tries all parameter combinations with 5-fold cross-validation.\n",
        "clf = DecisionTreeClassifier()\n",
        "grid_search_model = GridSearchCV(clf, param_grid=parameter, cv=5, scoring='accuracy')\n",
        "print(\"\\nRunning GridSearchCV... (This may take a moment)\")\n",
        "grid_search_model.fit(X_train, y_train)\n",
        "print(\"GridSearchCV complete.\")\n",
        "\n",
        "# ---------------- 3.3 DISPLAY BEST PARAMETERS AND SCORE ----------------\n",
        "print(\"\\n--- GridSearchCV Results ---\")\n",
        "print(\"Best Hyperparameters Found:\")\n",
        "print(grid_search_model.best_params_)\n",
        "print(f\"\\nBest Cross-Validated Accuracy Score on Training Data: {grid_search_model.best_score_:.4f}\")\n",
        "\n",
        "# ---------------- 3.4 EVALUATE THE BEST MODEL ON THE TEST SET ----------------\n",
        "print(\"\\n--- Final Evaluation of the Best Model from GridSearchCV on the Test Set ---\")\n",
        "# It's crucial to evaluate the final, tuned model on the unseen test data.\n",
        "best_model = grid_search_model.best_estimator_\n",
        "y_pred_best = best_model.predict(X_test)\n",
        "print(f\"Accuracy Score on Test Data: {accuracy_score(y_test, y_pred_best):.4f}\")\n",
        "print(\"\\nClassification Report on Test Data:\")\n",
        "print(classification_report(y_test, y_pred_best))\n",
        "print(\"\\n--- MASTER PIPELINE COMPLETE ---\")\n",
        "\n",
        "# ---------------- END OF SCRIPT ----------------\n"
      ],
      "metadata": {
        "id": "8ubMAtnWmTCE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DECION TREE REGRESSOR"
      ],
      "metadata": {
        "id": "zBcUG74JoA1S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------- MASTER DECISION TREE REGRESSOR PIPELINE ----------------\n",
        "# This single, unified script combines all the code and concepts from your\n",
        "# Decision Tree Regression examples into one complete workflow.\n",
        "\n",
        "# Import required libraries\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn import tree\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "# ====================================================================================\n",
        "# --- STEP 1: LOAD, PREPARE, AND SAMPLE THE DATASET ---\n",
        "# ====================================================================================\n",
        "print(\"--- Step 1: Loading and Preparing the California Housing Dataset ---\")\n",
        "\n",
        "# ---------------- 1.1 LOAD DATASET AND CREATE DATAFRAME ----------------\n",
        "data = fetch_california_housing()\n",
        "df = pd.DataFrame(data.data, columns=data.feature_names)\n",
        "df['Price'] = data.target\n",
        "print(f\"Original dataset shape: {df.shape}\")\n",
        "\n",
        "# ---------------- 1.2 SAMPLE DATA ----------------\n",
        "# Take a 20% random sample for faster processing, as done in your code.\n",
        "df = df.sample(frac=0.20, random_state=1) # Added random_state for reproducibility\n",
        "print(f\"Shape after sampling (20%): {df.shape}\")\n",
        "\n",
        "# ---------------- 1.3 SEPARATE FEATURES AND TARGET ----------------\n",
        "X = df.iloc[:, :-1]  # Features (all columns except the last)\n",
        "y = df.iloc[:, -1]   # Target (the last column)\n",
        "print(\"\\nFeatures (X) and target (y) have been separated.\")\n",
        "print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
        "\n",
        "\n",
        "# ====================================================================================\n",
        "# --- STEP 2: INITIAL MODEL TRAINING AND EVALUATION ---\n",
        "# ====================================================================================\n",
        "print(\"--- Step 2: Training and Evaluating an Initial (Untuned) Decision Tree ---\")\n",
        "\n",
        "# ---------------- 2.1 SPLIT DATA INTO TRAINING AND TESTING SETS ----------------\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
        "print(f\"Data split into {X_train.shape[0]} training samples and {X_test.shape[0]} testing samples.\")\n",
        "\n",
        "# ---------------- 2.2 CREATE AND FIT THE INITIAL MODEL ----------------\n",
        "initial_model = DecisionTreeRegressor(random_state=1) # Added random_state for reproducibility\n",
        "initial_model.fit(X_train, y_train)\n",
        "print(\"\\nInitial Decision Tree Regressor model has been trained.\")\n",
        "\n",
        "# ---------------- 2.3 PREDICT AND EVALUATE ----------------\n",
        "y_pred_initial = initial_model.predict(X_test)\n",
        "initial_r2 = r2_score(y_test, y_pred_initial) # Correct order is (y_true, y_pred)\n",
        "print(f\"\\nInitial R-squared Score: {initial_r2:.4f}\")\n",
        "print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
        "\n",
        "\n",
        "# ====================================================================================\n",
        "# --- STEP 3: HYPERPARAMETER TUNING WITH GRIDSEARCHCV ---\n",
        "# ====================================================================================\n",
        "print(\"--- Step 3: Finding the Best Model with Hyperparameter Tuning (GridSearchCV) ---\")\n",
        "\n",
        "# ---------------- 3.1 DEFINE HYPERPARAMETER GRID ----------------\n",
        "parameter = {\n",
        "    'criterion': ['squared_error', 'friedman_mse', 'absolute_error', 'poisson'],\n",
        "    'splitter': ['best', 'random'],\n",
        "    'max_depth': [1, 2, 3, 4, 5, 6],\n",
        "    'max_features': ['auto', 'sqrt', 'log2']\n",
        "}\n",
        "print(\"Defined parameter grid for hyperparameter search.\")\n",
        "\n",
        "# ---------------- 3.2 PERFORM GRID SEARCH WITH CROSS-VALIDATION ----------------\n",
        "regressor = DecisionTreeRegressor(random_state=1)\n",
        "grid_search_model = GridSearchCV(regressor, param_grid=parameter, cv=3, scoring='neg_mean_squared_error')\n",
        "print(\"\\nRunning GridSearchCV... (This may take a moment)\")\n",
        "grid_search_model.fit(X_train, y_train)\n",
        "print(\"GridSearchCV complete.\")\n",
        "\n",
        "# ---------------- 3.3 DISPLAY BEST PARAMETERS ----------------\n",
        "print(\"\\n--- GridSearchCV Results ---\")\n",
        "print(\"Best Hyperparameters Found:\")\n",
        "print(grid_search_model.best_params_)\n",
        "print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
        "\n",
        "\n",
        "# ====================================================================================\n",
        "# --- STEP 4: FINAL MODEL TRAINING, VISUALIZATION, AND EVALUATION ---\n",
        "# ====================================================================================\n",
        "print(\"--- Step 4: Training and Visualizing the Final, Tuned Model ---\")\n",
        "\n",
        "# ---------------- 4.1 CREATE AND TRAIN THE FINAL MODEL ----------------\n",
        "# Use the best parameters found from GridSearchCV (as specified in your code).\n",
        "# Note: Your code had a hardcoded example, we will use that for consistency.\n",
        "final_model = DecisionTreeRegressor(\n",
        "    criterion='poisson',\n",
        "    max_depth=6,\n",
        "    max_features='auto',\n",
        "    splitter='best',\n",
        "    random_state=1 # Added for reproducibility\n",
        ")\n",
        "final_model.fit(X_train, y_train)\n",
        "print(\"Final model with best parameters has been trained.\")\n",
        "\n",
        "# ---------------- 4.2 VISUALIZE THE FINAL TREE ----------------\n",
        "print(\"\\nVisualizing the final, tuned Decision Tree...\")\n",
        "plt.figure(figsize=(20, 15))\n",
        "tree.plot_tree(final_model, filled=True, feature_names=X.columns, rounded=True)\n",
        "plt.title(\"Final Tuned Decision Tree Regressor\")\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# ---------------- 4.3 FINAL PREDICTION AND EVALUATION ----------------\n",
        "# Your code uses the 'model' (GridSearchCV object) for the final prediction.\n",
        "# The .predict() method of a fitted GridSearchCV object automatically uses the best estimator.\n",
        "y_pred_final = grid_search_model.predict(X_test)\n",
        "final_r2 = r2_score(y_test, y_pred_final) # Correct order is (y_true, y_pred)\n",
        "print(f\"\\nFinal R-squared Score from Best Model: {final_r2:.4f}\")\n",
        "print(\"\\n--- MASTER PIPELINE COMPLETE ---\")\n",
        "\n",
        "# ---------------- END OF SCRIPT ----------------\n"
      ],
      "metadata": {
        "id": "pZsISyTwoESA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SUPPORT VECTOR CLASSIFIER"
      ],
      "metadata": {
        "id": "ReebCen4o_3M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------- MASTER SVM CLASSIFIER PIPELINE ----------------\n",
        "# This single, unified script combines all the code and concepts from your\n",
        "# Support Vector Machine (SVM) example into one complete workflow.\n",
        "\n",
        "# Import required libraries\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "\n",
        "# ====================================================================================\n",
        "# --- STEP 1: GENERATE AND VISUALIZE THE DATASET ---\n",
        "# ====================================================================================\n",
        "print(\"--- Step 1: Generating and Visualizing Synthetic Classification Data ---\")\n",
        "\n",
        "# ---------------- 1.1 GENERATE SYNTHETIC DATA ----------------\n",
        "# As per your code, we create a dataset with 2 informative features and 2 classes.\n",
        "X, y = make_classification(\n",
        "    n_samples=1000,\n",
        "    n_features=2,\n",
        "    n_classes=2,\n",
        "    n_clusters_per_class=2,\n",
        "    n_redundant=0,\n",
        "    random_state=42 # Added for reproducibility\n",
        ")\n",
        "print(\"Synthetic dataset with 1000 samples and 2 features has been generated.\")\n",
        "\n",
        "# ---------------- 1.2 VISUALIZE THE DATA ----------------\n",
        "# A scatter plot helps to visualize the clusters for each class.\n",
        "print(\"\\nVisualizing the data...\")\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.scatterplot(\n",
        "    x=pd.DataFrame(X)[0],\n",
        "    y=pd.DataFrame(X)[1],\n",
        "    hue=y\n",
        ")\n",
        "plt.title(\"Scatter Plot of Synthetic Data\")\n",
        "plt.xlabel(\"Feature 1\")\n",
        "plt.ylabel(\"Feature 2\")\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
        "\n",
        "\n",
        "# ====================================================================================\n",
        "# --- STEP 2: INITIAL MODEL TRAINING AND EVALUATION ---\n",
        "# ====================================================================================\n",
        "print(\"--- Step 2: Training and Evaluating an Initial (Untuned) SVM Classifier ---\")\n",
        "\n",
        "# ---------------- 2.1 SPLIT DATA INTO TRAINING AND TESTING SETS ----------------\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=1)\n",
        "print(f\"Data split into {X_train.shape[0]} training samples and {X_test.shape[0]} testing samples.\")\n",
        "\n",
        "# ---------------- 2.2 CREATE AND FIT THE INITIAL MODEL ----------------\n",
        "# An SVM with a linear kernel attempts to find a straight line that best separates the classes.\n",
        "initial_classifier = SVC(kernel='linear')\n",
        "initial_classifier.fit(X_train, y_train)\n",
        "print(\"\\nInitial SVM model with linear kernel has been trained.\")\n",
        "print(\"Learned Coefficients (weights):\", initial_classifier.coef_)\n",
        "\n",
        "# ---------------- 2.3 PREDICT AND EVALUATE ----------------\n",
        "y_pred_initial = initial_classifier.predict(X_test)\n",
        "print(\"\\n--- Initial Model Evaluation ---\")\n",
        "print(\"Confusion Matrix:\")\n",
        "print(confusion_matrix(y_test, y_pred_initial))\n",
        "print(\"\\nAccuracy Score:\", accuracy_score(y_test, y_pred_initial))\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred_initial))\n",
        "print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
        "\n",
        "\n",
        "# ====================================================================================\n",
        "# --- STEP 3: HYPERPARAMETER TUNING WITH GRIDSEARCHCV ---\n",
        "# ====================================================================================\n",
        "print(\"--- Step 3: Finding the Best Model with Hyperparameter Tuning (GridSearchCV) ---\")\n",
        "\n",
        "# ---------------- 3.1 DEFINE HYPERPARAMETER GRID ----------------\n",
        "# Define the grid of parameters to search through.\n",
        "params = {\n",
        "    'C': [0.1, 1, 10, 50, 100],\n",
        "    'gamma': [1, 0.1, 0.01, 0.001],\n",
        "    'kernel': ['linear', 'rbf'] # Added 'rbf' to make the search more comprehensive\n",
        "}\n",
        "print(\"Defined parameter grid for hyperparameter search.\")\n",
        "\n",
        "# ---------------- 3.2 PERFORM GRID SEARCH WITH CROSS-VALIDATION ----------------\n",
        "# GridSearchCV exhaustively tests all parameter combinations.\n",
        "# verbose=3 provides detailed output during the fitting process.\n",
        "grid_search = GridSearchCV(SVC(), param_grid=params, cv=5, verbose=3)\n",
        "print(\"\\nRunning GridSearchCV... (This may take a moment)\")\n",
        "grid_search.fit(X_train, y_train)\n",
        "print(\"GridSearchCV complete.\")\n",
        "\n",
        "# ---------------- 3.3 DISPLAY BEST PARAMETERS AND SCORE ----------------\n",
        "print(\"\\n--- GridSearchCV Results ---\")\n",
        "print(\"Best Hyperparameters Found:\")\n",
        "print(grid_search.best_params_)\n",
        "print(f\"\\nBest Cross-Validated Accuracy Score: {grid_search.best_score_:.4f}\")\n",
        "print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
        "\n",
        "\n",
        "# ====================================================================================\n",
        "# --- STEP 4: FINAL EVALUATION OF THE BEST MODEL ---\n",
        "# ====================================================================================\n",
        "print(\"--- Step 4: Final Evaluation Using the Best Model from GridSearchCV ---\")\n",
        "\n",
        "# ---------------- 4.1 PREDICT USING THE BEST MODEL ----------------\n",
        "# The fitted GridSearchCV object automatically uses the best estimator for predictions.\n",
        "y_pred_final = grid_search.predict(X_test)\n",
        "print(\"Predictions made using the best model found by GridSearchCV.\")\n",
        "\n",
        "# ---------------- 4.2 FINAL EVALUATION METRICS ----------------\n",
        "print(\"\\n--- Final Tuned Model Evaluation ---\")\n",
        "print(\"Confusion Matrix:\")\n",
        "print(confusion_matrix(y_test, y_pred_final))\n",
        "print(\"\\nAccuracy Score:\", accuracy_score(y_test, y_pred_final))\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred_final))\n",
        "print(\"\\n--- MASTER PIPELINE COMPLETE ---\")\n",
        "\n",
        "# ---------------- END OF SCRIPT ----------------\n"
      ],
      "metadata": {
        "id": "BpAMy0ZzpGJY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SUPPORT VACTOR REGRESSOR"
      ],
      "metadata": {
        "id": "ytOY_jd2qs_X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------- MASTER SUPPORT VECTOR REGRESSOR (SVR) PIPELINE ----------------\n",
        "# This single, unified script combines all the code and concepts from your\n",
        "# Support Vector Regressor (SVR) example into one complete workflow.\n",
        "\n",
        "# Import required libraries\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import make_regression\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "# ====================================================================================\n",
        "# --- STEP 1: GENERATE AND VISUALIZE THE DATASET ---\n",
        "# ====================================================================================\n",
        "print(\"--- Step 1: Generating and Visualizing Synthetic Regression Data ---\")\n",
        "\n",
        "# ---------------- 1.1 GENERATE SYNTHETIC DATA ----------------\n",
        "# As per your code, we create a dataset with 2 features and 1 target.\n",
        "X, y = make_regression(\n",
        "    n_samples=1000,\n",
        "    n_features=2,\n",
        "    n_targets=1,\n",
        "    noise=3.0,\n",
        "    random_state=42 # Added for reproducibility\n",
        ")\n",
        "print(\"Synthetic dataset with 1000 samples and 2 features has been generated.\")\n",
        "\n",
        "# ---------------- 1.2 VISUALIZE THE DATA ----------------\n",
        "# A scatter plot helps to visualize the relationship between features and the target.\n",
        "print(\"\\nVisualizing the data...\")\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.scatterplot(\n",
        "    x=pd.DataFrame(X)[0],\n",
        "    y=pd.DataFrame(X)[1],\n",
        "    hue=y,\n",
        "    palette='viridis' # Use a continuous color palette for regression\n",
        ")\n",
        "plt.title(\"Scatter Plot of Synthetic Regression Data\")\n",
        "plt.xlabel(\"Feature 1\")\n",
        "plt.ylabel(\"Feature 2\")\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
        "\n",
        "\n",
        "# ====================================================================================\n",
        "# --- STEP 2: INITIAL MODEL TRAINING AND EVALUATION ---\n",
        "# ====================================================================================\n",
        "print(\"--- Step 2: Training and Evaluating an Initial (Untuned) SVR ---\")\n",
        "\n",
        "# ---------------- 2.1 SPLIT DATA INTO TRAINING AND TESTING SETS ----------------\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=1)\n",
        "print(f\"Data split into {X_train.shape[0]} training samples and {X_test.shape[0]} testing samples.\")\n",
        "\n",
        "# ---------------- 2.2 CREATE AND FIT THE INITIAL MODEL ----------------\n",
        "# An SVR with a linear kernel attempts to fit a hyperplane to the data.\n",
        "initial_svr = SVR(kernel='linear')\n",
        "initial_svr.fit(X_train, y_train)\n",
        "print(\"\\nInitial SVR model with linear kernel has been trained.\")\n",
        "print(\"Learned Coefficients (weights):\", initial_svr.coef_)\n",
        "\n",
        "# ---------------- 2.3 PREDICT AND EVALUATE ----------------\n",
        "y_pred_initial = initial_svr.predict(X_test)\n",
        "initial_r2 = r2_score(y_test, y_pred_initial)\n",
        "print(f\"\\nInitial R-squared Score: {initial_r2:.4f}\")\n",
        "print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
        "\n",
        "\n",
        "# ====================================================================================\n",
        "# --- STEP 3: HYPERPARAMETER TUNING WITH GRIDSEARCHCV ---\n",
        "# ====================================================================================\n",
        "print(\"--- Step 3: Finding the Best Model with Hyperparameter Tuning (GridSearchCV) ---\")\n",
        "\n",
        "# ---------------- 3.1 DEFINE HYPERPARAMETER GRID ----------------\n",
        "# Define the grid of parameters to search through.\n",
        "params = {\n",
        "    'C': [0.1, 1, 10, 50, 100],\n",
        "    'gamma': [1, 0.1, 0.001], # 'gamma' is not used by linear kernel, but kept for consistency\n",
        "    'kernel': ['linear'],\n",
        "    'epsilon': [0.01, 0.1, 0.2, 0.3]\n",
        "}\n",
        "print(\"Defined parameter grid for hyperparameter search.\")\n",
        "\n",
        "# ---------------- 3.2 PERFORM GRID SEARCH WITH CROSS-VALIDATION ----------------\n",
        "# GridSearchCV exhaustively tests all parameter combinations.\n",
        "# verbose=3 provides detailed output during the fitting process.\n",
        "grid_search = GridSearchCV(SVR(), param_grid=params, cv=5, verbose=3)\n",
        "print(\"\\nRunning GridSearchCV... (This may take a moment)\")\n",
        "grid_search.fit(X_train, y_train)\n",
        "print(\"GridSearchCV complete.\")\n",
        "\n",
        "# ---------------- 3.3 DISPLAY BEST PARAMETERS AND SCORE ----------------\n",
        "print(\"\\n--- GridSearchCV Results ---\")\n",
        "print(\"Best Hyperparameters Found:\")\n",
        "print(grid_search.best_params_)\n",
        "print(f\"\\nBest Cross-Validated R2 Score: {grid_search.best_score_:.4f}\")\n",
        "print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
        "\n",
        "\n",
        "# ====================================================================================\n",
        "# --- STEP 4: FINAL EVALUATION OF THE BEST MODEL ---\n",
        "# ====================================================================================\n",
        "print(\"--- Step 4: Final Evaluation Using the Best Model from GridSearchCV ---\")\n",
        "\n",
        "# ---------------- 4.1 PREDICT USING THE BEST MODEL ----------------\n",
        "# The fitted GridSearchCV object automatically uses the best estimator for predictions.\n",
        "y_pred_final = grid_search.predict(X_test)\n",
        "print(\"Predictions made using the best model found by GridSearchCV.\")\n",
        "\n",
        "# ---------------- 4.2 FINAL EVALUATION METRICS ----------------\n",
        "final_r2 = r2_score(y_test, y_pred_final)\n",
        "print(f\"\\nFinal R-squared Score on Test Data: {final_r2:.4f}\")\n",
        "print(\"\\n--- MASTER PIPELINE COMPLETE ---\")\n",
        "\n",
        "# ---------------- END OF SCRIPT ----------------\n"
      ],
      "metadata": {
        "id": "ROdX7BaLqyMW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SVM KERNAL"
      ],
      "metadata": {
        "id": "L7tBR2JLrLpP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------- SVM NON-LINEAR DATA PREPARATION PIPELINE ----------------\n",
        "# This script contains the exact code you provided for generating non-linear data,\n",
        "# engineering new features to make it separable, and visualizing the results.\n",
        "\n",
        "# Import required libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import plotly.express as px\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# ====================================================================================\n",
        "# --- STEP 1: GENERATE AND VISUALIZE NON-LINEAR DATA (CONCENTRIC CIRCLES) ---\n",
        "# ====================================================================================\n",
        "print(\"--- Step 1: Generating and Visualizing Non-Linear Classification Data ---\")\n",
        "\n",
        "# ---------------- 1.1 CREATE DATA FOR TWO CIRCLES ----------------\n",
        "# Create first circle points (radius 10)\n",
        "x_outer = np.linspace(-6.0, 6.0, 100)\n",
        "y_outer = np.sqrt(10**2 - x_outer**2)\n",
        "y_outer = np.hstack([y_outer, -y_outer])\n",
        "x_outer = np.hstack([x_outer, -x_outer])\n",
        "\n",
        "# Create second circle points (radius 4)\n",
        "x_inner = np.linspace(-6.0, 6.0, 100)\n",
        "y_inner = np.sqrt(4**2 - x_inner**2)\n",
        "y_inner = np.hstack([y_inner, -y_inner])\n",
        "x_inner = np.hstack([x_inner, -x_inner])\n",
        "\n",
        "# ---------------- 1.2 PLOT THE CIRCLES ----------------\n",
        "print(\"\\nVisualizing the original 2D data...\")\n",
        "plt.figure(figsize=(8, 8))\n",
        "plt.scatter(y_outer, x_outer, label='Circle radius 10')\n",
        "plt.scatter(y_inner, x_inner, label='Circle radius 4')\n",
        "plt.title(\"Original Non-Linearly Separable Data\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
        "\n",
        "\n",
        "# ====================================================================================\n",
        "# --- STEP 2: PREPARE DATAFRAME AND ENGINEER NEW FEATURES ---\n",
        "# ====================================================================================\n",
        "print(\"--- Step 2: Preparing DataFrame and Engineering Non-Linear Features ---\")\n",
        "\n",
        "# ---------------- 2.1 CREATE DATAFRAMES FOR EACH CLASS ----------------\n",
        "# Create DataFrame for the outer circle and assign class 0\n",
        "df1 = pd.DataFrame(np.vstack([y_outer, x_outer]).T, columns=['X1', 'X2'])\n",
        "df1['Y'] = 0\n",
        "\n",
        "# Create DataFrame for the inner circle and assign class 1\n",
        "df2 = pd.DataFrame(np.vstack([y_inner, x_inner]).T, columns=['X1', 'X2'])\n",
        "df2['Y'] = 1\n",
        "\n",
        "# Combine both datasets into one\n",
        "df = pd.concat([df1, df2], ignore_index=True)\n",
        "print(\"Combined DataFrame created. Head:\")\n",
        "print(df.head(5))\n",
        "\n",
        "# ---------------- 2.2 CREATE NEW FEATURES (NON-LINEAR TRANSFORMATION) ----------------\n",
        "# Make a copy and drop any missing values from the sqrt operation\n",
        "df1 = df.copy()\n",
        "df1 = df1.dropna()\n",
        "\n",
        "# This is the key step: creating new features from the original ones.\n",
        "df1[\"x1square\"] = df1[\"X1\"]**2\n",
        "df1[\"x2square\"] = df1[\"X2\"]**2\n",
        "df1[\"x1x2\"] = df1[\"X1\"] * df1[\"X2\"]\n",
        "print(\"\\nDataFrame with new polynomial features:\")\n",
        "print(df1.head())\n",
        "print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
        "\n",
        "\n",
        "# ====================================================================================\n",
        "# --- STEP 3: SPLIT DATA AND VISUALIZE THE TRANSFORMED FEATURES ---\n",
        "# ====================================================================================\n",
        "print(\"--- Step 3: Splitting Data and Visualizing in 3D ---\")\n",
        "\n",
        "# ---------------- 3.1 DEFINE FEATURES, TARGET, AND SPLIT ----------------\n",
        "# Define features (X) and target (y) for modeling\n",
        "X = df1[[\"x1square\", \"x2square\", \"x1x2\"]]  # Only the new non-linear features\n",
        "y = df1['Y']                               # Target label\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=1)\n",
        "print(\"Data has been split into training and testing sets. Training features head:\")\n",
        "print(X_train.head())\n",
        "\n",
        "# ---------------- 3.2 VISUALIZE THE TRANSFORMED FEATURES IN 3D ----------------\n",
        "# By plotting the new features, we can see if they are now linearly separable.\n",
        "print(\"\\nDisplaying the 3D plot of the transformed features...\")\n",
        "fig = px.scatter_3d(df1, x=\"x1square\", y=\"x2square\", z=\"x1x2\", color=\"Y\")\n",
        "fig.update_layout(title=\"Data in Transformed 3D Feature Space\")\n",
        "fig.show()\n",
        "print(\"\\nThis concludes the data preparation and visualization steps from your code.\")\n",
        "print(\"\\n--- PIPELINE COMPLETE ---\")\n",
        "\n",
        "# ---------------- END OF SCRIPT ----------------\n"
      ],
      "metadata": {
        "id": "DcVLkg3vrPW1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Gaussian Naive Bayes"
      ],
      "metadata": {
        "id": "eoqNY_-EtRRH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------- GAUSSIAN NAIVE BAYES PIPELINE ----------------\n",
        "# This script contains the exact code you provided for implementing a Gaussian\n",
        "# Naive Bayes classifier on the Iris dataset.\n",
        "\n",
        "# Import required libraries\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "import pandas as pd\n",
        "\n",
        "# ====================================================================================\n",
        "# --- STEP 1: LOAD THE IRIS DATASET ---\n",
        "# ====================================================================================\n",
        "print(\"--- Step 1: Loading the Iris Dataset ---\")\n",
        "\n",
        "# Load the iris dataset directly as features (X) and target (y)\n",
        "X, y = load_iris(return_X_y=True)\n",
        "\n",
        "print(\"Dataset loaded successfully.\")\n",
        "print(f\"Shape of features (X): {X.shape}\")\n",
        "print(f\"Shape of target (y): {y.shape}\")\n",
        "print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
        "\n",
        "\n",
        "# ====================================================================================\n",
        "# --- STEP 2: SPLIT DATA INTO TRAINING AND TESTING SETS ---\n",
        "# ====================================================================================\n",
        "print(\"--- Step 2: Splitting Data into Training and Testing Sets ---\")\n",
        "\n",
        "# 30% of data is reserved for testing, 70% for training\n",
        "# random_state=1 ensures the split is the same every time the code is run\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)\n",
        "\n",
        "print(f\"Data split into {X_train.shape[0]} training samples and {X_test.shape[0]} testing samples.\")\n",
        "print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
        "\n",
        "\n",
        "# ====================================================================================\n",
        "# --- STEP 3: TRAIN THE GAUSSIAN NAIVE BAYES MODEL ---\n",
        "# ====================================================================================\n",
        "print(\"--- Step 3: Training the Gaussian Naive Bayes Classifier ---\")\n",
        "\n",
        "# Create a GaussianNB classifier instance\n",
        "clf = GaussianNB()\n",
        "\n",
        "# Fit the classifier on the training data\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "print(\"Model has been trained successfully.\")\n",
        "print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
        "\n",
        "\n",
        "# ====================================================================================\n",
        "# --- STEP 4: MAKE PREDICTIONS ON THE TEST SET ---\n",
        "# ====================================================================================\n",
        "print(\"--- Step 4: Making Predictions on the Test Data ---\")\n",
        "\n",
        "# Predict the target labels for the test set\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "print(\"Predictions have been generated. Displaying the predicted labels for the test set:\")\n",
        "print(y_pred)\n",
        "print(\"\\nThis concludes the steps from the code you provided.\")\n",
        "print(\"\\n--- PIPELINE COMPLETE ---\")\n",
        "\n",
        "# ---------------- END OF SCRIPT ----------------\n"
      ],
      "metadata": {
        "id": "5l23XwdUtPcY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Ensamble Custom bagging classfier AND REGRESSOR"
      ],
      "metadata": {
        "id": "clVVZHLTtYn5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------- ENSEMBLE LEARNING PIPELINE (CLASSIFIER & REGRESSOR) ----------------\n",
        "# This single, unified script combines all the code you provided for both\n",
        "# ensemble classification (VotingClassifier) and ensemble regression (VotingRegressor).\n",
        "\n",
        "# Import required libraries\n",
        "import pandas as pd\n",
        "from sklearn.datasets import make_classification, make_regression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
        "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
        "from sklearn.svm import SVC, SVR\n",
        "from sklearn.ensemble import VotingClassifier, VotingRegressor\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# ====================================================================================\n",
        "# --- PART 1: ENSEMBLE CUSTOM BAGGING CLASSIFIER ---\n",
        "# ====================================================================================\n",
        "print(\"--- Part 1: Building and Evaluating an Ensemble Classifier ---\")\n",
        "\n",
        "# ---------------- 1.1 GENERATE AND SPLIT CLASSIFICATION DATA ----------------\n",
        "print(\"\\nStep 1.1: Generating synthetic classification data...\")\n",
        "X_clf, y_clf = make_classification(n_samples=1000, n_features=20, n_classes=2, random_state=1)\n",
        "X_clf_train, X_clf_test, y_clf_train, y_clf_test = train_test_split(X_clf, y_clf, test_size=0.2, random_state=1)\n",
        "print(f\"Classification data split into {X_clf_train.shape[0]} training and {X_clf_test.shape[0]} testing samples.\")\n",
        "\n",
        "# ---------------- 1.2 INITIALIZE BASE CLASSIFIERS ----------------\n",
        "print(\"\\nStep 1.2: Initializing base models for the ensemble...\")\n",
        "nb_clf = GaussianNB()\n",
        "lr_clf = LogisticRegression(random_state=1)\n",
        "dt_clf = DecisionTreeClassifier(random_state=1)\n",
        "svm_clf = SVC(kernel=\"linear\", random_state=1)\n",
        "print(\"Base classifiers (Naive Bayes, Logistic Regression, Decision Tree, SVM) created.\")\n",
        "\n",
        "# ---------------- 1.3 CREATE AND TRAIN THE ENSEMBLE CLASSIFIER ----------------\n",
        "print(\"\\nStep 1.3: Creating and training the VotingClassifier...\")\n",
        "# A VotingClassifier combines different models and predicts based on a majority vote ('hard' voting).\n",
        "ensemble_clf = VotingClassifier(\n",
        "    estimators=[\n",
        "        ('decision_tree', dt_clf),\n",
        "        ('naive_bayes', nb_clf),\n",
        "        ('log_reg', lr_clf),\n",
        "        ('svm', svm_clf)\n",
        "    ],\n",
        "    voting=\"hard\"\n",
        ")\n",
        "# Train the ensemble model on the training data\n",
        "ensemble_clf.fit(X_clf_train, y_clf_train)\n",
        "print(\"Ensemble classifier trained successfully.\")\n",
        "\n",
        "# ---------------- 1.4 EVALUATE THE ENSEMBLE CLASSIFIER ----------------\n",
        "print(\"\\nStep 1.4: Evaluating the ensemble classifier on the test data...\")\n",
        "y_pred_clf = ensemble_clf.predict(X_clf_test)\n",
        "accuracy = accuracy_score(y_clf_test, y_pred_clf)\n",
        "print(f\"Accuracy of the Ensemble Classifier: {accuracy:.4f}\")\n",
        "print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
        "\n",
        "\n",
        "# ====================================================================================\n",
        "# --- PART 2: ENSEMBLE CUSTOM BAGGING REGRESSOR ---\n",
        "# ====================================================================================\n",
        "print(\"--- Part 2: Building an Ensemble Regressor ---\")\n",
        "\n",
        "# ---------------- 2.1 GENERATE AND SPLIT REGRESSION DATA ----------------\n",
        "print(\"\\nStep 2.1: Generating synthetic regression data...\")\n",
        "X_reg, y_reg = make_regression(n_samples=1000, n_features=10, noise=0.1, random_state=42)\n",
        "X_reg_train, X_reg_test, y_reg_train, y_reg_test = train_test_split(X_reg, y_reg, test_size=0.2, random_state=1)\n",
        "print(f\"Regression data split into {X_reg_train.shape[0]} training and {X_reg_test.shape[0]} testing samples.\")\n",
        "\n",
        "# ---------------- 2.2 INITIALIZE BASE REGRESSORS ----------------\n",
        "print(\"\\nStep 2.2: Initializing base models for the ensemble...\")\n",
        "lr_reg = LinearRegression()\n",
        "dtr_reg = DecisionTreeRegressor(random_state=1)\n",
        "svr_reg = SVR(kernel=\"linear\")\n",
        "print(\"Base regressors (Linear Regression, Decision Tree, SVR) created.\")\n",
        "\n",
        "# ---------------- 2.3 CREATE AND TRAIN THE ENSEMBLE REGRESSOR ----------------\n",
        "print(\"\\nStep 2.3: Creating and training the VotingRegressor...\")\n",
        "# A VotingRegressor combines different models and averages their individual predictions.\n",
        "ensemble_regressor = VotingRegressor(\n",
        "    estimators=[\n",
        "        ('mlr', lr_reg),\n",
        "        (\"dtr\", dtr_reg),\n",
        "        (\"svr\", svr_reg)\n",
        "    ]\n",
        ")\n",
        "# Train the ensemble model on the training data\n",
        "ensemble_regressor.fit(X_reg_train, y_reg_train)\n",
        "print(\"Ensemble regressor trained successfully.\")\n",
        "print(\"\\nNote: Further steps would involve making predictions with 'ensemble_regressor.predict(X_reg_test)' and evaluating with a regression metric like R2 score.\")\n",
        "print(\"\\n--- MASTER PIPELINE COMPLETE ---\")\n",
        "\n",
        "# ---------------- END OF SCRIPT ----------------\n"
      ],
      "metadata": {
        "id": "c95JVyvwt7eS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Multiple models with Pipeline and ColumnTransformer"
      ],
      "metadata": {
        "id": "FHJrAKAyujeR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------- AUTOMATED FEATURE ENGINEERING & MODEL EVALUATION PIPELINE ----------------\n",
        "# This single, unified script combines all the code you provided for using Pipeline\n",
        "# and ColumnTransformer to preprocess data and evaluate multiple models.\n",
        "\n",
        "# Import required libraries\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler, OneHotEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# ====================================================================================\n",
        "# --- STEP 1: LOAD AND PREPARE THE DATA ---\n",
        "# ====================================================================================\n",
        "print(\"--- Step 1: Loading and Preparing the 'tips' Dataset ---\")\n",
        "\n",
        "# ---------------- 1.1 LOAD DATA AND ENCODE TARGET VARIABLE ----------------\n",
        "# Load the 'tips' dataset from seaborn\n",
        "df = sns.load_dataset(\"tips\")\n",
        "print(\"Original Data Head:\")\n",
        "print(df.head())\n",
        "\n",
        "# The target variable 'time' is categorical ('Lunch', 'Dinner').\n",
        "# We use LabelEncoder to convert it to a numerical format (0, 1).\n",
        "encoder = LabelEncoder()\n",
        "df['time'] = encoder.fit_transform(df['time'])\n",
        "print(\"\\n'time' column after Label Encoding (Lunch=0, Dinner=1):\")\n",
        "print(df['time'].unique())\n",
        "\n",
        "# ---------------- 1.2 SEPARATE FEATURES AND TARGET & SPLIT DATA ----------------\n",
        "X = df.drop('time', axis=1)  # Features\n",
        "y = df['time']               # Target\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=1)\n",
        "print(f\"\\nData split into {X_train.shape[0]} training and {X_test.shape[0]} testing samples.\")\n",
        "print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
        "\n",
        "\n",
        "# ====================================================================================\n",
        "# --- STEP 2: CREATE THE PREPROCESSING PIPELINE ---\n",
        "# ====================================================================================\n",
        "print(\"--- Step 2: Building an Automated Preprocessing Pipeline ---\")\n",
        "\n",
        "# ---------------- 2.1 DEFINE COLUMN TYPES ----------------\n",
        "# Identify which columns are categorical and which are numerical\n",
        "cat_cols = [\"sex\", \"smoker\", \"day\"]\n",
        "num_cols = [\"total_bill\", \"tip\", \"size\"]\n",
        "print(f\"Categorical columns identified: {cat_cols}\")\n",
        "print(f\"Numerical columns identified: {num_cols}\")\n",
        "\n",
        "# ---------------- 2.2 CREATE PIPELINE FOR NUMERICAL FEATURES ----------------\n",
        "# This pipeline will first fill any missing values with the median,\n",
        "# then scale the data to have a mean of 0 and a standard deviation of 1.\n",
        "num_pipeline = Pipeline(steps=[\n",
        "    ('imputation', SimpleImputer(strategy=\"median\")),\n",
        "    ('scaling', StandardScaler())\n",
        "])\n",
        "print(\"\\nNumerical pipeline created (Imputation -> Scaling).\")\n",
        "\n",
        "# ---------------- 2.3 CREATE PIPELINE FOR CATEGORICAL FEATURES ----------------\n",
        "# This pipeline will first fill missing values with the most frequent category,\n",
        "# then convert the categories into a numerical format using one-hot encoding.\n",
        "cat_pipeline = Pipeline(steps=[\n",
        "    ('imputation', SimpleImputer(strategy=\"most_frequent\")),\n",
        "    ('encoding', OneHotEncoder())\n",
        "])\n",
        "print(\"Categorical pipeline created (Imputation -> One-Hot Encoding).\")\n",
        "\n",
        "# ---------------- 2.4 COMBINE PIPELINES WITH COLUMNTRANSFORMER ----------------\n",
        "# ColumnTransformer applies the correct pipeline to the correct set of columns.\n",
        "preprocessor = ColumnTransformer([\n",
        "    (\"num_pipeline\", num_pipeline, num_cols),\n",
        "    (\"cat_pipeline\", cat_pipeline, cat_cols)\n",
        "])\n",
        "print(\"\\nPreprocessor created to apply pipelines to the correct columns.\")\n",
        "print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
        "\n",
        "\n",
        "# ====================================================================================\n",
        "# --- STEP 3: APPLY THE PREPROCESSOR TO THE DATA ---\n",
        "# ====================================================================================\n",
        "print(\"--- Step 3: Applying the Preprocessing Pipeline to the Data ---\")\n",
        "\n",
        "# We use fit_transform on the training data to learn the preprocessing steps\n",
        "# and apply them simultaneously.\n",
        "X_train_processed = preprocessor.fit_transform(X_train)\n",
        "\n",
        "# We use only transform on the test data to apply the same steps learned from the training data.\n",
        "X_test_processed = preprocessor.transform(X_test)\n",
        "\n",
        "print(\"Training and testing data have been successfully transformed.\")\n",
        "print(f\"Shape of processed training data: {X_train_processed.shape}\")\n",
        "print(f\"Shape of processed testing data: {X_test_processed.shape}\")\n",
        "print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
        "\n",
        "\n",
        "# ====================================================================================\n",
        "# --- STEP 4: TRAIN AND EVALUATE MULTIPLE MODELS ---\n",
        "# ====================================================================================\n",
        "print(\"--- Step 4: Training and Evaluating Multiple Classifiers ---\")\n",
        "\n",
        "# ---------------- 4.1 DEFINE MODELS AND EVALUATION FUNCTION ----------------\n",
        "# A dictionary to hold the models we want to train.\n",
        "models = {\n",
        "    \"support vector classifier\": SVC(),\n",
        "    \"DT classifier\": DecisionTreeClassifier(),\n",
        "    \"logistic regression\": LogisticRegression()\n",
        "}\n",
        "\n",
        "# A function to loop through the models, train each one, and return its accuracy.\n",
        "def model_train_eval(X_train, y_train, X_test, y_test, models):\n",
        "    evaluation = {}\n",
        "    for name, model in models.items():\n",
        "        model.fit(X_train, y_train)\n",
        "        y_pred = model.predict(X_test)\n",
        "        model_score = accuracy_score(y_test, y_pred)\n",
        "        evaluation[name] = model_score\n",
        "    return evaluation\n",
        "\n",
        "# ---------------- 4.2 RUN THE EVALUATION ----------------\n",
        "model_scores = model_train_eval(X_train_processed, y_train, X_test_processed, y_test, models)\n",
        "print(\"--- Model Evaluation Results (Accuracy) ---\")\n",
        "for name, score in model_scores.items():\n",
        "    print(f\"{name}: {score:.4f}\")\n",
        "\n",
        "print(\"\\n--- MASTER PIPELINE COMPLETE ---\")\n",
        "# ---------------- END OF SCRIPT ----------------\n"
      ],
      "metadata": {
        "id": "ThYBeynxuhtc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# OOB SCORE"
      ],
      "metadata": {
        "id": "Aac80hQyu7f2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ====================================\n",
        "# Random Forest Classifier with OOB\n",
        "# ====================================\n",
        "\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.datasets import make_classification\n",
        "\n",
        "# 1️⃣ Create a synthetic dataset\n",
        "X, y = make_classification(\n",
        "    n_samples=1000,     # 1000 samples\n",
        "    n_features=20,      # 20 features\n",
        "    n_classes=2,        # Binary classification\n",
        "    random_state=42     # Reproducibility\n",
        ")\n",
        "\n",
        "# 2️⃣ Initialize Random Forest Classifier\n",
        "rf_classifier = RandomForestClassifier(\n",
        "    n_estimators=100,   # 100 trees in the forest\n",
        "    oob_score=True,     # Use Out-of-Bag samples to estimate generalization\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# 3️⃣ Train the model\n",
        "rf_classifier.fit(X, y)\n",
        "\n",
        "# 4️⃣ Extract Out-of-Bag score\n",
        "oob_score = rf_classifier.oob_score_\n",
        "print(\"Out-of-Bag Score:\", oob_score)"
      ],
      "metadata": {
        "id": "m6Hq5Pgiu5yO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# RANDOM FORSEST CLASSIFIER"
      ],
      "metadata": {
        "id": "coENIGGGvdBU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------- AUTOMATED FEATURE ENGINEERING & MODEL EVALUATION PIPELINE ----------------\n",
        "# This single, unified script uses Pipeline and ColumnTransformer to preprocess data\n",
        "# and then trains, evaluates, and tunes a Random Forest Classifier.\n",
        "\n",
        "# Import required libraries\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler, OneHotEncoder\n",
        "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# ====================================================================================\n",
        "# --- STEP 1: LOAD AND PREPARE THE DATA ---\n",
        "# ====================================================================================\n",
        "print(\"--- Step 1: Loading and Preparing the 'tips' Dataset ---\")\n",
        "\n",
        "# ---------------- 1.1 LOAD DATA AND ENCODE TARGET VARIABLE ----------------\n",
        "df = sns.load_dataset(\"tips\")\n",
        "print(\"Original Data Head:\")\n",
        "print(df.head())\n",
        "\n",
        "# Use LabelEncoder to convert the target 'time' to a numerical format (0, 1).\n",
        "encoder = LabelEncoder()\n",
        "df['time'] = encoder.fit_transform(df['time'])\n",
        "print(\"\\n'time' column after Label Encoding (Lunch=0, Dinner=1).\")\n",
        "\n",
        "# ---------------- 1.2 SEPARATE FEATURES AND TARGET & SPLIT DATA ----------------\n",
        "X = df.drop('time', axis=1)\n",
        "y = df['time']\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=1)\n",
        "print(f\"\\nData split into {X_train.shape[0]} training and {X_test.shape[0]} testing samples.\")\n",
        "print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
        "\n",
        "\n",
        "# ====================================================================================\n",
        "# --- STEP 2: CREATE THE PREPROCESSING PIPELINE ---\n",
        "# ====================================================================================\n",
        "print(\"--- Step 2: Building an Automated Preprocessing Pipeline ---\")\n",
        "\n",
        "# ---------------- 2.1 DEFINE COLUMN TYPES ----------------\n",
        "cat_cols = [\"sex\", \"smoker\", \"day\"]\n",
        "num_cols = [\"total_bill\", \"tip\", \"size\"]\n",
        "print(f\"Categorical columns: {cat_cols}\")\n",
        "print(f\"Numerical columns: {num_cols}\")\n",
        "\n",
        "# ---------------- 2.2 CREATE SUB-PIPELINES ----------------\n",
        "num_pipeline = Pipeline(steps=[\n",
        "    ('imputation', SimpleImputer(strategy=\"median\")),\n",
        "    ('scaling', StandardScaler())\n",
        "])\n",
        "cat_pipeline = Pipeline(steps=[\n",
        "    ('imputation', SimpleImputer(strategy=\"most_frequent\")),\n",
        "    ('encoding', OneHotEncoder())\n",
        "])\n",
        "\n",
        "# ---------------- 2.3 COMBINE PIPELINES WITH COLUMNTRANSFORMER ----------------\n",
        "preprocessor = ColumnTransformer([\n",
        "    (\"num_pipeline\", num_pipeline, num_cols),\n",
        "    (\"cat_pipeline\", cat_pipeline, cat_cols)\n",
        "])\n",
        "print(\"\\nPreprocessor created to automate feature engineering.\")\n",
        "print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
        "\n",
        "\n",
        "# ====================================================================================\n",
        "# --- STEP 3: APPLY THE PREPROCESSOR AND TRAIN MULTIPLE MODELS ---\n",
        "# ====================================================================================\n",
        "print(\"--- Step 3: Preprocessing Data and Evaluating Multiple Models ---\")\n",
        "\n",
        "# ---------------- 3.1 APPLY THE PREPROCESSOR ----------------\n",
        "X_train_processed = preprocessor.fit_transform(X_train)\n",
        "X_test_processed = preprocessor.transform(X_test)\n",
        "print(\"Training and testing data have been successfully transformed.\")\n",
        "\n",
        "# ---------------- 3.2 DEFINE MODELS AND EVALUATION FUNCTION ----------------\n",
        "models = {\n",
        "    \"support vector classifier\": SVC(),\n",
        "    \"DT classifier\": DecisionTreeClassifier(),\n",
        "    \"Logistic regression\": LogisticRegression(),\n",
        "    \"Random_forest\": RandomForestClassifier() # Added Random Forest\n",
        "}\n",
        "\n",
        "def model_train_eval(X_train, y_train, X_test, y_test, models):\n",
        "    evaluation = {}\n",
        "    for name, model in models.items():\n",
        "        model.fit(X_train, y_train)\n",
        "        y_pred = model.predict(X_test)\n",
        "        model_score = accuracy_score(y_test, y_pred)\n",
        "        evaluation[name] = model_score\n",
        "    return evaluation\n",
        "\n",
        "# ---------------- 3.3 RUN THE INITIAL EVALUATION ----------------\n",
        "model_scores = model_train_eval(X_train_processed, y_train, X_test_processed, y_test, models)\n",
        "print(\"\\n--- Initial Model Evaluation Results (Accuracy) ---\")\n",
        "for name, score in model_scores.items():\n",
        "    print(f\"{name}: {score:.4f}\")\n",
        "print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
        "\n",
        "\n",
        "# ====================================================================================\n",
        "# --- STEP 4: HYPERPARAMETER TUNING FOR RANDOM FOREST ---\n",
        "# ====================================================================================\n",
        "print(\"--- Step 4: Hyperparameter Tuning for Random Forest Classifier ---\")\n",
        "\n",
        "# ---------------- 4.1 DEFINE HYPERPARAMETER GRID ----------------\n",
        "rf = RandomForestClassifier(random_state=1)\n",
        "params = {\n",
        "    'max_depth': [1, 2, 3, 5, 10, None],\n",
        "    'n_estimators': [50, 100, 200, 300],\n",
        "    'criterion': ['gini', 'entropy']\n",
        "}\n",
        "print(\"Defined parameter grid for hyperparameter search.\")\n",
        "\n",
        "# ---------------- 4.2 PERFORM RANDOMIZED SEARCH WITH CROSS-VALIDATION ----------------\n",
        "clf = RandomizedSearchCV(\n",
        "    estimator=rf,\n",
        "    param_distributions=params,\n",
        "    cv=5,\n",
        "    verbose=3,\n",
        "    scoring='accuracy',\n",
        "    random_state=1 # For reproducibility of the search\n",
        ")\n",
        "print(\"\\nRunning RandomizedSearchCV... (This may take a moment)\")\n",
        "clf.fit(X_train_processed, y_train)\n",
        "\n",
        "# ---------------- 4.3 DISPLAY BEST PARAMETERS AND SCORE ----------------\n",
        "print(\"\\n--- RandomizedSearchCV Results ---\")\n",
        "print(\"Best Hyperparameters Found:\")\n",
        "print(clf.best_params_)\n",
        "print(f\"\\nBest Cross-Validated Accuracy Score: {clf.best_score_:.4f}\")\n",
        "print(\"\\n--- MASTER PIPELINE COMPLETE ---\")\n",
        "# ---------------- END OF SCRIPT ----------------\n",
        "\n"
      ],
      "metadata": {
        "id": "LTnWTSNQvbHM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Multiple models with Pipeline and ColumnTransformer"
      ],
      "metadata": {
        "id": "9_gQn63AvkWd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------- AUTOMATED REGRESSION & MODEL EVALUATION PIPELINE ----------------\n",
        "# This single, unified script uses Pipeline and ColumnTransformer to preprocess data\n",
        "# and then trains, evaluates, and tunes a Random Forest Regressor.\n",
        "\n",
        "# Import required libraries\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "# ====================================================================================\n",
        "# --- STEP 1: LOAD AND PREPARE THE DATA FOR REGRESSION ---\n",
        "# ====================================================================================\n",
        "print(\"--- Step 1: Loading and Preparing the 'tips' Dataset for a Regression Task ---\")\n",
        "\n",
        "# ---------------- 1.1 LOAD DATA AND DEFINE TARGET ----------------\n",
        "df = sns.load_dataset(\"tips\")\n",
        "print(\"Original Data Head:\")\n",
        "print(df.head())\n",
        "\n",
        "# For this regression task, the target variable is 'total_bill'.\n",
        "X = df.drop('total_bill', axis=1)  # Features are all other columns\n",
        "y = df['total_bill']               # Target variable\n",
        "\n",
        "# ---------------- 1.2 SPLIT DATA ----------------\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=1)\n",
        "print(f\"\\nData split into {X_train.shape[0]} training and {X_test.shape[0]} testing samples.\")\n",
        "print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
        "\n",
        "\n",
        "# ====================================================================================\n",
        "# --- STEP 2: CREATE THE PREPROCESSING PIPELINE ---\n",
        "# ====================================================================================\n",
        "print(\"--- Step 2: Building an Automated Preprocessing Pipeline ---\")\n",
        "\n",
        "# ---------------- 2.1 DEFINE COLUMN TYPES ----------------\n",
        "# Identify which columns are categorical and which are numerical\n",
        "cat_cols = [\"sex\", \"smoker\", \"day\", \"time\"] # 'time' is now a feature\n",
        "num_cols = [\"tip\", \"size\"]\n",
        "print(f\"Categorical columns: {cat_cols}\")\n",
        "print(f\"Numerical columns: {num_cols}\")\n",
        "\n",
        "# ---------------- 2.2 CREATE SUB-PIPELINES ----------------\n",
        "num_pipeline = Pipeline(steps=[\n",
        "    ('imputation', SimpleImputer(strategy=\"median\")),\n",
        "    ('scaling', StandardScaler())\n",
        "])\n",
        "cat_pipeline = Pipeline(steps=[\n",
        "    ('imputation', SimpleImputer(strategy=\"most_frequent\")),\n",
        "    ('encoding', OneHotEncoder())\n",
        "])\n",
        "\n",
        "# ---------------- 2.3 COMBINE PIPELINES WITH COLUMNTRANSFORMER ----------------\n",
        "preprocessor = ColumnTransformer([\n",
        "    (\"num_pipeline\", num_pipeline, num_cols),\n",
        "    (\"cat_pipeline\", cat_pipeline, cat_cols)\n",
        "])\n",
        "print(\"\\nPreprocessor created to automate feature engineering for regression.\")\n",
        "print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
        "\n",
        "\n",
        "# ====================================================================================\n",
        "# --- STEP 3: APPLY PREPROCESSOR AND EVALUATE MULTIPLE MODELS ---\n",
        "# ====================================================================================\n",
        "print(\"--- Step 3: Preprocessing Data and Evaluating Multiple Regression Models ---\")\n",
        "\n",
        "# ---------------- 3.1 APPLY THE PREPROCESSOR ----------------\n",
        "X_train_processed = preprocessor.fit_transform(X_train)\n",
        "X_test_processed = preprocessor.transform(X_test)\n",
        "print(\"Training and testing data have been successfully transformed.\")\n",
        "\n",
        "# ---------------- 3.2 DEFINE MODELS AND EVALUATION FUNCTION ----------------\n",
        "models = {\n",
        "    \"Support Vector Regressor\": SVR(),\n",
        "    \"Decision Tree Regressor\": DecisionTreeRegressor(),\n",
        "    \"Multiple Linear Regression\": LinearRegression(),\n",
        "    \"Random Forest Regressor\": RandomForestRegressor()\n",
        "}\n",
        "\n",
        "def model_train_eval(X_train, y_train, X_test, y_test, models):\n",
        "    evaluation = {}\n",
        "    for name, model in models.items():\n",
        "        model.fit(X_train, y_train)\n",
        "        y_pred = model.predict(X_test)\n",
        "        model_score = r2_score(y_test, y_pred)\n",
        "        evaluation[name] = model_score\n",
        "    return evaluation\n",
        "\n",
        "# ---------------- 3.3 RUN THE INITIAL EVALUATION ----------------\n",
        "model_scores = model_train_eval(X_train_processed, y_train, X_test_processed, y_test, models)\n",
        "print(\"\\n--- Initial Model Evaluation Results (R-squared Score) ---\")\n",
        "for name, score in model_scores.items():\n",
        "    print(f\"{name}: {score:.4f}\")\n",
        "print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
        "\n",
        "\n",
        "# ====================================================================================\n",
        "# --- STEP 4: HYPERPARAMETER TUNING FOR RANDOM FOREST REGRESSOR ---\n",
        "# ====================================================================================\n",
        "print(\"--- Step 4: Hyperparameter Tuning for Random Forest Regressor ---\")\n",
        "\n",
        "# ---------------- 4.1 DEFINE HYPERPARAMETER GRID ----------------\n",
        "rfr = RandomForestRegressor(oob_score=True, random_state=42)\n",
        "params = {\n",
        "    'max_depth': [1, 50, 100, 150, 200],\n",
        "    'n_estimators': [50, 100, 200]\n",
        "}\n",
        "print(\"Defined parameter grid for hyperparameter search.\")\n",
        "\n",
        "# ---------------- 4.2 PERFORM RANDOMIZED SEARCH WITH CROSS-VALIDATION ----------------\n",
        "reg = RandomizedSearchCV(\n",
        "    estimator=rfr,\n",
        "    param_distributions=params,\n",
        "    cv=5,\n",
        "    verbose=3,\n",
        "    scoring='r2',\n",
        "    n_iter=10,\n",
        "    random_state=42\n",
        ")\n",
        "print(\"\\nRunning RandomizedSearchCV... (This may take a moment)\")\n",
        "reg.fit(X_train_processed, y_train)\n",
        "\n",
        "# ---------------- 4.3 DISPLAY BEST PARAMETERS AND SCORE ----------------\n",
        "print(\"\\n--- RandomizedSearchCV Results ---\")\n",
        "print(\"Best Hyperparameters Found:\")\n",
        "print(reg.best_params_)\n",
        "print(f\"\\nBest Cross-Validated R2 Score: {reg.best_score_:.4f}\")\n",
        "print(\"\\n--- MASTER PIPELINE COMPLETE ---\")\n",
        "\n",
        "# ---------------- END OF SCRIPT ----------------\n"
      ],
      "metadata": {
        "id": "jt8d4sIevpy7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# AdaBoost classifier and regressor"
      ],
      "metadata": {
        "id": "7iGNmBX4wK7I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------- ADABOOST CLASSIFIER & REGRESSOR PIPELINE ----------------\n",
        "# This single, unified script combines all the code you provided for both the\n",
        "# AdaBoost Classifier and the AdaBoost Regressor, including hyperparameter tuning.\n",
        "\n",
        "# Import required libraries\n",
        "import warnings\n",
        "from sklearn.datasets import make_classification, make_regression\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.ensemble import AdaBoostClassifier, AdaBoostRegressor\n",
        "from sklearn.metrics import (accuracy_score, classification_report, confusion_matrix,\n",
        "                             r2_score, mean_absolute_error, mean_squared_error)\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# ====================================================================================\n",
        "# --- PART 1: ADABOOST CLASSIFIER ---\n",
        "# ====================================================================================\n",
        "print(\"--- Part 1: Building and Tuning an AdaBoost Classifier ---\")\n",
        "\n",
        "# ---------------- 1.1 GENERATE AND SPLIT CLASSIFICATION DATA ----------------\n",
        "print(\"\\nStep 1.1: Generating synthetic classification data...\")\n",
        "X_clf, y_clf = make_classification(n_samples=1000, n_features=20, n_classes=2, random_state=1)\n",
        "X_clf_train, X_clf_test, y_clf_train, y_clf_test = train_test_split(X_clf, y_clf, test_size=0.33, random_state=42)\n",
        "print(f\"Classification data split into {X_clf_train.shape[0]} training and {X_clf_test.shape[0]} testing samples.\")\n",
        "\n",
        "# ---------------- 1.2 TRAIN AND EVALUATE INITIAL CLASSIFIER ----------------\n",
        "print(\"\\nStep 1.2: Training and evaluating the initial (untuned) AdaBoost Classifier...\")\n",
        "classifier = AdaBoostClassifier(random_state=1)\n",
        "classifier.fit(X_clf_train, y_clf_train)\n",
        "y_pred_initial_clf = classifier.predict(X_clf_test)\n",
        "\n",
        "print(\"\\n--- Initial Classifier Performance ---\")\n",
        "print(f\"Accuracy: {accuracy_score(y_clf_test, y_pred_initial_clf):.4f}\")\n",
        "print(\"Classification Report:\\n\", classification_report(y_clf_test, y_pred_initial_clf))\n",
        "print(\"Confusion Matrix:\\n\", confusion_matrix(y_clf_test, y_pred_initial_clf))\n",
        "\n",
        "# ---------------- 1.3 HYPERPARAMETER TUNING WITH GRIDSEARCHCV ----------------\n",
        "print(\"\\nStep 1.3: Performing hyperparameter tuning for the classifier...\")\n",
        "param_grid_clf = {\n",
        "    'n_estimators': [50, 100, 200],\n",
        "    'learning_rate': [0.01, 0.1, 1.0, 1.5],\n",
        "    'algorithm': ['SAMME', 'SAMME.R']\n",
        "}\n",
        "ada_clf = AdaBoostClassifier(random_state=1)\n",
        "clf_grid = GridSearchCV(estimator=ada_clf, param_grid=param_grid_clf, cv=5, verbose=3, n_jobs=-1)\n",
        "print(\"Running GridSearchCV for Classifier... (This may take a moment)\")\n",
        "clf_grid.fit(X_clf_train, y_clf_train)\n",
        "\n",
        "print(\"\\nBest Hyperparameters Found for Classifier:\", clf_grid.best_params_)\n",
        "best_clf_model = clf_grid.best_estimator_\n",
        "\n",
        "# ---------------- 1.4 EVALUATE THE TUNED CLASSIFIER ----------------\n",
        "print(\"\\nStep 1.4: Evaluating the tuned AdaBoost Classifier...\")\n",
        "y_pred_tuned_clf = best_clf_model.predict(X_clf_test)\n",
        "\n",
        "print(\"\\n--- Tuned Classifier Performance ---\")\n",
        "print(f\"Accuracy: {accuracy_score(y_clf_test, y_pred_tuned_clf):.4f}\")\n",
        "print(\"Classification Report:\\n\", classification_report(y_clf_test, y_pred_tuned_clf))\n",
        "print(\"Confusion Matrix:\\n\", confusion_matrix(y_clf_test, y_pred_tuned_clf))\n",
        "print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
        "\n",
        "\n",
        "# ====================================================================================\n",
        "# --- PART 2: ADABOOST REGRESSOR ---\n",
        "# ====================================================================================\n",
        "print(\"--- Part 2: Building and Tuning an AdaBoost Regressor ---\")\n",
        "\n",
        "# ---------------- 2.1 GENERATE AND SPLIT REGRESSION DATA ----------------\n",
        "print(\"\\nStep 2.1: Generating synthetic regression data...\")\n",
        "X_reg, y_reg = make_regression(n_samples=1000, n_features=2, noise=10, random_state=1)\n",
        "X_reg_train, X_reg_test, y_reg_train, y_reg_test = train_test_split(X_reg, y_reg, test_size=0.33, random_state=42)\n",
        "print(f\"Regression data split into {X_reg_train.shape[0]} training and {X_reg_test.shape[0]} testing samples.\")\n",
        "\n",
        "# ---------------- 2.2 TRAIN AND EVALUATE INITIAL REGRESSOR ----------------\n",
        "print(\"\\nStep 2.2: Training and evaluating the initial (untuned) AdaBoost Regressor...\")\n",
        "regressor = AdaBoostRegressor(random_state=1)\n",
        "regressor.fit(X_reg_train, y_reg_train)\n",
        "y_pred_initial_reg = regressor.predict(X_reg_test)\n",
        "\n",
        "print(\"\\n--- Initial Regressor Performance ---\")\n",
        "print(f\"R2 score: {r2_score(y_reg_test, y_pred_initial_reg):.4f}\")\n",
        "print(f\"Mean Absolute Error: {mean_absolute_error(y_reg_test, y_pred_initial_reg):.4f}\")\n",
        "print(f\"Mean Squared Error: {mean_squared_error(y_reg_test, y_pred_initial_reg):.4f}\")\n",
        "\n",
        "# ---------------- 2.3 HYPERPARAMETER TUNING WITH GRIDSEARCHCV ----------------\n",
        "print(\"\\nStep 2.3: Performing hyperparameter tuning for the regressor...\")\n",
        "param_grid_reg = {\n",
        "    'n_estimators': [50, 100, 200],\n",
        "    'learning_rate': [0.01, 0.1, 0.5, 1.0],\n",
        "    'loss': ['linear', 'square', 'exponential']\n",
        "}\n",
        "ada_reg = AdaBoostRegressor(random_state=1)\n",
        "grid_search_reg = GridSearchCV(estimator=ada_reg, param_grid=param_grid_reg, cv=5, verbose=3)\n",
        "print(\"Running GridSearchCV for Regressor... (This may take a moment)\")\n",
        "grid_search_reg.fit(X_reg_train, y_reg_train)\n",
        "\n",
        "print(\"\\nBest Hyperparameters Found for Regressor:\", grid_search_reg.best_params_)\n",
        "best_reg_model = grid_search_reg.best_estimator_\n",
        "\n",
        "# ---------------- 2.4 EVALUATE THE TUNED REGRESSOR ----------------\n",
        "print(\"\\nStep 2.4: Evaluating the tuned AdaBoost Regressor...\")\n",
        "y_pred_tuned_reg = best_reg_model.predict(X_reg_test)\n",
        "\n",
        "print(\"\\n--- Tuned Regressor Performance ---\")\n",
        "print(f\"R2 score: {r2_score(y_reg_test, y_pred_tuned_reg):.4f}\")\n",
        "print(f\"Mean Absolute Error: {mean_absolute_error(y_reg_test, y_pred_tuned_reg):.4f}\")\n",
        "print(f\"Mean Squared Error: {mean_squared_error(y_reg_test, y_pred_tuned_reg):.4f}\")\n",
        "print(\"\\n--- MASTER PIPELINE COMPLETE ---\")\n",
        "\n",
        "# ---------------- END OF SCRIPT ----------------\n"
      ],
      "metadata": {
        "id": "EruYmzapwMgw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GRADIENT BOOSTING CLASSIFIER"
      ],
      "metadata": {
        "id": "4Fe0dLFWx0JZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------- GRADIENT BOOSTING CLASSIFIER & REGRESSOR PIPELINE ----------------\n",
        "# This single, unified script combines all the code you provided for both the\n",
        "# Gradient Boosting Classifier and the Gradient Boosting Regressor, including hyperparameter tuning.\n",
        "\n",
        "# Import required libraries\n",
        "import warnings\n",
        "from sklearn.datasets import make_classification, make_regression\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.ensemble import GradientBoostingClassifier, GradientBoostingRegressor\n",
        "from sklearn.metrics import (accuracy_score, classification_report, confusion_matrix,\n",
        "                             r2_score, mean_absolute_error, mean_squared_error)\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# ====================================================================================\n",
        "# --- PART 1: GRADIENT BOOSTING CLASSIFIER ---\n",
        "# ====================================================================================\n",
        "print(\"--- Part 1: Building and Tuning a Gradient Boosting Classifier ---\")\n",
        "\n",
        "# ---------------- 1.1 GENERATE AND SPLIT CLASSIFICATION DATA ----------------\n",
        "print(\"\\nStep 1.1: Generating synthetic classification data...\")\n",
        "X_clf, y_clf = make_classification(n_samples=1000, n_features=20, n_classes=2, random_state=1)\n",
        "X_clf_train, X_clf_test, y_clf_train, y_clf_test = train_test_split(X_clf, y_clf, test_size=0.33, random_state=1)\n",
        "print(f\"Classification data split into {X_clf_train.shape[0]} training and {X_clf_test.shape[0]} testing samples.\")\n",
        "\n",
        "# ---------------- 1.2 TRAIN AND EVALUATE INITIAL CLASSIFIER ----------------\n",
        "print(\"\\nStep 1.2: Training and evaluating the initial (untuned) Gradient Boosting Classifier...\")\n",
        "clf = GradientBoostingClassifier(random_state=1)\n",
        "clf.fit(X_clf_train, y_clf_train)\n",
        "y_pred_initial_clf = clf.predict(X_clf_test)\n",
        "\n",
        "print(\"\\n--- Initial Classifier Performance ---\")\n",
        "print(f\"Accuracy: {accuracy_score(y_clf_test, y_pred_initial_clf):.4f}\")\n",
        "print(\"Classification Report:\\n\", classification_report(y_clf_test, y_pred_initial_clf))\n",
        "print(\"Confusion Matrix:\\n\", confusion_matrix(y_clf_test, y_pred_initial_clf))\n",
        "\n",
        "# ---------------- 1.3 HYPERPARAMETER TUNING WITH GRIDSEARCHCV ----------------\n",
        "print(\"\\nStep 1.3: Performing hyperparameter tuning for the classifier...\")\n",
        "param_grid_clf = {\n",
        "    'n_estimators': [100, 200, 300],\n",
        "    'learning_rate': [0.01, 0.1, 0.05, 0.2]\n",
        "}\n",
        "gbc = GradientBoostingClassifier(random_state=1)\n",
        "grid_search_clf = GridSearchCV(estimator=gbc, param_grid=param_grid_clf, cv=5, verbose=3)\n",
        "print(\"Running GridSearchCV for Classifier... (This may take a moment)\")\n",
        "grid_search_clf.fit(X_clf_train, y_clf_train)\n",
        "\n",
        "print(\"\\nBest Hyperparameters Found for Classifier:\", grid_search_clf.best_params_)\n",
        "best_clf_model = grid_search_clf.best_estimator_\n",
        "\n",
        "# ---------------- 1.4 EVALUATE THE TUNED CLASSIFIER ----------------\n",
        "print(\"\\nStep 1.4: Evaluating the tuned Gradient Boosting Classifier...\")\n",
        "y_pred_tuned_clf = best_clf_model.predict(X_clf_test)\n",
        "\n",
        "print(\"\\n--- Tuned Classifier Performance ---\")\n",
        "print(f\"Accuracy: {accuracy_score(y_clf_test, y_pred_tuned_clf):.4f}\")\n",
        "print(\"Classification Report:\\n\", classification_report(y_clf_test, y_pred_tuned_clf))\n",
        "print(\"Confusion Matrix:\\n\", confusion_matrix(y_clf_test, y_pred_tuned_clf))\n",
        "print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
        "\n",
        "\n",
        "# ====================================================================================\n",
        "# --- PART 2: GRADIENT BOOSTING REGRESSOR ---\n",
        "# ====================================================================================\n",
        "print(\"--- Part 2: Building and Tuning a Gradient Boosting Regressor ---\")\n",
        "\n",
        "# ---------------- 2.1 GENERATE AND SPLIT REGRESSION DATA ----------------\n",
        "print(\"\\nStep 2.1: Generating synthetic regression data...\")\n",
        "X_reg, y_reg = make_regression(n_samples=1000, n_features=2, noise=10, random_state=42)\n",
        "X_reg_train, X_reg_test, y_reg_train, y_reg_test = train_test_split(X_reg, y_reg, test_size=0.33, random_state=42)\n",
        "print(f\"Regression data split into {X_reg_train.shape[0]} training and {X_reg_test.shape[0]} testing samples.\")\n",
        "\n",
        "# ---------------- 2.2 TRAIN AND EVALUATE INITIAL REGRESSOR ----------------\n",
        "print(\"\\nStep 2.2: Training and evaluating the initial (untuned) Gradient Boosting Regressor...\")\n",
        "regressor = GradientBoostingRegressor(random_state=1)\n",
        "regressor.fit(X_reg_train, y_reg_train)\n",
        "y_pred_initial_reg = regressor.predict(X_reg_test)\n",
        "\n",
        "print(\"\\n--- Initial Regressor Performance ---\")\n",
        "print(f\"R2 score: {r2_score(y_reg_test, y_pred_initial_reg):.4f}\")\n",
        "print(f\"Mean Absolute Error: {mean_absolute_error(y_reg_test, y_pred_initial_reg):.4f}\")\n",
        "print(f\"Mean Squared Error: {mean_squared_error(y_reg_test, y_pred_initial_reg):.4f}\")\n",
        "\n",
        "# ---------------- 2.3 HYPERPARAMETER TUNING WITH GRIDSEARCHCV ----------------\n",
        "print(\"\\nStep 2.3: Performing hyperparameter tuning for the regressor...\")\n",
        "param_grid_reg = {\n",
        "    'n_estimators': [100, 200, 300],\n",
        "    'learning_rate': [0.01, 0.05, 0.1, 0.2]\n",
        "}\n",
        "gbr = GradientBoostingRegressor(random_state=1)\n",
        "grid_search_reg = GridSearchCV(estimator=gbr, param_grid=param_grid_reg, cv=5, verbose=3)\n",
        "print(\"Running GridSearchCV for Regressor... (This may take a moment)\")\n",
        "grid_search_reg.fit(X_reg_train, y_reg_train)\n",
        "\n",
        "print(\"\\nBest Hyperparameters Found for Regressor:\", grid_search_reg.best_params_)\n",
        "best_reg_model = grid_search_reg.best_estimator_\n",
        "\n",
        "# ---------------- 2.4 EVALUATE THE TUNED REGRESSOR ----------------\n",
        "print(\"\\nStep 2.4: Evaluating the tuned Gradient Boosting Regressor...\")\n",
        "y_pred_tuned_reg = best_reg_model.predict(X_reg_test)\n",
        "\n",
        "print(\"\\n--- Tuned Regressor Performance ---\")\n",
        "print(f\"R2 score: {r2_score(y_reg_test, y_pred_tuned_reg):.4f}\")\n",
        "print(f\"Mean Absolute Error: {mean_absolute_error(y_reg_test, y_pred_tuned_reg):.4f}\")\n",
        "print(f\"Mean Squared Error: {mean_squared_error(y_reg_test, y_pred_tuned_reg):.4f}\")\n",
        "print(\"\\n--- MASTER PIPELINE COMPLETE ---\")\n",
        "\n",
        "# ---------------- END OF SCRIPT ----------------\n"
      ],
      "metadata": {
        "id": "rOYlPg1qxvVI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# XGBOOST CLASSIFIER"
      ],
      "metadata": {
        "id": "MsgdVyxlx3px"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------- XGBOOST CLASSIFIER & REGRESSOR PIPELINE ----------------\n",
        "# This single, unified script combines all the code you provided for both the\n",
        "# XGBoost Classifier and the XGBoost Regressor, including hyperparameter tuning.\n",
        "\n",
        "# Import required libraries\n",
        "import warnings\n",
        "from sklearn.datasets import make_classification, make_regression\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from xgboost import XGBClassifier, XGBRegressor\n",
        "from sklearn.metrics import (accuracy_score, classification_report, confusion_matrix,\n",
        "                             r2_score, mean_absolute_error, mean_squared_error)\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# ====================================================================================\n",
        "# --- PART 1: XGBOOST CLASSIFIER ---\n",
        "# ====================================================================================\n",
        "print(\"--- Part 1: Building and Tuning an XGBoost Classifier ---\")\n",
        "\n",
        "# ---------------- 1.1 GENERATE AND SPLIT CLASSIFICATION DATA ----------------\n",
        "print(\"\\nStep 1.1: Generating synthetic classification data...\")\n",
        "# make_classification ek synthetic dataset banata hai classification tasks ke liye\n",
        "# n_samples=1000 → total 1000 rows\n",
        "# n_features=20 → har row me 20 input features\n",
        "# n_classes=2 → binary classification (0 ya 1 output)\n",
        "# random_state=1 → reproducibility ke liye same random split\n",
        "X_clf, y_clf = make_classification(n_samples=1000, n_features=20, n_classes=2, random_state=1)\n",
        "# Dataset ko train aur test set me split karna\n",
        "# test_size=0.33 → 33% data test ke liye, 67% train ke liye\n",
        "X_clf_train, X_clf_test, y_clf_train, y_clf_test = train_test_split(X_clf, y_clf, test_size=0.33, random_state=1)\n",
        "print(f\"Classification data split into {X_clf_train.shape[0]} training and {X_clf_test.shape[0]} testing samples.\")\n",
        "\n",
        "# ---------------- 1.2 TRAIN AND EVALUATE INITIAL CLASSIFIER ----------------\n",
        "print(\"\\nStep 1.2: Training and evaluating the initial (untuned) XGBoost Classifier...\")\n",
        "# XGBClassifier ka ek object banaya (default hyperparameters ke sath)\n",
        "classifier = XGBClassifier(eval_metric='logloss', use_label_encoder=False)\n",
        "# Model ko training karna (fit karna) training dataset (X_train, y_train) par\n",
        "classifier.fit(X_clf_train, y_clf_train)\n",
        "# Model se predictions lena\n",
        "y_pred_initial_clf = classifier.predict(X_clf_test)\n",
        "\n",
        "print(\"\\n--- Initial Classifier Performance ---\")\n",
        "print(f\"Accuracy: {accuracy_score(y_clf_test, y_pred_initial_clf):.4f}\")\n",
        "print(\"Classification Report:\\n\", classification_report(y_clf_test, y_pred_initial_clf))\n",
        "print(\"Confusion Matrix:\\n\", confusion_matrix(y_clf_test, y_pred_initial_clf))\n",
        "\n",
        "# ---------------- 1.3 HYPERPARAMETER TUNING WITH GRIDSEARCHCV ----------------\n",
        "print(\"\\nStep 1.3: Performing hyperparameter tuning for the classifier...\")\n",
        "# param_grid ek dictionary hai jisme hyperparameters aur unke values ki list di jaati hai\n",
        "param_grid_clf = {\n",
        "    'n_estimators': [100, 200, 300],\n",
        "    'learning_rate': [0.01, 0.05, 0.1, 0.2]\n",
        "}\n",
        "# GridSearchCV lagana\n",
        "grid_search_clf = GridSearchCV(estimator=classifier, param_grid=param_grid_clf, cv=5, n_jobs=-1, verbose=3)\n",
        "print(\"Running GridSearchCV for Classifier... (This may take a moment)\")\n",
        "grid_search_clf.fit(X_clf_train, y_clf_train)\n",
        "\n",
        "# Best parameters milenge jo sabse acha performance dete hain cross-validation me\n",
        "print(\"\\nBest Hyperparameters Found for Classifier:\", grid_search_clf.best_params_)\n",
        "# Best tuned model ko extract karna\n",
        "best_clf_model = grid_search_clf.best_estimator_\n",
        "\n",
        "# ---------------- 1.4 EVALUATE THE TUNED CLASSIFIER ----------------\n",
        "print(\"\\nStep 1.4: Evaluating the tuned XGBoost Classifier...\")\n",
        "# Best tuned model se predictions lena test data ke liye\n",
        "y_pred_tuned_clf = best_clf_model.predict(X_clf_test)\n",
        "\n",
        "print(\"\\n--- Tuned Classifier Performance ---\")\n",
        "print(f\"Accuracy: {accuracy_score(y_clf_test, y_pred_tuned_clf):.4f}\")\n",
        "print(\"Classification Report:\\n\", classification_report(y_clf_test, y_pred_tuned_clf))\n",
        "print(\"Confusion Matrix:\\n\", confusion_matrix(y_clf_test, y_pred_tuned_clf))\n",
        "print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
        "\n",
        "\n",
        "# ====================================================================================\n",
        "# --- PART 2: XGBOOST REGRESSOR ---\n",
        "# ====================================================================================\n",
        "print(\"--- Part 2: Building and Tuning an XGBoost Regressor ---\")\n",
        "\n",
        "# ---------------- 2.1 GENERATE AND SPLIT REGRESSION DATA ----------------\n",
        "print(\"\\nStep 2.1: Generating synthetic regression data...\")\n",
        "X_reg, y_reg = make_regression(n_samples=1000, n_features=2, noise=10, random_state=1)\n",
        "X_reg_train, X_reg_test, y_reg_train, y_reg_test = train_test_split(X_reg, y_reg, test_size=0.33, random_state=1)\n",
        "print(f\"Regression data split into {X_reg_train.shape[0]} training and {X_reg_test.shape[0]} testing samples.\")\n",
        "\n",
        "# ---------------- 2.2 TRAIN AND EVALUATE INITIAL REGRESSOR ----------------\n",
        "print(\"\\nStep 2.2: Training and evaluating the initial (untuned) XGBoost Regressor...\")\n",
        "regressor = XGBRegressor(random_state=1)\n",
        "regressor.fit(X_reg_train, y_reg_train)\n",
        "y_pred_initial_reg = regressor.predict(X_reg_test)\n",
        "\n",
        "print(\"\\n--- Initial Regressor Performance ---\")\n",
        "print(f\"R2 score: {r2_score(y_reg_test, y_pred_initial_reg):.4f}\")\n",
        "print(f\"Mean Absolute Error: {mean_absolute_error(y_reg_test, y_pred_initial_reg):.4f}\")\n",
        "print(f\"Mean Squared Error: {mean_squared_error(y_reg_test, y_pred_initial_reg):.4f}\")\n",
        "\n",
        "# ---------------- 2.3 HYPERPARAMETER TUNING WITH GRIDSEARCHCV ----------------\n",
        "print(\"\\nStep 2.3: Performing hyperparameter tuning for the regressor...\")\n",
        "param_grid_reg = {\n",
        "    'n_estimators': [100, 200, 300],\n",
        "    'learning_rate': [0.01, 0.05, 0.1, 0.2]\n",
        "}\n",
        "grid_search_reg = GridSearchCV(estimator=regressor, param_grid=param_grid_reg, cv=5, n_jobs=-1, verbose=2)\n",
        "print(\"Running GridSearchCV for Regressor... (This may take a moment)\")\n",
        "grid_search_reg.fit(X_reg_train, y_reg_train)\n",
        "\n",
        "print(\"\\nBest Hyperparameters Found for Regressor:\", grid_search_reg.best_params_)\n",
        "best_reg_model = grid_search_reg.best_estimator_\n",
        "\n",
        "# ---------------- 2.4 EVALUATE THE TUNED REGRESSOR ----------------\n",
        "print(\"\\nStep 2.4: Evaluating the tuned XGBoost Regressor...\")\n",
        "y_pred_tuned_reg = best_reg_model.predict(X_reg_test)\n",
        "\n",
        "print(\"\\n--- Tuned Regressor Performance ---\")\n",
        "print(f\"R2 score: {r2_score(y_reg_test, y_pred_tuned_reg):.4f}\")\n",
        "print(f\"Mean Absolute Error: {mean_absolute_error(y_reg_test, y_pred_tuned_reg):.4f}\")\n",
        "print(f\"Mean Squared Error: {mean_squared_error(y_reg_test, y_pred_tuned_reg):.4f}\")\n",
        "print(\"\\n--- MASTER PIPELINE COMPLETE ---\")\n",
        "\n",
        "# ---------------- END OF SCRIPT ----------------\n"
      ],
      "metadata": {
        "id": "5bkZsSfPyZoR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# k nearest neighbhour classifier AND regressor with hyperparameter tunning"
      ],
      "metadata": {
        "id": "F6IpUU_gygnJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------- K-NEAREST NEIGHBORS (KNN) CLASSIFIER & REGRESSOR PIPELINE ----------------\n",
        "# This single, unified script combines all the code you provided for both the\n",
        "# KNN Classifier and the KNN Regressor, including hyperparameter tuning.\n",
        "\n",
        "# Import required libraries\n",
        "import warnings\n",
        "from sklearn.datasets import make_classification, make_regression\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor\n",
        "from sklearn.metrics import (accuracy_score, classification_report, confusion_matrix, r2_score)\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# ====================================================================================\n",
        "# --- PART 1: K-NEAREST NEIGHBORS (KNN) CLASSIFIER ---\n",
        "# ====================================================================================\n",
        "print(\"--- Part 1: Building and Tuning a KNN Classifier ---\")\n",
        "\n",
        "# ---------------- 1.1 GENERATE AND SPLIT CLASSIFICATION DATA ----------------\n",
        "print(\"\\nStep 1.1: Generating synthetic classification data...\")\n",
        "# make_classification function ek synthetic dataset generate karta hai\n",
        "# n_samples=1000 → 1000 rows\n",
        "# n_features=3   → 3 independent features\n",
        "# n_classes=2    → binary classification (0 aur 1)\n",
        "X_clf, y_clf = make_classification(n_samples=1000, n_features=3, n_redundant=0, n_classes=2, random_state=1)\n",
        "# train_test_split → dataset ko training aur testing parts me divide karne ke liye\n",
        "# test_size=0.30 → 30% data testing ke liye\n",
        "X_clf_train, X_clf_test, y_clf_train, y_clf_test = train_test_split(X_clf, y_clf, test_size=0.30, random_state=1)\n",
        "print(f\"Classification data split into {X_clf_train.shape[0]} training and {X_clf_test.shape[0]} testing samples.\")\n",
        "\n",
        "# ---------------- 1.2 TRAIN AND EVALUATE INITIAL CLASSIFIER ----------------\n",
        "print(\"\\nStep 1.2: Training and evaluating the initial (untuned) KNN Classifier...\")\n",
        "# KNN classifier ka object banaya (default: n_neighbors=5)\n",
        "clf = KNeighborsClassifier()\n",
        "# training data ke upar model ko train karna\n",
        "clf.fit(X_clf_train, y_clf_train)\n",
        "# testing data (X_test) ke liye predictions lena\n",
        "y_pred_initial_clf = clf.predict(X_clf_test)\n",
        "\n",
        "print(\"\\n--- Initial Classifier Performance ---\")\n",
        "print(\"Confusion Matrix:\\n\", confusion_matrix(y_clf_test, y_pred_initial_clf))\n",
        "print(f\"Accuracy: {accuracy_score(y_clf_test, y_pred_initial_clf):.4f}\")\n",
        "print(\"Classification Report:\\n\", classification_report(y_clf_test, y_pred_initial_clf))\n",
        "\n",
        "# ---------------- 1.3 HYPERPARAMETER TUNING WITH GRIDSEARCHCV ----------------\n",
        "print(\"\\nStep 1.3: Performing hyperparameter tuning for the classifier...\")\n",
        "# param_grid → alag-alag parameter combinations try karne ke liye dictionary\n",
        "param_grid_clf = {\n",
        "    'n_neighbors': [3, 5, 6, 7, 9, 11, 13],\n",
        "    'weights': ['uniform', 'distance'],\n",
        "    'algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute'],\n",
        "    'leaf_size': [20, 30, 40, 50]\n",
        "}\n",
        "# GridSearchCV → sabhi parameter combinations ke liye model train karke best parameters nikalta hai\n",
        "grid_clf = GridSearchCV(estimator=clf, param_grid=param_grid_clf, cv=5, verbose=3)\n",
        "print(\"Running GridSearchCV for Classifier... (This may take a moment)\")\n",
        "grid_clf.fit(X_clf_train, y_clf_train)\n",
        "\n",
        "# best_params_ → wo parameters jo best accuracy dete hain\n",
        "print(\"\\nBest Hyperparameters Found for Classifier:\", grid_clf.best_params_)\n",
        "# best_score_ → cross validation me jo best accuracy mili\n",
        "print(f\"Best Cross-Validated Accuracy: {grid_clf.best_score_:.4f}\")\n",
        "# grid.best_estimator_ → GridSearchCV se automatically best hyperparameters wala model return hota hai\n",
        "best_clf_model = grid_clf.best_estimator_\n",
        "\n",
        "# ---------------- 1.4 EVALUATE THE TUNED CLASSIFIER ----------------\n",
        "print(\"\\nStep 1.4: Evaluating the tuned KNN Classifier...\")\n",
        "# ab is best_model se test data ke liye predictions karte hain\n",
        "y_pred_tuned_clf = best_clf_model.predict(X_clf_test)\n",
        "\n",
        "print(\"\\n--- Tuned Classifier Performance ---\")\n",
        "print(f\"Accuracy: {accuracy_score(y_clf_test, y_pred_tuned_clf):.4f}\")\n",
        "print(\"Classification Report:\\n\", classification_report(y_clf_test, y_pred_tuned_clf))\n",
        "print(\"Confusion Matrix:\\n\", confusion_matrix(y_clf_test, y_pred_tuned_clf))\n",
        "print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
        "\n",
        "\n",
        "# ====================================================================================\n",
        "# --- PART 2: K-NEAREST NEIGHBORS (KNN) REGRESSOR ---\n",
        "# ====================================================================================\n",
        "print(\"--- Part 2: Building and Tuning a KNN Regressor ---\")\n",
        "\n",
        "# ---------------- 2.1 GENERATE AND SPLIT REGRESSION DATA ----------------\n",
        "print(\"\\nStep 2.1: Generating synthetic regression data...\")\n",
        "# Synthetic regression dataset generate karna\n",
        "# n_samples=1000 → 1000 data points\n",
        "# n_features=2 → 2 input features\n",
        "# noise=3 → thoda randomness add karna\n",
        "X_reg, y_reg = make_regression(n_samples=1000, n_features=2, noise=3, random_state=1)\n",
        "# Dataset ko training aur testing parts me split karna\n",
        "X_reg_train, X_reg_test, y_reg_train, y_reg_test = train_test_split(X_reg, y_reg, test_size=0.30, random_state=1)\n",
        "print(f\"Regression data split into {X_reg_train.shape[0]} training and {X_reg_test.shape[0]} testing samples.\")\n",
        "\n",
        "# ---------------- 2.2 TRAIN AND EVALUATE INITIAL REGRESSOR ----------------\n",
        "print(\"\\nStep 2.2: Training and evaluating the initial (untuned) KNN Regressor...\")\n",
        "# KNN Regressor ka object banaya\n",
        "reg = KNeighborsRegressor()\n",
        "# Model ko training dataset par fit karna\n",
        "reg.fit(X_reg_train, y_reg_train)\n",
        "# Testing data ke liye prediction karna\n",
        "y_pred_initial_reg = reg.predict(X_reg_test)\n",
        "\n",
        "# Model ki performance ko evaluate karne ke liye R² score nikalte hain\n",
        "initial_r2 = r2_score(y_reg_test, y_pred_initial_reg)\n",
        "print(f\"\\nInitial Regressor R2 Score: {initial_r2:.4f}\")\n",
        "\n",
        "# ---------------- 2.3 HYPERPARAMETER TUNING WITH GRIDSEARCHCV ----------------\n",
        "print(\"\\nStep 2.3: Performing hyperparameter tuning for the regressor...\")\n",
        "# param_grid me sabhi possible parameter combinations diye hain\n",
        "param_grid_reg = {\n",
        "    'n_neighbors': [3, 5, 6, 7, 9, 11, 13],\n",
        "    'weights': ['uniform', 'distance'],\n",
        "    'algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute'],\n",
        "    'leaf_size': [20, 30, 40, 50]\n",
        "}\n",
        "# GridSearchCV setup\n",
        "grid_reg = GridSearchCV(estimator=reg, param_grid=param_grid_reg, cv=5, verbose=3)\n",
        "print(\"Running GridSearchCV for Regressor... (This may take a moment)\")\n",
        "# Training data par GridSearchCV ko fit karna\n",
        "grid_reg.fit(X_reg_train, y_reg_train)\n",
        "\n",
        "# Best parameter combination jo sabse acha perform kare\n",
        "print(\"\\nBest Hyperparameters Found for Regressor:\", grid_reg.best_params_)\n",
        "# Best cross-validation score (R² score) jo tuning ke dauran mila\n",
        "print(f\"Best Cross-Validated R2 Score: {grid_reg.best_score_:.4f}\")\n",
        "# Best model ke parameters ko store kar liya\n",
        "best_model_params = grid_reg.best_params_\n",
        "print(\"\\nBest model parameters stored.\")\n",
        "print(\"\\n--- MASTER PIPELINE COMPLETE ---\")\n",
        "\n",
        "# ---------------- END OF SCRIPT ----------------\n"
      ],
      "metadata": {
        "id": "mYUHIUnlym0q"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}