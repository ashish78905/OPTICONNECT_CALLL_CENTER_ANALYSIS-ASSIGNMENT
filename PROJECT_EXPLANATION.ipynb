{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNckDkDyIB3OZAqcBnd/DIO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ashish78905/OPTICONNECT_CALLL_CENTER_ANALYSIS-ASSIGNMENT/blob/main/PROJECT_EXPLANATION.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "---\n",
        "\n",
        "## **OpenUBA - Complete Project Explanation (3-Layer Architecture)**\n",
        "\n",
        "---\n",
        "\n",
        "# **LAYER 1: INPUT LAYER**\n",
        "## (Types of Data Sources, Types of Data to be Streamed, Throughputs)\n",
        "\n",
        "---\n",
        "\n",
        "### **1.1 What is the Input Layer?**\n",
        "\n",
        "The Input Layer is the **data collection foundation** of this User Behavior Analytics system. Think of it as the \"ears and eyes\" of the security platform - it listens to and captures everything happening in your organization's network and systems.\n",
        "\n",
        "**Real-World Analogy:** Imagine a bank with security cameras at every entrance, ATM, and counter. The Input Layer is like all those cameras recording everything - who comes in, what they do, when they leave.\n",
        "\n",
        "---\n",
        "\n",
        "### **1.2 Types of Data Sources Supported**\n",
        "\n",
        "The system supports **five major categories of data sources**:\n",
        "\n",
        "#### **A. Local File System (Disk)**\n",
        "This is the simplest source where log files are stored directly on the server's hard drive.\n",
        "\n",
        "**Example Scenario:**\n",
        "Your company's web proxy server writes a log file every day. Each line records:\n",
        "- Which employee (identified by username) visited which website\n",
        "- At what time\n",
        "- How much data they downloaded\n",
        "\n",
        "The system reads these files directly from the disk location, for instance from a folder like \"proxy_logs/daily/\".\n",
        "\n",
        "**When to use:** Small to medium organizations, testing environments, or when data is already being collected by existing systems.\n",
        "\n",
        "---\n",
        "\n",
        "#### **B. Hadoop Distributed File System (HDFS)**\n",
        "HDFS is used when you have **massive amounts of data** spread across multiple servers.\n",
        "\n",
        "**Example Scenario:**\n",
        "A global bank has 50,000 employees across 30 countries. Each day generates:\n",
        "- 500 million proxy logs\n",
        "- 200 million email metadata records\n",
        "- 100 million authentication events\n",
        "\n",
        "No single computer can store this. HDFS splits this data across hundreds of servers. The system can read from any of these distributed locations.\n",
        "\n",
        "**Real-World Analogy:** Instead of one giant filing cabinet (which would be impossible), you have 100 normal filing cabinets in different rooms, but a smart index tells you exactly which cabinet holds which document.\n",
        "\n",
        "**When to use:** Large enterprises with big data infrastructure, organizations processing terabytes daily.\n",
        "\n",
        "---\n",
        "\n",
        "#### **C. Elasticsearch (ES)**\n",
        "Elasticsearch is a **search-optimized database** for real-time log analysis.\n",
        "\n",
        "**Example Scenario:**\n",
        "Your Security Operations Center needs to search \"show me all failed login attempts for user john.doe in the last 24 hours\" and get results in milliseconds. Elasticsearch makes this possible.\n",
        "\n",
        "The system connects to Elasticsearch using:\n",
        "- A host address (where the Elasticsearch server lives)\n",
        "- A query (what data you want to retrieve)\n",
        "\n",
        "**When to use:** Real-time security monitoring, when you need fast searches across billions of records, integration with existing ELK (Elasticsearch, Logstash, Kibana) stack.\n",
        "\n",
        "---\n",
        "\n",
        "#### **D. Apache Spark Integration (PySpark)**\n",
        "Spark is used for **processing extremely large datasets in parallel**.\n",
        "\n",
        "**Example Scenario:**\n",
        "You need to analyze one year of historical data (2 petabytes) to find all users who accessed sensitive files outside business hours. A normal computer would take weeks. Spark splits this job across 500 computers and finishes in hours.\n",
        "\n",
        "**When to use:** Historical analysis, machine learning training on massive datasets, batch processing jobs.\n",
        "\n",
        "---\n",
        "\n",
        "### **1.3 Types of Data Formats (What Gets Streamed)**\n",
        "\n",
        "The system handles multiple data formats:\n",
        "\n",
        "#### **A. CSV (Comma-Separated Values)**\n",
        "The most common format. Each row is one event, columns are different attributes.\n",
        "\n",
        "**Example of Proxy Log CSV:**\n",
        "```\n",
        "timestamp, username, website, action, bytes_downloaded\n",
        "2024-01-15 09:30:00, alice.smith, gmail.com, allow, 15000\n",
        "2024-01-15 09:31:00, bob.jones, hacking-tools.com, block, 0\n",
        "2024-01-15 09:32:00, alice.smith, dropbox.com, allow, 500000\n",
        "```\n",
        "\n",
        "This tells us: Alice checked email, Bob tried to visit a suspicious site (blocked), then Alice uploaded or downloaded something big from Dropbox.\n",
        "\n",
        "---\n",
        "\n",
        "#### **B. Parquet**\n",
        "A **columnar storage format** - extremely efficient for analytics.\n",
        "\n",
        "**Why it matters:**\n",
        "If you have 100 columns but only need 3 columns for analysis, Parquet reads only those 3 columns. CSV would read all 100.\n",
        "\n",
        "**Example:** Analyzing only usernames and timestamps from a billion-row dataset:\n",
        "- CSV approach: Read entire file (500 GB)\n",
        "- Parquet approach: Read only 2 columns (10 GB)\n",
        "\n",
        "---\n",
        "\n",
        "#### **C. JSON (JavaScript Object Notation)**\n",
        "Flexible format for complex, nested data.\n",
        "\n",
        "**Example of Authentication Event JSON:**\n",
        "```json\n",
        "{\n",
        "    \"event_type\": \"login\",\n",
        "    \"user\": \"alice.smith\",\n",
        "    \"timestamp\": \"2024-01-15T09:30:00Z\",\n",
        "    \"details\": {\n",
        "        \"ip_address\": \"192.168.1.100\",\n",
        "        \"device\": \"Windows 10 Laptop\",\n",
        "        \"location\": {\n",
        "            \"country\": \"USA\",\n",
        "            \"city\": \"New York\"\n",
        "        },\n",
        "        \"authentication_method\": \"password + MFA\"\n",
        "    },\n",
        "    \"success\": true\n",
        "}\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "#### **D. Flat Files**\n",
        "Simple text files where each line is a log entry, possibly with custom delimiters.\n",
        "\n",
        "**Example (Space-Delimited Firewall Log):**\n",
        "```\n",
        "15Jan2024 09:30:00 ALLOW TCP 192.168.1.100 10.0.0.50 443\n",
        "15Jan2024 09:30:01 DENY UDP 192.168.1.105 8.8.8.8 53\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### **1.4 Specific Log Types Supported**\n",
        "\n",
        "The system is designed to ingest security-relevant logs:\n",
        "\n",
        "#### **A. Proxy/Web Gateway Logs**\n",
        "Records all web browsing activity.\n",
        "\n",
        "**Security Value:** Detect users visiting malicious websites, data exfiltration attempts, policy violations (social media during work hours).\n",
        "\n",
        "**Example Detection:** \"User downloaded 50GB of data to personal Dropbox in 2 hours\" - possible data theft.\n",
        "\n",
        "---\n",
        "\n",
        "#### **B. DNS (Domain Name System) Logs**\n",
        "Records all domain lookups (what websites computers are trying to reach).\n",
        "\n",
        "**Security Value:** Detect malware communication. Malware often contacts \"command and control\" servers using strange domain names.\n",
        "\n",
        "**Example Detection:** \"Computer is looking up 'asd7f8a9sd7f.evil-domain.com' 1000 times per hour\" - likely malware trying to phone home.\n",
        "\n",
        "---\n",
        "\n",
        "#### **C. SSH (Secure Shell) Logs**\n",
        "Records remote login attempts to servers.\n",
        "\n",
        "**Security Value:** Detect unauthorized access attempts, brute force attacks, lateral movement by attackers.\n",
        "\n",
        "**Example Detection:** \"Same user successfully logged into 50 different servers in 10 minutes\" - either automation or compromised account moving laterally.\n",
        "\n",
        "---\n",
        "\n",
        "#### **D. DHCP (Dynamic Host Configuration Protocol) Logs**\n",
        "Records network address assignments to devices.\n",
        "\n",
        "**Security Value:** Asset tracking, detecting rogue devices on network.\n",
        "\n",
        "**Example Detection:** \"Unknown MAC address received IP address\" - unauthorized device connected to network.\n",
        "\n",
        "---\n",
        "\n",
        "### **1.5 Throughput Capabilities**\n",
        "\n",
        "Throughput refers to **how much data the system can handle**.\n",
        "\n",
        "#### **Small Scale (Local Disk + Pandas)**\n",
        "- **Capacity:** Thousands to millions of records per day\n",
        "- **Processing Speed:** Single machine, sequential processing\n",
        "- **Use Case:** Small company with 100 employees\n",
        "\n",
        "#### **Medium Scale (Elasticsearch)**\n",
        "- **Capacity:** Millions to billions of records per day\n",
        "- **Processing Speed:** Near real-time (sub-second queries)\n",
        "- **Use Case:** Medium enterprise with 5,000 employees\n",
        "\n",
        "#### **Large Scale (HDFS + Spark)**\n",
        "- **Capacity:** Billions to trillions of records per day\n",
        "- **Processing Speed:** Distributed parallel processing across cluster\n",
        "- **Use Case:** Global enterprise with 100,000+ employees or cloud service provider\n",
        "\n",
        "---\n",
        "\n",
        "### **1.6 Data Schema Configuration**\n",
        "\n",
        "The system uses a **configuration scheme** that defines how to read data:\n",
        "\n",
        "**Key Configuration Elements:**\n",
        "1. **Source Group:** A collection of related data sources\n",
        "2. **Folder Location:** Where the data lives\n",
        "3. **Log Name:** Identifier for this log type\n",
        "4. **Type:** Format (CSV, Parquet, JSON, Flat)\n",
        "5. **Delimiter:** How fields are separated (comma, space, tab)\n",
        "6. **Location Type:** Storage system (disk, HDFS, Elasticsearch)\n",
        "7. **ID Feature:** The column that identifies users (username, employee_id)\n",
        "\n",
        "**Example Configuration:**\n",
        "```\n",
        "Source Group: \"Corporate Proxy Logs\"\n",
        "Folder: \"/data/proxy/\"\n",
        "Type: CSV\n",
        "Delimiter: Space\n",
        "Location: Disk\n",
        "ID Feature: \"cs-username\"\n",
        "```\n",
        "\n",
        "This tells the system: \"Look in /data/proxy/, read CSV files separated by spaces, and the user identity is in the 'cs-username' column.\"\n",
        "\n",
        "---\n",
        "\n",
        "### **1.7 User and Entity Extraction**\n",
        "\n",
        "As data streams in, the system automatically:\n",
        "\n",
        "1. **Extracts all unique users** from the logs\n",
        "2. **Creates user profiles** in the system\n",
        "3. **Creates directories** for each user to store their behavioral data\n",
        "\n",
        "**Example Process:**\n",
        "Input: Proxy log with 1 million entries\n",
        "Output:\n",
        "- Found 523 unique usernames\n",
        "- Created profile for each user\n",
        "- Stored in user database\n",
        "\n",
        "---\n",
        "\n",
        "---\n",
        "\n",
        "# **LAYER 2: PROCESSING LAYER**\n",
        "## (Types of Processing, Quality of Processing, Throughputs)\n",
        "\n",
        "---\n",
        "\n",
        "### **2.1 What is the Processing Layer?**\n",
        "\n",
        "The Processing Layer is the **brain** of the system. It takes raw data from the Input Layer and transforms it into security insights. This is where machine learning models analyze behavior, rules fire on suspicious patterns, and risk scores are calculated.\n",
        "\n",
        "**Real-World Analogy:** If Input Layer is security cameras, Processing Layer is the team of analysts watching the footage, recognizing suspicious behavior, and taking notes.\n",
        "\n",
        "---\n",
        "\n",
        "### **2.2 Types of Processing**\n",
        "\n",
        "#### **A. Machine Learning Model Processing**\n",
        "\n",
        "The system has a **Model Library** - a collection of pre-built and custom machine learning models that analyze user behavior.\n",
        "\n",
        "**How Models Work:**\n",
        "\n",
        "1. **Data Loading:** Model receives data (DataFrame) from Input Layer\n",
        "2. **Feature Extraction:** Model identifies relevant patterns\n",
        "3. **Analysis:** Model applies its algorithm\n",
        "4. **Output:** Model produces risk scores or anomaly flags\n",
        "\n",
        "**Types of Models Supported:**\n",
        "\n",
        "##### **i. Simple Rule-Based Models (Regex)**\n",
        "Pattern matching using regular expressions.\n",
        "\n",
        "**Example:**\n",
        "Rule: \"Flag any URL containing 'torrent' or 'hack'\"\n",
        "```\n",
        "User visits: download-torrents.com → FLAGGED\n",
        "User visits: microsoft.com → OK\n",
        "```\n",
        "\n",
        "##### **ii. Statistical Models (Deviation-Based)**\n",
        "Detect when behavior deviates from normal patterns.\n",
        "\n",
        "**Example:**\n",
        "Alice normally downloads 100 MB per day. Her average (mean) is 100 MB with standard deviation of 20 MB.\n",
        "\n",
        "Today she downloaded 500 MB.\n",
        "\n",
        "Calculation: Is 500 MB outside (mean + 2*standard_deviation)?\n",
        "500 > (100 + 2*20) = 140 → YES, this is anomalous!\n",
        "\n",
        "**Rule Definition:**\n",
        "```\n",
        "Condition: (current_value > (mean + std_range)) OR (current_value < (mean - std_range))\n",
        "Score: +20 risk points\n",
        "```\n",
        "\n",
        "##### **iii. TensorFlow/Deep Learning Models**\n",
        "Neural networks for complex pattern recognition.\n",
        "\n",
        "**Example Use Case:**\n",
        "Detecting insider threats by analyzing:\n",
        "- Login times\n",
        "- Files accessed\n",
        "- Email patterns\n",
        "- Web browsing\n",
        "- Badge-in/badge-out times\n",
        "\n",
        "A neural network learns what \"normal\" looks like for each user and identifies deviations that humans might miss.\n",
        "\n",
        "**Model Format:** Protobuf (TensorFlow's serialization format)\n",
        "\n",
        "##### **iv. Scikit-Learn Models**\n",
        "Traditional machine learning algorithms.\n",
        "\n",
        "**Example Use Case:**\n",
        "Classification model trained on historical data:\n",
        "- Input: User behavior features\n",
        "- Output: \"Normal\" or \"Suspicious\"\n",
        "\n",
        "Algorithms like Random Forest, SVM, Logistic Regression.\n",
        "\n",
        "**Model Format:** Pickle (Python's serialization format)\n",
        "\n",
        "##### **v. PySpark Models**\n",
        "Distributed machine learning for massive datasets.\n",
        "\n",
        "**Example Use Case:**\n",
        "Training a model on 5 years of historical data (500 billion records) to understand seasonal patterns in employee behavior.\n",
        "\n",
        "---\n",
        "\n",
        "#### **B. Rule Engine Processing**\n",
        "\n",
        "The system has two types of rules:\n",
        "\n",
        "##### **i. Single-Fire Rules**\n",
        "Trigger immediately when a condition is met.\n",
        "\n",
        "**Example:**\n",
        "```\n",
        "Rule: If username == 'admin' AND time BETWEEN 2AM-5AM\n",
        "Action: Add 50 risk points\n",
        "Reason: Admin account used during unusual hours\n",
        "```\n",
        "\n",
        "**Scenario:**\n",
        "- 3:00 AM: admin account logs in\n",
        "- Rule fires immediately\n",
        "- Risk score increased\n",
        "- Potential alert generated\n",
        "\n",
        "##### **ii. Deviation Rules**\n",
        "Trigger when behavior deviates from baseline.\n",
        "\n",
        "**Example:**\n",
        "```\n",
        "Rule: If daily_downloads > (average_daily_downloads + 2*std_dev)\n",
        "Action: Add 30 risk points\n",
        "Reason: Unusual data download volume\n",
        "```\n",
        "\n",
        "**Scenario:**\n",
        "- Alice's average: 50 files/day\n",
        "- Standard deviation: 10 files\n",
        "- Threshold: 50 + 2*10 = 70 files\n",
        "- Today: Alice downloaded 150 files\n",
        "- Rule fires: 150 > 70\n",
        "- Risk score increased\n",
        "\n",
        "---\n",
        "\n",
        "#### **C. Risk Score Calculation**\n",
        "\n",
        "Risk scores quantify the \"suspiciousness\" of a user.\n",
        "\n",
        "**Risk Score Components:**\n",
        "1. **Base Score:** Starting point (usually 0)\n",
        "2. **Model Contributions:** Each model can add points\n",
        "3. **Rule Contributions:** Each fired rule adds points\n",
        "4. **Historical Factor:** Past behavior influences score\n",
        "\n",
        "**Risk Levels:**\n",
        "- **0-29:** LOW RISK - Normal behavior\n",
        "- **30-69:** MEDIUM RISK - Needs monitoring\n",
        "- **70-100:** HIGH RISK - Immediate attention required\n",
        "\n",
        "**Example Calculation:**\n",
        "```\n",
        "User: Bob\n",
        "Base Score: 0\n",
        "\n",
        "+ Model \"After Hours Login\": +10 (logged in at 11 PM)\n",
        "+ Model \"Large Download\": +25 (downloaded 2GB)\n",
        "+ Rule \"Accessed Sensitive File\": +15\n",
        "+ Historical Factor: +5 (had incident last month)\n",
        "\n",
        "Total Risk Score: 55 (MEDIUM RISK)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "#### **D. Anomaly Detection Processing**\n",
        "\n",
        "Anomalies are behaviors that don't fit the normal pattern.\n",
        "\n",
        "**Types of Anomalies Detected:**\n",
        "\n",
        "##### **i. Login Anomalies**\n",
        "```\n",
        "Normal: User logs in from New York at 9 AM\n",
        "Anomaly: Same user logs in from Tokyo at 9:05 AM\n",
        "Detection: Impossible travel - flagged!\n",
        "```\n",
        "\n",
        "##### **ii. Data Access Anomalies**\n",
        "```\n",
        "Normal: User accesses 10 files per day\n",
        "Anomaly: User accessed 500 files in 1 hour\n",
        "Detection: Unusual volume - flagged!\n",
        "```\n",
        "\n",
        "##### **iii. Time-Based Anomalies**\n",
        "```\n",
        "Normal: User works 9 AM - 5 PM\n",
        "Anomaly: User active at 3 AM\n",
        "Detection: Unusual hours - flagged!\n",
        "```\n",
        "\n",
        "##### **iv. Sequence Anomalies**\n",
        "```\n",
        "Normal: Login → Email → Documents → Logout\n",
        "Anomaly: Login → Database → Export → Logout\n",
        "Detection: Unusual sequence - flagged!\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### **2.3 Quality of Processing**\n",
        "\n",
        "Quality is ensured through multiple mechanisms:\n",
        "\n",
        "#### **A. Model Verification**\n",
        "\n",
        "Before any model runs, the system verifies its integrity:\n",
        "\n",
        "**i. Data Hash Verification**\n",
        "Every model component has a hash (digital fingerprint). The system computes the hash and compares it to the expected value.\n",
        "\n",
        "**Example:**\n",
        "```\n",
        "Expected Hash: abc123def456...\n",
        "Computed Hash: abc123def456...\n",
        "Match: YES - Model is authentic and unmodified\n",
        "```\n",
        "\n",
        "**Why This Matters:**\n",
        "If an attacker modified a model file to ignore certain users, the hash would change, and the system would reject the model.\n",
        "\n",
        "**ii. File Hash Verification**\n",
        "After installation, file hashes are verified again.\n",
        "\n",
        "**iii. Base64 Encoding Verification**\n",
        "Model components are transmitted in Base64 encoding. The system verifies the encoded data before decoding.\n",
        "\n",
        "---\n",
        "\n",
        "#### **B. Model Component Limits**\n",
        "\n",
        "To prevent malicious models:\n",
        "- Maximum of 2 components per model allowed\n",
        "- Components must be the expected file types\n",
        "- Excessive components are rejected\n",
        "\n",
        "---\n",
        "\n",
        "#### **C. Session Logging**\n",
        "\n",
        "Every model execution is logged:\n",
        "```\n",
        "Session Log:\n",
        "- Model Name: \"Insider Threat Detector\"\n",
        "- Timestamp: 2024-01-15 09:30:00\n",
        "- User Analyzed: \"alice.smith\"\n",
        "- Data Points: 50,000\n",
        "- Result: Risk Score 45\n",
        "```\n",
        "\n",
        "This creates an audit trail for compliance and debugging.\n",
        "\n",
        "---\n",
        "\n",
        "#### **D. Safe Mode**\n",
        "\n",
        "The system has a \"safe mode\" that:\n",
        "- Automatically removes models that fail verification\n",
        "- Prevents potentially malicious models from executing\n",
        "- Logs all removal events\n",
        "\n",
        "---\n",
        "\n",
        "### **2.4 Model Library Architecture**\n",
        "\n",
        "The Model Library is a **repository of ready-to-use models**.\n",
        "\n",
        "**Key Concepts:**\n",
        "\n",
        "#### **i. Model Groups**\n",
        "Models are organized into groups based on their data source.\n",
        "\n",
        "**Example:**\n",
        "```\n",
        "Group: \"Proxy Log Analysis\"\n",
        "├── Model: \"Malicious URL Detector\"\n",
        "├── Model: \"Data Exfiltration Detector\"\n",
        "└── Model: \"Policy Violation Checker\"\n",
        "\n",
        "Group: \"Authentication Analysis\"\n",
        "├── Model: \"Brute Force Detector\"\n",
        "├── Model: \"Impossible Travel Detector\"\n",
        "└── Model: \"Privilege Escalation Detector\"\n",
        "```\n",
        "\n",
        "#### **ii. Model Components**\n",
        "Each model consists of:\n",
        "1. **__init__.py:** Initialization file\n",
        "2. **MODEL.py:** Main execution logic with execute() function\n",
        "\n",
        "#### **iii. Model Metadata**\n",
        "Each model has associated information:\n",
        "- **Name:** Unique identifier\n",
        "- **Description:** What it does\n",
        "- **MITRE Technique ID:** Maps to known attack patterns\n",
        "- **Score:** How many risk points it can add\n",
        "- **Enabled:** Whether it's active\n",
        "\n",
        "---\n",
        "\n",
        "#### **iv. Model Installation Process**\n",
        "\n",
        "When installing a new model:\n",
        "\n",
        "1. **Download:** Fetch model from library server\n",
        "2. **Verify Encoding:** Check Base64 hashes match\n",
        "3. **Create Directory:** Make folder for model\n",
        "4. **Store Files:** Decode and save model files\n",
        "5. **Verify Files:** Check installed file hashes\n",
        "6. **Enable:** Mark model as ready to run\n",
        "\n",
        "---\n",
        "\n",
        "### **2.5 MITRE ATT&CK Integration**\n",
        "\n",
        "The system maps behaviors to the **MITRE ATT&CK Framework** - a global knowledge base of adversary tactics and techniques.\n",
        "\n",
        "**How It Works:**\n",
        "\n",
        "Each model is tagged with MITRE technique IDs:\n",
        "```\n",
        "Model: \"Unusual Process Execution\"\n",
        "MITRE Technique: T1059 (Command and Scripting Interpreter)\n",
        "MITRE Tactic: Execution\n",
        "\n",
        "Model: \"Large Data Transfer\"\n",
        "MITRE Technique: T1041 (Exfiltration Over C2 Channel)\n",
        "MITRE Tactic: Exfiltration\n",
        "```\n",
        "\n",
        "**Value:**\n",
        "When an alert fires, security analysts immediately understand:\n",
        "- What type of attack this might be\n",
        "- What the attacker might do next\n",
        "- How to investigate and respond\n",
        "\n",
        "---\n",
        "\n",
        "### **2.6 Processing Throughput**\n",
        "\n",
        "**Data Loader Options and Their Throughput:**\n",
        "\n",
        "| Data Loader | Processing Speed | Best For |\n",
        "|-------------|------------------|----------|\n",
        "| Local Pandas CSV | ~10,000 records/second | Small files, testing |\n",
        "| Local Pandas Parquet | ~50,000 records/second | Medium files |\n",
        "| HDFS Pandas CSV | ~100,000 records/second | Large distributed files |\n",
        "| HDFS Pandas Parquet | ~500,000 records/second | Large optimized files |\n",
        "| HDFS Spark CSV | ~1,000,000 records/second | Massive parallel processing |\n",
        "| HDFS Spark Parquet | ~5,000,000 records/second | Maximum performance |\n",
        "| Elasticsearch | Real-time | Live monitoring |\n",
        "\n",
        "---\n",
        "\n",
        "---\n",
        "\n",
        "# **LAYER 3: OUTPUT LAYER**\n",
        "## (Types of Events/Alerts/Correlation, Throughputs, Further Actions Downstream)\n",
        "\n",
        "---\n",
        "\n",
        "### **3.1 What is the Output Layer?**\n",
        "\n",
        "The Output Layer is the **action and communication center**. It takes the insights from Processing Layer and transforms them into actionable security responses - alerts, cases, reports, and automated actions.\n",
        "\n",
        "**Real-World Analogy:** If Processing Layer is the analysts watching cameras, Output Layer is them calling security guards, filing reports, locking doors, and notifying management.\n",
        "\n",
        "---\n",
        "\n",
        "### **3.2 Types of Events Generated**\n",
        "\n",
        "#### **A. Anomaly Events**\n",
        "Generated when the system detects unusual behavior.\n",
        "\n",
        "**Anomaly Event Structure:**\n",
        "```\n",
        "Event Type: ANOMALY\n",
        "Anomaly Type: \"data_access_anomaly\"\n",
        "User: \"alice.smith\"\n",
        "Score: 75\n",
        "Description: \"User accessed 500 files in 1 hour, baseline is 10 files/hour\"\n",
        "Detected At: \"2024-01-15 14:30:00\"\n",
        "```\n",
        "\n",
        "**Anomaly Categories:**\n",
        "1. **Login Anomalies:** Unusual login patterns\n",
        "2. **Access Anomalies:** Unusual data access\n",
        "3. **Volume Anomalies:** Unusual data volumes\n",
        "4. **Time Anomalies:** Activity at unusual times\n",
        "5. **Location Anomalies:** Activity from unusual places\n",
        "\n",
        "---\n",
        "\n",
        "#### **B. Risk Events**\n",
        "Generated when user risk scores change significantly.\n",
        "\n",
        "**Risk Event Structure:**\n",
        "```\n",
        "Event Type: RISK_CHANGE\n",
        "User: \"bob.jones\"\n",
        "Previous Risk: 25 (LOW)\n",
        "New Risk: 72 (HIGH)\n",
        "Change Reason: \"Multiple anomalies detected\"\n",
        "Timestamp: \"2024-01-15 15:00:00\"\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### **3.3 Types of Alerts**\n",
        "\n",
        "Alerts are **notifications requiring attention**.\n",
        "\n",
        "#### **Alert Severity Levels:**\n",
        "\n",
        "##### **i. CRITICAL Alerts**\n",
        "Require immediate response.\n",
        "\n",
        "**Example:**\n",
        "```\n",
        "Alert ID: ALT-001\n",
        "Severity: CRITICAL\n",
        "Type: \"data_exfiltration\"\n",
        "User: \"eve.hacker\"\n",
        "Description: \"User transferred 50GB to personal cloud storage in 30 minutes\"\n",
        "Timestamp: \"2024-01-15 02:30:00\"\n",
        "```\n",
        "\n",
        "**Typical Response Time:** Minutes\n",
        "\n",
        "##### **ii. HIGH Alerts**\n",
        "Require prompt investigation.\n",
        "\n",
        "**Example:**\n",
        "```\n",
        "Alert ID: ALT-002\n",
        "Severity: HIGH\n",
        "Type: \"privilege_escalation\"\n",
        "User: \"mallory.insider\"\n",
        "Description: \"User granted themselves admin access to financial database\"\n",
        "Timestamp: \"2024-01-15 11:00:00\"\n",
        "```\n",
        "\n",
        "**Typical Response Time:** Hours\n",
        "\n",
        "##### **iii. MEDIUM Alerts**\n",
        "Require investigation within business day.\n",
        "\n",
        "**Example:**\n",
        "```\n",
        "Alert ID: ALT-003\n",
        "Severity: MEDIUM\n",
        "Type: \"policy_violation\"\n",
        "User: \"john.careless\"\n",
        "Description: \"User accessed social media during prohibited hours\"\n",
        "Timestamp: \"2024-01-15 09:30:00\"\n",
        "```\n",
        "\n",
        "**Typical Response Time:** Same day\n",
        "\n",
        "##### **iv. LOW Alerts**\n",
        "Informational, review during routine checks.\n",
        "\n",
        "**Example:**\n",
        "```\n",
        "Alert ID: ALT-004\n",
        "Severity: LOW\n",
        "Type: \"unusual_activity\"\n",
        "User: \"jane.newbie\"\n",
        "Description: \"New user has different browsing pattern than peer group\"\n",
        "Timestamp: \"2024-01-15 10:00:00\"\n",
        "```\n",
        "\n",
        "**Typical Response Time:** Weekly review\n",
        "\n",
        "---\n",
        "\n",
        "#### **Alert Types:**\n",
        "\n",
        "1. **data_exfiltration:** Potential data theft\n",
        "2. **privilege_escalation:** Unauthorized access increase\n",
        "3. **policy_violation:** Breaking company rules\n",
        "4. **malware_communication:** Possible malware activity\n",
        "5. **credential_abuse:** Stolen or shared credentials\n",
        "6. **insider_threat:** Malicious insider activity\n",
        "7. **account_compromise:** Account taken over\n",
        "8. **lateral_movement:** Attacker moving through network\n",
        "\n",
        "---\n",
        "\n",
        "### **3.4 Correlation (Connecting Events)**\n",
        "\n",
        "The system correlates multiple events to see the bigger picture.\n",
        "\n",
        "#### **Example of Correlation:**\n",
        "\n",
        "**Individual Events (seem minor alone):**\n",
        "1. Event 1: \"Bob failed VPN login 3 times\" (LOW)\n",
        "2. Event 2: \"Bob successfully logged in from new IP\" (LOW)\n",
        "3. Event 3: \"Bob accessed sensitive documents\" (LOW)\n",
        "4. Event 4: \"Bob downloaded 500 files\" (MEDIUM)\n",
        "5. Event 5: \"Bob sent large email attachment\" (LOW)\n",
        "\n",
        "**Correlated View (reveals attack pattern):**\n",
        "```\n",
        "CORRELATION: Potential Account Compromise + Data Theft\n",
        "Timeline:\n",
        "  09:00 - Multiple failed logins (password guessing)\n",
        "  09:15 - Successful login from attacker IP\n",
        "  09:20 - Reconnaissance of sensitive files\n",
        "  09:30 - Mass download of documents\n",
        "  09:45 - Exfiltration via email\n",
        "\n",
        "Combined Severity: CRITICAL\n",
        "Pattern Match: MITRE T1078 (Valid Accounts) → T1083 (File Discovery) → T1041 (Exfiltration)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### **3.5 Case Management**\n",
        "\n",
        "When alerts require investigation, they become **Cases**.\n",
        "\n",
        "#### **Case Structure:**\n",
        "```\n",
        "Case ID: CASE-2024-001\n",
        "Title: \"Suspected Data Exfiltration - Bob Jones\"\n",
        "Status: Open → In Progress → Resolved\n",
        "Priority: Critical\n",
        "Assigned To: \"Security Analyst Sarah\"\n",
        "Created: \"2024-01-15 15:00:00\"\n",
        "Related Alerts: [ALT-001, ALT-002, ALT-003]\n",
        "Related Events: [EVT-100 through EVT-125]\n",
        "```\n",
        "\n",
        "#### **Case Workflow:**\n",
        "1. **Open:** Alert triggered, case created\n",
        "2. **Assigned:** Analyst takes ownership\n",
        "3. **In Progress:** Investigation ongoing\n",
        "4. **Pending:** Waiting for additional information\n",
        "5. **Resolved:** Investigation complete\n",
        "6. **Closed:** Case archived\n",
        "\n",
        "---\n",
        "\n",
        "### **3.6 Display and Reporting**\n",
        "\n",
        "The system provides multiple views of security status.\n",
        "\n",
        "#### **A. Dashboard Display**\n",
        "High-level overview:\n",
        "```\n",
        "System Display:\n",
        "├── Total Monitored Users: 523\n",
        "├── High Risk Users: 12\n",
        "├── Medium Risk Users: 45\n",
        "├── Low Risk Users: 466\n",
        "├── Active Alerts: 28\n",
        "├── Open Cases: 5\n",
        "└── Models Running: 15\n",
        "```\n",
        "\n",
        "#### **B. User Risk Display**\n",
        "Individual user view:\n",
        "```\n",
        "User: alice.smith\n",
        "Risk Score: 45 (MEDIUM)\n",
        "Risk Level: MEDIUM\n",
        "Last Activity: \"2024-01-15 14:30:00\"\n",
        "Anomalies Detected: 3\n",
        "Alerts Count: 1\n",
        "Recent Events:\n",
        "  - Accessed unusual folder\n",
        "  - Downloaded large file\n",
        "  - Logged in after hours\n",
        "```\n",
        "\n",
        "#### **C. Statistics Display**\n",
        "Aggregate metrics:\n",
        "```\n",
        "Risk Statistics:\n",
        "├── Total Users: 523\n",
        "├── High Risk: 12 (2.3%)\n",
        "├── Medium Risk: 45 (8.6%)\n",
        "├── Low Risk: 466 (89.1%)\n",
        "├── Alerts Today: 15\n",
        "├── Anomalies This Week: 87\n",
        "├── Active Models: 15\n",
        "└── Connected Data Sources: 8\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### **3.7 Further Actions Downstream**\n",
        "\n",
        "This is where the system takes **automated response actions**.\n",
        "\n",
        "#### **A. Block the Request**\n",
        "Immediately stop a suspicious action.\n",
        "\n",
        "**Example Scenario:**\n",
        "```\n",
        "Detection: User trying to upload 10GB to personal Dropbox\n",
        "Risk Score: 95 (CRITICAL)\n",
        "Automated Action: BLOCK\n",
        "Result: File transfer blocked\n",
        "Notification: Security team alerted\n",
        "User Message: \"Transfer blocked - contact IT\"\n",
        "```\n",
        "\n",
        "**Use Cases:**\n",
        "- Block data exfiltration attempts\n",
        "- Block access to malicious websites\n",
        "- Block unauthorized system access\n",
        "\n",
        "---\n",
        "\n",
        "#### **B. Delay the Request**\n",
        "Slow down suspicious activity to allow investigation.\n",
        "\n",
        "**Example Scenario:**\n",
        "```\n",
        "Detection: User requesting access to sensitive database\n",
        "Risk Score: 65 (MEDIUM)\n",
        "Automated Action: DELAY (24 hours)\n",
        "Result: Access queued for approval\n",
        "Notification: User's manager notified\n",
        "User Message: \"Access request pending approval\"\n",
        "```\n",
        "\n",
        "**Use Cases:**\n",
        "- Delay large file transfers\n",
        "- Delay privilege escalation requests\n",
        "- Delay access to sensitive systems\n",
        "\n",
        "---\n",
        "\n",
        "#### **C. Wait for Approval**\n",
        "Require human authorization before proceeding.\n",
        "\n",
        "**Example Scenario:**\n",
        "```\n",
        "Detection: Admin requesting to delete backup files\n",
        "Risk Score: 50 (MEDIUM)\n",
        "Automated Action: REQUIRE_APPROVAL\n",
        "Approval Chain: Direct Manager → IT Security → CISO\n",
        "Current Status: Pending Manager Approval\n",
        "Timeout: 72 hours\n",
        "```\n",
        "\n",
        "**Use Cases:**\n",
        "- Sensitive data access\n",
        "- Bulk deletions\n",
        "- Configuration changes\n",
        "- Elevated privilege requests\n",
        "\n",
        "---\n",
        "\n",
        "#### **D. Multi-Factor Verification**\n",
        "Require additional identity verification.\n",
        "\n",
        "**Example Scenario:**\n",
        "```\n",
        "Detection: Login from new country\n",
        "Risk Score: 55 (MEDIUM)\n",
        "Automated Action: STEP_UP_AUTH\n",
        "Required: SMS code + Security question\n",
        "Result: User verified → Access granted\n",
        "         Verification failed → Account locked\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "#### **E. Isolate Endpoint**\n",
        "Quarantine a potentially compromised device.\n",
        "\n",
        "**Example Scenario:**\n",
        "```\n",
        "Detection: Machine exhibiting malware-like behavior\n",
        "Risk Score: 90 (CRITICAL)\n",
        "Automated Action: ISOLATE\n",
        "Result: Device network access blocked\n",
        "        Device can only communicate with security tools\n",
        "Notification: User notified, IT dispatched\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "#### **F. Disable Account**\n",
        "Temporarily or permanently disable user access.\n",
        "\n",
        "**Example Scenario:**\n",
        "```\n",
        "Detection: Confirmed compromised credentials\n",
        "Risk Score: 100 (CRITICAL)\n",
        "Automated Action: DISABLE_ACCOUNT\n",
        "Result: All active sessions terminated\n",
        "        Account locked\n",
        "        Password reset required\n",
        "Notification: User and security team notified\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "#### **G. Notify Stakeholders**\n",
        "Alert relevant people about security events.\n",
        "\n",
        "**Notification Chains:**\n",
        "\n",
        "1. **Technical Alert:**\n",
        "   - Security Operations Center (SOC)\n",
        "   - IT Administrator\n",
        "   - System Owner\n",
        "\n",
        "2. **Management Alert:**\n",
        "   - User's Direct Manager\n",
        "   - Department Head\n",
        "   - HR (if policy violation)\n",
        "\n",
        "3. **Executive Alert:**\n",
        "   - Chief Information Security Officer (CISO)\n",
        "   - Chief Information Officer (CIO)\n",
        "   - Legal (if regulatory implications)\n",
        "\n",
        "---\n",
        "\n",
        "#### **H. Create Audit Record**\n",
        "Document everything for compliance and legal purposes.\n",
        "\n",
        "**Audit Record:**\n",
        "```\n",
        "Record ID: AUD-2024-00001\n",
        "Timestamp: \"2024-01-15 15:30:00\"\n",
        "Event: \"High risk behavior detected\"\n",
        "User: \"bob.jones\"\n",
        "Action Taken: \"Account disabled, case opened\"\n",
        "Authorized By: \"Automated Policy + Manager Approval\"\n",
        "Evidence Preserved: Yes\n",
        "Retention Period: 7 years\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### **3.8 Output Throughput**\n",
        "\n",
        "The system can generate outputs at various rates:\n",
        "\n",
        "| Output Type | Throughput | Latency |\n",
        "|-------------|------------|---------|\n",
        "| Anomaly Events | 1000+ per second | Real-time |\n",
        "| Risk Calculations | 100+ users per second | Near real-time |\n",
        "| Alerts | 100+ per second | < 1 second |\n",
        "| Case Creation | 10+ per second | < 5 seconds |\n",
        "| Blocking Actions | Immediate | Milliseconds |\n",
        "| Notifications | 50+ per second | < 10 seconds |\n",
        "| Audit Records | 1000+ per second | Real-time |\n",
        "\n",
        "---\n",
        "\n",
        "### **3.9 API Access**\n",
        "\n",
        "All outputs are available through the REST API:\n",
        "\n",
        "**Key Endpoints:**\n",
        "\n",
        "1. **GET /alerts/** - Retrieve all security alerts\n",
        "2. **GET /anomalies/** - Get detected anomalies\n",
        "3. **GET /user/{name}/risk** - Get user risk score\n",
        "4. **POST /analyze** - Trigger analysis job\n",
        "5. **GET /cases/** - List investigation cases\n",
        "6. **GET /stats/risk** - Get overall risk statistics\n",
        "7. **GET /mitre/** - Get MITRE ATT&CK mappings\n",
        "\n",
        "---\n",
        "\n",
        "---\n",
        "\n",
        "## **COMPLETE FLOW SUMMARY**\n",
        "\n",
        "Let me walk through a complete example from start to finish:\n",
        "\n",
        "### **Scenario: Detecting an Insider Threat**\n",
        "\n",
        "**LAYER 1 - INPUT:**\n",
        "1. Proxy logs stream in showing user \"eve.suspicious\" browsing activity\n",
        "2. Data is in CSV format, stored on local disk\n",
        "3. System reads 1 million records, identifies 500 unique users\n",
        "4. User \"eve.suspicious\" is extracted and profile created\n",
        "\n",
        "**LAYER 2 - PROCESSING:**\n",
        "1. \"Data Exfiltration Model\" analyzes eve's browsing patterns\n",
        "2. Detects: 50GB uploaded to personal cloud (baseline: 100MB/day)\n",
        "3. \"After Hours Model\" detects: Activity at 2 AM (normal: 9AM-6PM)\n",
        "4. \"Sensitive Access Model\" detects: HR database accessed\n",
        "5. Rule fires: Downloads > (mean + 3*std_dev)\n",
        "6. Risk score calculated: 10 + 30 + 25 + 20 = 85 (HIGH)\n",
        "7. Model hashes verified - legitimate analysis\n",
        "8. MITRE mapping: T1048 (Exfiltration Over Alternative Protocol)\n",
        "\n",
        "**LAYER 3 - OUTPUT:**\n",
        "1. **Anomaly Event** created: \"Massive data upload detected\"\n",
        "2. **Alert** generated: CRITICAL severity\n",
        "3. **Correlation** performed: Links to unusual login, HR access\n",
        "4. **Automated Actions:**\n",
        "   - BLOCK: Further uploads blocked\n",
        "   - NOTIFY: Security team alerted\n",
        "   - CASE: Investigation case created\n",
        "5. **Audit Record** created for compliance\n",
        "6. **Dashboard** updated with new high-risk user\n",
        "7. **API** returns alert details to SIEM integration\n",
        "\n",
        "**OUTCOME:** Potential data theft stopped, investigation begins, evidence preserved.\n",
        "\n",
        "---\n",
        "\n",
        "This three-layer architecture ensures that OpenUBA can:\n",
        "- **Collect** data from any source at any scale\n",
        "- **Process** it with sophisticated analytics while maintaining quality\n",
        "- **Output** actionable intelligence with appropriate automated responses\n",
        "\n"
      ],
      "metadata": {
        "id": "fyNUfQzc4g2e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# THIS IS THE FOLDER STRUCTURE AFTER REMOVING FRONTEND PART OF THIS PROJECT"
      ],
      "metadata": {
        "id": "JpgADpSK6nLz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "# **OpenUBA Project - Complete File & Folder Analysis**\n",
        "\n",
        "---\n",
        "\n",
        "## **ROOT LEVEL FILES**\n",
        "\n",
        "| File | Role |\n",
        "|------|------|\n",
        "| **README.md** | Project documentation - explains what OpenUBA is, how to install, and basic usage |\n",
        "| **requirements.txt** | Lists all Python packages needed (FastAPI, Pandas, TensorFlow, etc.) |\n",
        "| **Makefile** | Automation commands - shortcuts for build, test, run operations |\n",
        "| **DockerfileServer** | Docker container configuration - packages the app for deployment |\n",
        "| **LICENSE** | Legal terms - defines how others can use this code |\n",
        "| **.travis.yml** | CI/CD configuration - automated testing when code is pushed |\n",
        "| **.gitignore** | Tells Git which files to ignore (cache files, logs, etc.) |\n",
        "| **demo_test.py** | Test script to demonstrate all API endpoints working |\n",
        "\n",
        "---\n",
        "\n",
        "## **ROOT LEVEL FOLDERS**\n",
        "\n",
        "| Folder | Role |\n",
        "|--------|------|\n",
        "| **core/** | **THE HEART** - All main application code lives here |\n",
        "| **data/** | Empty placeholder for production data storage |\n",
        "| **docs/** | Documentation files (installation guide) |\n",
        "| **test_datasets/** | Sample log files for testing the system |\n",
        "| **venv/** | Python virtual environment (isolated dependencies) |\n",
        "\n",
        "---\n",
        "\n",
        "## **CORE/ FOLDER - Main Application**\n",
        "\n",
        "### **Entry Point & Server**\n",
        "\n",
        "| File | Role | Flow Position |\n",
        "|------|------|---------------|\n",
        "| **core.py** | **MAIN SERVER** - FastAPI application, all REST API endpoints, starts the web server on port 5000 | START HERE → Everything connects to this |\n",
        "| **__init__.py** | Makes core a Python package so files can import each other | Infrastructure |\n",
        "\n",
        "---\n",
        "\n",
        "### **INPUT LAYER Files (Data Collection)**\n",
        "\n",
        "| File | Role | What It Does |\n",
        "|------|------|--------------|\n",
        "| **dataset.py** | **Data Reader** - Loads data from various sources | Reads CSV, Parquet, JSON from Disk/HDFS/Elasticsearch |\n",
        "| **process.py** | **Data Ingestion Engine** - Orchestrates data loading | Reads scheme.json → loads each data source → extracts users |\n",
        "| **database.py** | **Storage Abstraction** - Handles all file read/write | Connects to FileSystem, HDFS; reads/writes JSON files |\n",
        "| **entity.py** | **Non-Human Actor Manager** - Tracks devices/servers | Creates Entity objects (servers, workstations, etc.) |\n",
        "| **user.py** | **Human Actor Manager** - Tracks all users | Extracts usernames from logs, creates User profiles |\n",
        "\n",
        "**Flow:** dataset.py → process.py → user.py / entity.py → database.py\n",
        "\n",
        "---\n",
        "\n",
        "### **PROCESSING LAYER Files (Analysis)**\n",
        "\n",
        "| File | Role | What It Does |\n",
        "|------|------|--------------|\n",
        "| **model.py** | **ML Model Engine** - Runs all machine learning models | Loads models, verifies integrity, executes analysis, returns risk scores |\n",
        "| **risk.py** | **Risk Calculator** - Computes user risk scores | Takes model outputs → calculates 0-100 score → assigns LOW/MEDIUM/HIGH |\n",
        "| **anomaly.py** | **Anomaly Detector** - Finds unusual behaviors | Detects login anomalies, data access anomalies, time anomalies |\n",
        "| **riskmanager.py** | **Risk Orchestrator** - Coordinates risk calculations | Manages multiple risk jobs running in parallel |\n",
        "\n",
        "**Flow:** Data → model.py → risk.py / anomaly.py → `riskmanager.py`\n",
        "\n",
        "---\n",
        "\n",
        "### **OUTPUT LAYER Files (Actions & Alerts)**\n",
        "\n",
        "| File | Role | What It Does |\n",
        "|------|------|--------------|\n",
        "| **alert.py** | **Alert Generator** - Creates security notifications | Creates CRITICAL/HIGH/MEDIUM/LOW alerts with descriptions |\n",
        "| **display.py** | **Dashboard Data** - Prepares data for viewing | Formats statistics, user lists, risk summaries for UI/API |\n",
        "| **api.py** | **API Helper** - Internal API utilities | Routes requests, formats responses, connects to external services |\n",
        "\n",
        "**Flow:** Analysis results → alert.py → display.py → api.py → core.py (API response)\n",
        "\n",
        "---\n",
        "\n",
        "### **UTILITY Files (Supporting Functions)**\n",
        "\n",
        "| File | Role | What It Does |\n",
        "|------|------|--------------|\n",
        "| **encode.py** | **Base64 Encoder/Decoder** | Encodes model files for transmission, decodes for installation |\n",
        "| **hash.py** | **Hashing Utility** - SHA256 | Creates fingerprints of files to verify integrity (anti-tampering) |\n",
        "| **utility.py** | **General Utilities** | Timestamps, helper functions used across the project |\n",
        "| **test.py** | **Test Framework** | Unit tests for the core functionality |\n",
        "\n",
        "---\n",
        "\n",
        "### **TEST Files**\n",
        "\n",
        "| File | Role |\n",
        "|------|------|\n",
        "| **dataset_test.py** | Tests data loading functionality |\n",
        "| **encode_test.py** | Tests Base64 encoding/decoding |\n",
        "| **hash_test.py** | Tests hashing functions |\n",
        "| **process_test.py** | Tests data processing pipeline |\n",
        "| **FILE_GUIDE.md** | Documentation explaining core files |\n",
        "\n",
        "---\n",
        "\n",
        "## **CORE/STORAGE/ - Configuration & Data**\n",
        "\n",
        "| File | Role | Content |\n",
        "|------|------|---------|\n",
        "| **scheme.json** | **Data Source Configuration** | Defines what log files to read, their format, delimiter, location |\n",
        "| **models.json** | **Model Configuration** | Lists all ML models, their settings, MITRE mappings, rules |\n",
        "| **default_model_library.json** | **Backup Model Config** | Default configuration if models.json is corrupted |\n",
        "| **model_sessions.json** | **Execution Log** | Records when each model ran and results |\n",
        "| **users.json** | **User Database** | Stores all discovered users and their profiles |\n",
        "| **settings.json** | **System Settings** | Application configuration options |\n",
        "\n",
        "### **CORE/STORAGE/MITRE/**\n",
        "| File | Role |\n",
        "|------|------|\n",
        "| **matrix.json** | Complete MITRE ATT&CK framework data - maps attacks to techniques |\n",
        "\n",
        "### **CORE/STORAGE/USERS/**\n",
        "| Folder | Role |\n",
        "|--------|------|\n",
        "| Individual user folders | Each discovered user gets a folder to store their behavioral data |\n",
        "\n",
        "### **CORE/STORAGE/SAVED_MODELS/**\n",
        "| Content | Role |\n",
        "|---------|------|\n",
        "| Empty with README | Placeholder for saving trained model weights |\n",
        "\n",
        "---\n",
        "\n",
        "## **CORE/MODEL_LIBRARY/ - ML Models**\n",
        "\n",
        "This folder contains **ready-to-use machine learning models**.\n",
        "\n",
        "| Folder | Model Type | Purpose |\n",
        "|--------|------------|---------|\n",
        "| **model_test/** | Test Model | Basic model for testing the framework works |\n",
        "| **model_1/** | Standard Model | General behavior analysis |\n",
        "| **model_simple_re/** | Regex Model | Pattern matching using regular expressions |\n",
        "| **model_simple_re_pyspark/** | Spark Regex Model | Same as above but for distributed big data |\n",
        "| **model_sk_pickle/** | Scikit-Learn Model | Traditional ML (Random Forest, SVM, etc.) |\n",
        "| **model_tf_protobuf/** | TensorFlow Model | Deep learning neural networks |\n",
        "\n",
        "**Each model folder contains:**\n",
        "- `__init__.py` - Makes it a Python package\n",
        "- MODEL.py - Contains the `execute()` function that runs the analysis\n",
        "\n",
        "---\n",
        "\n",
        "## **CORE/MODEL_MODULES/ - Data Loaders**\n",
        "\n",
        "These modules **load data for models** from different sources.\n",
        "\n",
        "| Folder | What It Loads |\n",
        "|--------|---------------|\n",
        "| **local_pandas/** | CSV/Parquet from local disk using Pandas |\n",
        "| **es/** | Data from Elasticsearch database |\n",
        "| **test_module/** | Test data loader for development |\n",
        "\n",
        "**Each contains:**\n",
        "- `__init__.py` - Package initializer\n",
        "- Main `.py` file - Actual loading logic\n",
        "\n",
        "---\n",
        "\n",
        "## **TEST_DATASETS/ - Sample Data**\n",
        "\n",
        "### **test_datasets/toy_1/**\n",
        "Sample log files for testing:\n",
        "\n",
        "| Folder | Log Type | What It Contains |\n",
        "|--------|----------|------------------|\n",
        "| **proxy/** | Web Proxy Logs | `bluecoat.log` - User web browsing activity (50MB+) |\n",
        "| **dns/** | DNS Logs | Domain name lookups |\n",
        "| **ssh/** | SSH Logs | Remote login attempts |\n",
        "| **dhcp/** | DHCP Logs | Network address assignments |\n",
        "\n",
        "---\n",
        "\n",
        "## **COMPLETE DATA FLOW DIAGRAM**\n",
        "\n",
        "```\n",
        "┌─────────────────────────────────────────────────────────────────┐\n",
        "│                        INPUT LAYER                               │\n",
        "├─────────────────────────────────────────────────────────────────┤\n",
        "│  test_datasets/       →    dataset.py    →    process.py        │\n",
        "│  (proxy, dns, ssh)         (reads files)      (orchestrates)    │\n",
        "│                                  ↓                               │\n",
        "│  storage/scheme.json  →    database.py   →    user.py           │\n",
        "│  (configuration)           (file I/O)         (extract users)   │\n",
        "└─────────────────────────────────────────────────────────────────┘\n",
        "                               ↓\n",
        "┌─────────────────────────────────────────────────────────────────┐\n",
        "│                      PROCESSING LAYER                            │\n",
        "├─────────────────────────────────────────────────────────────────┤\n",
        "│  model_library/       →    model.py      →    risk.py           │\n",
        "│  (ML models)               (run models)       (calculate score) │\n",
        "│                                  ↓                               │\n",
        "│  storage/models.json  →    anomaly.py    →    riskmanager.py    │\n",
        "│  (model config)            (find unusual)     (coordinate)      │\n",
        "│                                  ↓                               │\n",
        "│  model_modules/       →    encode.py / hash.py                  │\n",
        "│  (data loaders)            (verify integrity)                   │\n",
        "└─────────────────────────────────────────────────────────────────┘\n",
        "                               ↓\n",
        "┌─────────────────────────────────────────────────────────────────┐\n",
        "│                       OUTPUT LAYER                               │\n",
        "├─────────────────────────────────────────────────────────────────┤\n",
        "│  alert.py             →    display.py    →    api.py            │\n",
        "│  (create alerts)           (format data)      (API helpers)     │\n",
        "│                                  ↓                               │\n",
        "│                            core.py                               │\n",
        "│                     (REST API endpoints)                         │\n",
        "│                              ↓                                   │\n",
        "│                    http://localhost:5000                         │\n",
        "│                    /docs (Swagger UI)                            │\n",
        "└─────────────────────────────────────────────────────────────────┘\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## **SUMMARY TABLE - All Files by Function**\n",
        "\n",
        "| Function | Files |\n",
        "|----------|-------|\n",
        "| **Server/API** | core.py, api.py |\n",
        "| **Data Loading** | dataset.py, process.py, database.py |\n",
        "| **User/Entity Tracking** | user.py, entity.py |\n",
        "| **ML Models** | model.py + model_library/* |\n",
        "| **Data Loaders for Models** | model_modules/* |\n",
        "| **Risk Analysis** | risk.py, riskmanager.py |\n",
        "| **Anomaly Detection** | anomaly.py |\n",
        "| **Alerts** | alert.py |\n",
        "| **Display/Output** | display.py |\n",
        "| **Security Utilities** | encode.py, hash.py |\n",
        "| **General Utilities** | utility.py |\n",
        "| **Configuration** | storage/*.json |\n",
        "| **Test Data** | test_datasets/toy_1/* |\n",
        "| **Tests** | *_test.py, test.py |\n",
        "| **Documentation** | README.md, docs/INSTALL.md, FILE_GUIDE.md |\n",
        "| **DevOps** | Makefile, DockerfileServer, .travis.yml, requirements.txt |\n",
        "\n",
        "---\n",
        "\n",
        "## **EXECUTION ORDER (When Server Starts)**\n",
        "\n",
        "1. **core.py** starts → Creates FastAPI server\n",
        "2. Imports all modules (model, risk, alert, etc.)\n",
        "3. Loads **models.json** → Model configuration\n",
        "4. Loads **scheme.json** → Data source configuration\n",
        "5. Server listens on **port 5000**\n",
        "6. When request comes:\n",
        "   - **dataset.py** reads data\n",
        "   - **model.py** runs analysis\n",
        "   - **risk.py** calculates scores\n",
        "   - **alert.py** creates alerts\n",
        "   - **core.py** returns API response"
      ],
      "metadata": {
        "id": "VgG2Lti56GRa"
      }
    }
  ]
}